5bbd6dd4 fix ci bug
298cb04b Add DeepEP submodule and installation script for CI
35433fe4 fix import error
47dc3668 fix review issues
71ece5e1 make ci happy
c37575a8 upd doc
7eddf31a lint
d23a65e1 [Feat] Support auto vectorization for ld/st to optimize combine to surpass deepep
2c1bd1fb support int4 ld/st ptx in cuda template
08281a68 use cuda postproc for vectorization in combine
a36afea8 make rank as an argument of the kernel
072324bd add dispatch benchmark result
05a93004 optimze dispatch perf via skipping tensor validation
1e8ad160 use comm_stream for comm kernels
f66691d4 use int4 vectorization for dispatch
2db7a385 update doc
32804bcd remove redundant test
ed2ca7bf Refactor to pre-alloc buffers and expose interface, add benchmark
bc4c6d65 unify dispatch, migrate topk_idx to u64, support cached dispatch
449be5bc intra-node combine test passed
6b6b9903 support massage-only debug print
1d7c4569 draft combine
1cb41e7a intra-node dispatch test passed
3cefc96b refactor T.wait_* and refine dispatch test logic
8333785a support device-side wait_ne
ea25c7f8 support warp vote and add test
9a4e5e56 suupport ld, st, warp_sync, continue and add test
01b9996c draft dispatch
b745a761 support elect_one_sync() and add test
c28e0c61 [Feat] Add `get_device_tensor` function and related test
5d686109 fix prev typo
2f65de98 rename and refactor `T.barrier/sync_blocks`
b13fe3ff draft notify dispatch
43e6965d [Feat] Support warp reduce operators
158e98a2 use strided loop to simplify get_dispatch a bit
da032593 [Feat] Enhance `T.st` to support intra-node store to peer's symm memory
f6df001b [DeepEP] Move deepep benchmark to example and allow compatible with new version DeepEP
75b67b04 Temporarily exclude sink tests from non-distributed example tests in CI to address timeout issues
da0eea12 [Cache] Rename sparse compress cache directory
7cce21c6 Enhance threadblock swizzle templates with default offset parameter and streamline parser.py for better readability
fe7bbdd3 [Feat] Support stride in `T.serial`
1de65fdd fix a bug
7d0b1045 [Feat] Support step in T.serial
a5ce988a [BugFix] Fix port conflict in ci.yml pytest   (#29)
3a66a723 [Feat] Support container install docs (#38)
dafaa6fd [Example] Add CP example (#37)
fae99e98 [Example] Update intra-node GEMM-RS example (#35)
aa916f49 [Feat] Support internode copy with intranode copy (#26)
190723c6 [README] Update dependency in installation doc (#34)
538ceafb [README] Update installation doc (#32)
b51fc918 Merge remote-tracking branch 'upstream/main' into main (#31)
5845232c [Example] Update AG-GEMM example (#30)
7df1a99d [Refactor][Example] Add AG-GEMM overlapping example (#28)
2cc1a2cb [Feature][Example] Support  `return_peers` in `_allocate_tensor` and add ag_gemm_ipc example (#25)
2fbbd765 [Feat] Generalize example_all_to_all and use absolute for benchmark_all_to_all (#24)
aed06c39 [Feat] Add `put_block`/`get_block` implementation & examples and rename primitives (#23)
46b1be49 [Benchmark] Add benchmark for IPC communication and support for fence operators (#22)
845586b2 [Feat] Support sequence parallel all2all with transpose example (#21)
ad223f23 [Doc] Update README.md to include a link to the project roadmap for contributors (#20)
97f5d3b5 [Feat] Implement device and system level barrier for blocks based on atomic operations (#9)
70137ee7 Merge pull request #17 from tile-ai/tzj/debug
60604f26 Merge branch 'main' into tzj/debug
89e406d4 Remove ABI configuration in setup.py
1449e3c8 [Feat][Doc] Enhance TileLang with NVSHMEM support and build system improvements (#18)
46c8e358 [Lint]
760b16d5 [BugFix] Robust import for different versions of cuda-python package
ab56a2f7 [Example] Support SP all2all without transpose example (#16)
10f4fdb8 [Feat] Refactor UVA mechanism and introduce metadata table to support remote device-to-device copy (#15)
b52bcae4 [Feat] Add support for IPC-based pull from remote tensors and update naming to clarify remote copy operators' scope (#14)
c2cfb4ac Merge pull request #13 from tile-ai/wt/fix-ipc
c3d0298f [Bugfix] Fix the ipc bug caused by incorrect tensor management and update example_remote_copy
c7dc0a7a Merge upstream/main into main (#12)
39b3f8d0 [Feature] Support UVA copy with tile primitive (#11)
adbef0d3 [Feat] support example allgather gemm (#10)
32dca925 [Feat] Add `copy_unrolled` operation for optimized memory copying (#2)
9642b0f5 [Feat] Implement `get_dispatch_layout` v1 for intranode DeepEP
f73ca730 [Refactor] Refactor NVSHMEM enum bindings and adjust the logic of including the distributed header - Use Python enum class to hold NVSHMEM enum values - Include the distributed header based on environment variable to improve robustness - Fix lint - Update write32/64 naming
b716b37f [Dev] Add draft gemm-reducescatter benchmark
288b6fae [Dev] Update benchmark - Use new `T.use_swizzle` API in AG-GEMM - Adjust blocksize and the order of blockIdx
b28aef0d Fix lint
4b382780 [Feat] Introduce an offset option in threadblock swizzle (#668)
94bd9ceb [Feat] Add `disable_rdc` option to config and update launch script - Introduce an option to diable `rdc` in compilation of distributed programs, which temporarily avoids the misaligned address error. - Use random port in the distributed launching script to enable simultaneously launching multiple distributed programs. - Enable TMA in allgather-gemm benchmark.
590fdad1 [Documentation] Update README.md
f9b430ef Update README.md
873d6bd1 [Documentation] Revise README.md to introduce TileScale and its hierarchical architecture
c119092e [Fix] Fix installation script - Adjust import order to avoid import error during installation - Fix typo - Refactor `launch.sh`
08244fb4 [Feat] Enhance distributed support in CUDA code generation - Conditional inclusion of distributed.h based on use_distributed_ flag - Updated code generation to set use_distributed_ for relevant operations - Adjusted environment configuration for NVSHMEM paths in Python scripts
c231e268 [Fix] Add doc for installation and fix path - Clarify NVSHMEM lib and pynvshmem package in doc - Fix path error in `setup.py` in pynvshmem
3bbf1bd9 [Feat] Introduce persistent consumer and threadblock swizzle to AG-GEMM - Introduce persistent-gemm consumer in allgather-gemm - Introduce the threadblock swizzle strategy tailored for allgather - Report TMA bug of misaligned address - Fix typo - Add a memcheck option in `launch.sh` for debugging
c9767076 [Feat] Add ag-gemm benchmark and host-side ops - Add allgather-gemm benchmark v1 - Add host-side atomic signal op. to pynvshmem - Fix typo in previous benchmarks
22f31f09 [feat] Enhance distributed support and NVSHMEM integration - Updated README with installation instructions for pynvshmem and NVSHMEM setup - Introduced build script for NVSHMEM installation - Refactored code to utilize USE_DISTRIBUTED environment variable for conditional NVSHMEM paths - Improved handling of NVSHMEM paths across various modules - Added testing command for pynvshmem import
9b100fbf [Enhancement] Enhance Code Generation and Optimization - Added support for global thread synchronization in code generation - Improved management of dynamic shared memory and persistent L2 cache - Refined handling of TMA descriptors - Updated Cython wrappers to support stricter tensor validation
218b6a5f [bugfix] Fix nvshmem_create_tensor and reduce_scatter benchmark - Add synchronization to `nvshmem_create_tensor` - Add zero initialization to flags triton reduce_scatter - Use fp32 accumulator in tilelang reduce_scatter
9acf5747 [feat] Add launch options
c9469d6a release and adapt to tilelang pynvshmem
b77cddf7 Add bindings and test for basic queries
7c7808eb Adjust pynvshmem pkg postion and set CMAKE_CUDA_ARCH to native
ca8a64fe [test] Update Triton and TileLang implementations for benchmarking
78de25df [test] add triton rs impl for benchmarking
ce80aecb [test] Add reducescatter benchmark
f57df4c2 [feat] Add prompt and codegen for T.getmem_nbi_block
15dd2e10 [test] add barrier_all after tilelang allgather kernel to ensure correctness
0e5291d1 [dev] update allgather example
c4157c82 fix lint
f1437256 [test] Update allgather example to improve robustness
2f06f992 [dev] move dtype_map
5f695850 [test] Add AG benchmark
f0015bde [feat] Add utility functions
effb615c [doc] Add docstring and arg lists for some api
8bf13337 [feat] Add codegen for quiet()
a4d2c7dc fix lint
9f39650f [enhancement] Enhance the example of AG and testing
a64861a8 [refactor] Remove dsize_map in distributed/utils and use dtype.itemsize
19a7fd08 [feat] Add CUDA codegen for barrier_all
322b9067 [Test] Debug message and gitignore for redundant source code (#608)
af115e05 Remove dtype_size_in_bytes() and use dtype.itemsize instead.
36cf8268 [Benchmark] Add alltoall benchmark
36510d66 [Dev] Update SUMMA example
8eb75054 [Dev] Update cannon example, with non-specialize and specialize implementations
26534ad5 [Enhancement] Add context manager import to distributed utilities
c500c00c [BugFix] Fix incorrect variable assignment in `cannon` function of `example_cannon.py`
edfb5012 [Feature] Add distributed GEMM and Cannon examples
d39e4394 [Enhancement] Refactor distributed examples to use init_distributed utility
50d9c19d [Feature] Introduce cpengine intrinsics and example for all-to-all communication
ff4706ba [Update] Enhance .gitignore for build artifacts
855fc939 [Feature] Add allgather GEMM example for distributed operations
06fff098 [Feature] Add allgather example for distributed operations
306b3074 [Refactor] Remove example_nvshmem.py and add example_simple_shift.py
e2cff27a [Feature] Expand NVSHMEM Distributed Functionality
46ae907f [Feature] Enhanced Distributed Support and New Functionality
db55eb7f [Dev] Update Example and Distributed Support
dd77e6ab [Dev] Add NVSHMEM support and new functionality
35cff570 [Feature] Implement NVSHMEM tensor creation and initialization
e58d34ab [Feature] Enhance NVSHMEM integration with new scripts and CUDA support
7d4e3c19 [Feature] Integrate NVSHMEM support and add example for distributed operations
