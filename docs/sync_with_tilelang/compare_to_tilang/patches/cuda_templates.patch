diff --git a/src/tl_templates/cuda/atomic.h b/src/tl_templates/cuda/atomic.h
new file mode 100644
index 00000000..fe460702
--- /dev/null
+++ b/src/tl_templates/cuda/atomic.h
@@ -0,0 +1,279 @@
+#pragma once
+
+#ifndef __CUDACC_RTC__
+#include <cuda_runtime.h>
+#endif
+
+#include <cuda/atomic>
+#include <cutlass/numeric_types.h>
+
+using cutlass::bfloat16_t;
+using cutlass::half_t;
+
+#define TL_DEVICE __forceinline__ __device__
+
+template <typename T> struct normalize_atomic_type {
+  using type = T;
+};
+
+template <> struct normalize_atomic_type<half_t> {
+  using type = half;
+};
+
+#if (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ > 750))
+template <> struct normalize_atomic_type<bfloat16_t> {
+  using type = __nv_bfloat16;
+};
+#endif
+
+template <typename T1, typename T2> TL_DEVICE T1 cuda_cast(T2 val) {
+  return T1(val);
+}
+
+template <> TL_DEVICE half cuda_cast<half, float>(float val) {
+  return __float2half(val);
+}
+
+#if (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ > 750))
+template <> TL_DEVICE __nv_bfloat16 cuda_cast<__nv_bfloat16, float>(float val) {
+  return __float2bfloat16(val);
+}
+#endif
+
+template <typename T1, typename T2>
+TL_DEVICE void AtomicMax(T1 &ref, T2 val,
+                         int memory_order = int(cuda::memory_order_relaxed)) {
+  using NT1 = typename normalize_atomic_type<T1>::type;
+  T1 *address = &ref;
+  if constexpr (std::is_same_v<NT1, half> ||
+                std::is_same_v<NT1, __nv_bfloat16>) {
+    atomicMax(reinterpret_cast<NT1 *>(address), static_cast<NT1>(val));
+  } else {
+    cuda::atomic_ref<NT1, cuda::thread_scope_device> aref(*address);
+    aref.fetch_max(cuda_cast<NT1>(val), cuda::memory_order(memory_order));
+  }
+}
+
+template <typename T1, typename T2>
+TL_DEVICE T1 AtomicMaxRet(T1 &ref, T2 val,
+                          int memory_order = int(cuda::memory_order_relaxed)) {
+  using NT1 = typename normalize_atomic_type<T1>::type;
+  T1 *address = &ref;
+  if constexpr (std::is_same_v<NT1, half> ||
+                std::is_same_v<NT1, __nv_bfloat16>) {
+    return static_cast<T1>(
+        atomicMax(reinterpret_cast<NT1 *>(address), static_cast<NT1>(val)));
+  } else {
+    cuda::atomic_ref<NT1, cuda::thread_scope_device> aref(*address);
+    return static_cast<T1>(
+        aref.fetch_max(cuda_cast<NT1>(val), cuda::memory_order(memory_order)));
+  }
+}
+
+template <typename T1, typename T2>
+TL_DEVICE void AtomicMin(T1 &ref, T2 val,
+                         int memory_order = int(cuda::memory_order_relaxed)) {
+  using NT1 = typename normalize_atomic_type<T1>::type;
+  T1 *address = &ref;
+  if constexpr (std::is_same_v<NT1, half> ||
+                std::is_same_v<NT1, __nv_bfloat16>) {
+    atomicMin(reinterpret_cast<NT1 *>(address), static_cast<NT1>(val));
+  } else {
+    cuda::atomic_ref<NT1, cuda::thread_scope_device> aref(*address);
+    aref.fetch_min(cuda_cast<NT1>(val), cuda::memory_order(memory_order));
+  }
+}
+
+template <typename T1, typename T2>
+TL_DEVICE T1 AtomicMinRet(T1 &ref, T2 val,
+                          int memory_order = int(cuda::memory_order_relaxed)) {
+  using NT1 = typename normalize_atomic_type<T1>::type;
+  T1 *address = &ref;
+  if constexpr (std::is_same_v<NT1, half> ||
+                std::is_same_v<NT1, __nv_bfloat16>) {
+    return static_cast<T1>(
+        atomicMin(reinterpret_cast<NT1 *>(address), static_cast<NT1>(val)));
+  } else {
+    cuda::atomic_ref<NT1, cuda::thread_scope_device> aref(*address);
+    return static_cast<T1>(
+        aref.fetch_min(cuda_cast<NT1>(val), cuda::memory_order(memory_order)));
+  }
+}
+
+template <typename T1, typename T2>
+TL_DEVICE void AtomicAdd(T1 &ref, T2 val,
+                         int memory_order = int(cuda::memory_order_relaxed)) {
+  using NT1 = typename normalize_atomic_type<T1>::type;
+  T1 *address = &ref;
+  if constexpr ((std::is_same_v<NT1, half> ||
+                 std::is_same_v<NT1, __nv_bfloat16>) &&
+                memory_order == int(cuda::memory_order_relaxed)) {
+    atomicAdd(reinterpret_cast<NT1 *>(address), static_cast<NT1>(val));
+  } else {
+    cuda::atomic_ref<NT1, cuda::thread_scope_device> aref(*address);
+    aref.fetch_add(cuda_cast<NT1>(val), cuda::memory_order(memory_order));
+  }
+}
+
+template <typename T1, typename T2>
+TL_DEVICE T1 AtomicAddRet(T1 &ref, T2 val,
+                          int memory_order = int(cuda::memory_order_relaxed)) {
+  using NT1 = typename normalize_atomic_type<T1>::type;
+  T1 *address = &ref;
+  if constexpr ((std::is_same_v<NT1, half> ||
+                 std::is_same_v<NT1, __nv_bfloat16>) &&
+                memory_order == int(cuda::memory_order_relaxed)) {
+    return static_cast<T1>(
+        atomicAdd(reinterpret_cast<NT1 *>(address), static_cast<NT1>(val)));
+  } else {
+    cuda::atomic_ref<NT1, cuda::thread_scope_device> aref(*address);
+    return static_cast<T1>(
+        aref.fetch_add(cuda_cast<NT1>(val), cuda::memory_order(memory_order)));
+  }
+}
+
+// TODO add memory_order for vectorized atomic add
+TL_DEVICE void AtomicAddx2(half_t *ref, half_t *val,
+                           int memory_order = int(cuda::memory_order_relaxed)) {
+  atomicAdd(reinterpret_cast<half2 *>(ref),
+            static_cast<half2>(*reinterpret_cast<half2 *>(val)));
+}
+
+TL_DEVICE half2
+AtomicAddx2Ret(half_t *ref, half_t *val,
+               int memory_order = int(cuda::memory_order_relaxed)) {
+  return atomicAdd(reinterpret_cast<half2 *>(ref),
+                   static_cast<half2>(*reinterpret_cast<half2 *>(val)));
+}
+
+#if (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ > 750))
+TL_DEVICE void AtomicAddx2(bfloat16_t *ref, bfloat16_t *val,
+                           int memory_order = int(cuda::memory_order_relaxed)) {
+  atomicAdd(
+      reinterpret_cast<__nv_bfloat162 *>(ref),
+      static_cast<__nv_bfloat162>(*reinterpret_cast<__nv_bfloat162 *>(val)));
+}
+
+TL_DEVICE __nv_bfloat162
+AtomicAddx2Ret(bfloat16_t *ref, bfloat16_t *val,
+               int memory_order = int(cuda::memory_order_relaxed)) {
+  return atomicAdd(
+      reinterpret_cast<__nv_bfloat162 *>(ref),
+      static_cast<__nv_bfloat162>(*reinterpret_cast<__nv_bfloat162 *>(val)));
+}
+#endif
+
+#if (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 900))
+TL_DEVICE void AtomicAddx2(float *ref, float *val,
+                           int memory_order = int(cuda::memory_order_relaxed)) {
+  atomicAdd(reinterpret_cast<float2 *>(ref),
+            static_cast<float2>(*reinterpret_cast<float2 *>(val)));
+}
+
+TL_DEVICE float2
+AtomicAddx2Ret(float *ref, float *val,
+               int memory_order = int(cuda::memory_order_relaxed)) {
+  return atomicAdd(reinterpret_cast<float2 *>(ref),
+                   static_cast<float2>(*reinterpret_cast<float2 *>(val)));
+}
+
+TL_DEVICE void AtomicAddx4(float *ref, float *val,
+                           int memory_order = int(cuda::memory_order_relaxed)) {
+  atomicAdd(reinterpret_cast<float4 *>(ref),
+            static_cast<float4>(*reinterpret_cast<float4 *>(val)));
+}
+
+TL_DEVICE float4
+AtomicAddx4Ret(float *ref, float *val,
+               int memory_order = int(cuda::memory_order_relaxed)) {
+  return atomicAdd(reinterpret_cast<float4 *>(ref),
+                   static_cast<float4>(*reinterpret_cast<float4 *>(val)));
+}
+#endif
+
+template <typename T> TL_DEVICE T AtomicLoad(T &ref, int memory_order) {
+  cuda::atomic_ref<T, cuda::thread_scope_device> aref(ref);
+  return aref.load(cuda::memory_order(memory_order));
+}
+
+template <typename T1, typename T2>
+TL_DEVICE void AtomicStore(T1 &ref, T2 value, int memory_order) {
+  using NT1 = typename normalize_atomic_type<T1>::type;
+  cuda::atomic_ref<NT1, cuda::thread_scope_device> aref(ref);
+  aref.store(cuda_cast<NT1>(value), cuda::memory_order(memory_order));
+}
+
+namespace tl {
+
+TL_DEVICE uint32_t ptx_atom_add_relaxed_gpu(const uint32_t *ptr,
+                                            uint32_t value) {
+  uint32_t ret;
+  asm volatile("atom.add.relaxed.gpu.global.u32 %0, [%1], %2;\n"
+               : "=r"(ret)
+               : "l"(ptr), "r"(value));
+  return ret;
+}
+
+TL_DEVICE uint32_t ptx_atom_add_acquire_gpu(const uint32_t *ptr,
+                                            uint32_t value) {
+  uint32_t ret;
+  asm volatile("atom.add.acquire.gpu.global.u32 %0, [%1], %2;\n"
+               : "=r"(ret)
+               : "l"(ptr), "r"(value));
+  return ret;
+}
+
+TL_DEVICE uint32_t ptx_atom_add_release_gpu(const uint32_t *ptr,
+                                            uint32_t value) {
+  uint32_t ret;
+  asm volatile("atom.add.release.gpu.global.u32 %0, [%1], %2;\n"
+               : "=r"(ret)
+               : "l"(ptr), "r"(value));
+  return ret;
+}
+
+TL_DEVICE uint32_t ptx_atom_add_acq_rel_gpu(const uint32_t *ptr,
+                                            uint32_t value) {
+  uint32_t ret;
+  asm volatile("atom.add.acq_rel.gpu.global.u32 %0, [%1], %2;\n"
+               : "=r"(ret)
+               : "l"(ptr), "r"(value));
+  return ret;
+}
+
+TL_DEVICE uint32_t ptx_atom_add_relaxed_sys(const uint32_t *ptr,
+                                            uint32_t value) {
+  uint32_t ret;
+  asm volatile("atom.add.relaxed.sys.global.u32 %0, [%1], %2;\n"
+               : "=r"(ret)
+               : "l"(ptr), "r"(value));
+  return ret;
+}
+
+TL_DEVICE uint32_t ptx_atom_add_acquire_sys(const uint32_t *ptr,
+                                            uint32_t value) {
+  uint32_t ret;
+  asm volatile("atom.add.acquire.sys.global.u32 %0, [%1], %2;\n"
+               : "=r"(ret)
+               : "l"(ptr), "r"(value));
+  return ret;
+}
+
+TL_DEVICE uint32_t ptx_atom_add_release_sys(const uint32_t *ptr,
+                                            uint32_t value) {
+  uint32_t ret;
+  asm volatile("atom.add.release.sys.global.u32 %0, [%1], %2;\n"
+               : "=r"(ret)
+               : "l"(ptr), "r"(value));
+  return ret;
+}
+
+TL_DEVICE uint32_t ptx_atom_add_acq_rel_sys(const uint32_t *ptr,
+                                            uint32_t value) {
+  uint32_t ret;
+  asm volatile("atom.add.acq_rel.sys.global.u32 %0, [%1], %2;\n"
+               : "=r"(ret)
+               : "l"(ptr), "r"(value));
+  return ret;
+}
+} // namespace tl
\ No newline at end of file
diff --git a/src/tl_templates/cuda/barrier.h b/src/tl_templates/cuda/barrier.h
new file mode 100644
index 00000000..5eeb4abd
--- /dev/null
+++ b/src/tl_templates/cuda/barrier.h
@@ -0,0 +1,158 @@
+#pragma once
+
+#include "common.h"
+#include <cutlass/arch/barrier.h>
+
+// Reuse cutlass advanced barrier abstraction
+using Barrier = cutlass::arch::ClusterTransactionBarrier;
+
+namespace tl {
+
+TL_DEVICE void mbarrier_init(uint64_t &smem_barrier, uint32_t arrive_count) {
+  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
+  asm volatile("mbarrier.init.shared.b64 [%1], %0;"
+               :
+               : "r"(arrive_count), "r"(smem_int_ptr));
+}
+
+TL_DEVICE uint32_t mbarrier_try_wait(uint64_t &smem_barrier, int phase_bit) {
+
+  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
+  uint32_t waitComplete;
+
+  asm volatile("{\n\t"
+               ".reg .pred P1; \n\t"
+               "mbarrier.try_wait.parity.shared.b64 P1, [%1], %2; \n\t"
+               "selp.b32 %0, 1, 0, P1; \n\t"
+               "}"
+               : "=r"(waitComplete)
+               : "r"(smem_int_ptr), "r"(phase_bit));
+
+  return waitComplete;
+}
+
+TL_DEVICE void mbarrier_wait(uint64_t &smem_barrier, int phase_bit) {
+  if (mbarrier_try_wait(smem_barrier, phase_bit) == 0) {
+    uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
+    // Arbitrarily large timer value after which try-wait expires and re-tries.
+    uint32_t ticks = 0x989680;
+    asm volatile("{\n\t"
+                 ".reg .pred       P1; \n\t"
+                 "LAB_WAIT: \n\t"
+                 "mbarrier.try_wait.parity.shared.b64 P1, [%0], %1, %2; \n\t"
+                 "@P1 bra DONE; \n\t"
+                 "bra     LAB_WAIT; \n\t"
+                 "DONE: \n\t"
+                 "}"
+                 :
+                 : "r"(smem_int_ptr), "r"(phase_bit), "r"(ticks));
+  }
+}
+
+TL_DEVICE void mbarrier_test_wait(uint64_t &smem_barrier, int phase_bit) {
+  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
+  asm volatile(
+      "{\n"
+      ".reg .pred                P1;\n"
+      "LAB_WAIT:\n"
+      "mbarrier.test_wait.parity.shared::cta.b64 P1, [%0], %1;\n"
+      "@P1                       bra.uni DONE;\n"
+      "nanosleep.u32 5;\n" // wait a few nanoseconds on pre-Hopper architectures
+                           // to save instruction issue slots
+      "bra.uni                   LAB_WAIT;\n"
+      "DONE:\n"
+      "}\n" ::"r"(smem_int_ptr),
+      "r"(phase_bit));
+}
+
+TL_DEVICE void mbarrier_arrive(uint64_t &smem_barrier) {
+  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
+  asm volatile("mbarrier.arrive.shared.b64 _, [%0];" : : "r"(smem_int_ptr));
+}
+
+TL_DEVICE void mbarrier_arrive(uint64_t &smem_barrier, int cta_id,
+                               uint32_t pred) {
+  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
+  if (pred) {
+    asm volatile("{\n\t"
+                 ".reg .b32 remAddr32;\n\t"
+                 "mapa.shared::cluster.u32  remAddr32, %0, %1;\n\t"
+                 "mbarrier.arrive.shared::cluster.b64  _, [remAddr32];\n\t"
+                 "}"
+                 :
+                 : "r"(smem_int_ptr), "r"(cta_id));
+  }
+}
+
+TL_DEVICE void mbarrier_expect_tx(uint64_t &smem_barrier,
+                                  uint32_t transaction_bytes) {
+  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
+  asm volatile("mbarrier.expect_tx.shared.b64 [%1], %0;"
+               :
+               : "r"(transaction_bytes), "r"(smem_int_ptr));
+}
+
+TL_DEVICE void mbarrier_arrive_expect_tx(uint64_t &smem_barrier,
+                                         uint32_t transaction_bytes) {
+  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
+  asm volatile("mbarrier.arrive.expect_tx.shared.b64 _, [%1], %0;"
+               :
+               : "r"(transaction_bytes), "r"(smem_int_ptr));
+}
+
+template <typename BarrierType = uint64_t>
+TL_DEVICE void mbarrier_cp_async_arrive(BarrierType &smem_mbar) {
+  uint32_t smem_int_mbar;
+  if constexpr (std::is_pointer_v<BarrierType>) {
+    smem_int_mbar = smem_ptr_to_uint(reinterpret_cast<uint64_t *>(smem_mbar));
+  } else {
+    smem_int_mbar = smem_ptr_to_uint(reinterpret_cast<uint64_t *>(&smem_mbar));
+  }
+  asm volatile("cp.async.mbarrier.arrive.shared.b64 [%0];"
+               :
+               : "r"(smem_int_mbar));
+}
+
+template <typename BarrierType = uint64_t>
+TL_DEVICE void mbarrier_cp_async_arrive_noinc(BarrierType &smem_mbar) {
+  uint32_t smem_int_mbar;
+  if constexpr (std::is_pointer_v<BarrierType>) {
+    smem_int_mbar = smem_ptr_to_uint(reinterpret_cast<uint64_t *>(smem_mbar));
+  } else {
+    smem_int_mbar = smem_ptr_to_uint(reinterpret_cast<uint64_t *>(&smem_mbar));
+  }
+  asm volatile("{\n\t"
+               "cp.async.mbarrier.arrive.noinc.shared::cta.b64 [%0];\n\t"
+               "}"
+               :
+               : "r"(smem_int_mbar));
+  cutlass::arch::synclog_emit_cpasync_barrier_arrive(__LINE__, smem_int_mbar);
+}
+
+TL_DEVICE void fence_proxy_async() {
+  asm volatile("fence.proxy.async.shared::cta;" : :);
+}
+
+// Indicate arrival of warp issuing TMA_STORE
+TL_DEVICE void tma_store_arrive() {
+  asm volatile("cp.async.bulk.commit_group;");
+}
+
+template <int Count> TL_DEVICE void tma_store_wait() {
+  asm volatile("cp.async.bulk.wait_group.read %0;" : : "n"(Count) : "memory");
+}
+
+TL_DEVICE void syncthreads_partial(uint64_t &smem_barrier) {
+  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
+  uint64_t state = 0;
+  asm volatile("{\n"
+               ".reg .pred                P1;\n"
+               "mbarrier.arrive.shared.b64 %1, [%0];\n"
+               "LAB_WAIT:\n"
+               "mbarrier.try_wait.shared.b64 P1, [%0], %1;\n"
+               "@!P1                      bra.uni LAB_WAIT;\n"
+               "}\n"
+               :
+               : "r"(smem_int_ptr), "l"(state));
+}
+} // namespace tl
diff --git a/src/tl_templates/cuda/common.h b/src/tl_templates/cuda/common.h
index d92b58b3..057f6a25 100644
--- a/src/tl_templates/cuda/common.h
+++ b/src/tl_templates/cuda/common.h
@@ -4,6 +4,8 @@
 #include <cuda_runtime.h>
 #endif
 
+#include "atomic.h"
+#include <cute/arch/util.hpp>
 #include <cutlass/fast_math.h>
 #include <cutlass/numeric_types.h>
 #include <math_constants.h>
@@ -12,11 +14,15 @@ using cutlass::bfloat16_t;
 using cutlass::half_t;
 using cutlass::tfloat32_t;
 
+using cute::cast_smem_ptr_to_uint;
+
 using int4_t = int4;
 
 #define hexp cutlass::fast_exp
 #define hlog cutlass::fast_log
 #define hsqrt cutlass::fast_sqrt
+#define hsin cutlass::fast_sin
+#define hcos cutlass::fast_cos
 #define htanh cutlass::fast_tanh
 #define hpow powf
 
@@ -42,16 +48,21 @@ using int4_t = int4;
   do {                                                                         \
     cudaError_t __err = cudaGetLastError();                                    \
     if (__err != cudaSuccess) {                                                \
-      snprintf(error_buf, ERROR_BUF_SIZE, "kernel_name: %s - %s",              \
+      snprintf(error_buf, ERROR_BUF_SIZE, kernel_name ": %s - %s",             \
                cudaGetErrorName(__err), cudaGetErrorString(__err));            \
       return -1;                                                               \
     }                                                                          \
   } while (0)
 
-// abs function for bfloat_t and half_t since there is no implicit convertion
-// method
-TL_PATCH TL_DEVICE half_t __habs(const half_t x) {
-  return half_t(__habs(x.to_half()));
+// using cutlass abs function for half_t
+TL_PATCH TL_DEVICE half_t __habs(const half_t x) { return abs(x); }
+
+// using cutlass abs function for bfloat_t
+TL_PATCH TL_DEVICE bfloat16_t __habs(const bfloat16_t x) { return abs(x); }
+
+// hrsqrt function for half_t
+TL_PATCH TL_DEVICE half_t hrsqrt(const half_t x) {
+  return half_t(hrsqrt(x.to_half()));
 }
 
 // Pack two half values.
@@ -108,7 +119,19 @@ TL_DEVICE uint32_t smem_ptr_to_uint(void const *const ptr) {
   return static_cast<uint32_t>(__cvta_generic_to_shared(ptr));
 }
 
-// Helper to cast SMEM pointer to unsigned
+/**
+ * Convert a shared-memory pointer to a 32-bit unsigned integer address.
+ *
+ * Casts the given pointer (expected to reference shared memory) into a 32-bit
+ * unsigned integer using the device address-space conversion required for
+ * shared-memory pointers.
+ *
+ * @param smem_ptr Pointer into shared memory.
+ * @return 32-bit unsigned integer representation of the shared-memory address.
+ *
+ * @note The pointer must refer to shared memory; behavior is undefined for
+ *       pointers in other address spaces.
+ */
 TL_DEVICE unsigned int cast_smem_ptr_to_int(const void *const smem_ptr) {
   unsigned int smem_int;
   asm volatile("{ .reg .u64 smem_int; cvta.to.shared.u64 smem_int, %1; "
@@ -118,86 +141,27 @@ TL_DEVICE unsigned int cast_smem_ptr_to_int(const void *const smem_ptr) {
   return smem_int;
 }
 
-template <typename T1, typename T2>
-TL_DEVICE void AtomicAdd(T1 *address, T2 val) {
-  atomicAdd(reinterpret_cast<T1 *>(address), static_cast<T1>(val));
-}
-
-// // AtomicAdd Functions for FP32
-// TL_DEVICE void AtomicAdd(float *address, float val) {
-//   atomicAdd(reinterpret_cast<float *>(address), val);
-// }
-
-// AtomicAdd Functions for FP16
-template <> TL_DEVICE void AtomicAdd(half_t *address, half_t val) {
-  // Use atomicCAS with built-in cuda_fp16 support
-  atomicAdd(reinterpret_cast<half *>(address), static_cast<half>(val));
-}
-
-// AtomicAdd Functions for FP16
-template <> TL_DEVICE void AtomicAdd(half_t *address, half_t *val) {
-  atomicAdd(reinterpret_cast<half *>(address), static_cast<half>(*val));
-}
-
-// AtomicAdd Functions for FP16
-template <> TL_DEVICE void AtomicAdd(half_t *address, float val) {
-  // Use atomicCAS with built-in cuda_fp16 support
-  atomicAdd(reinterpret_cast<half *>(address), __float2half(val));
-}
-
-// AtomicAdd Functions for BFLOAT16
-#if (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ > 750))
-// AtomicAdd Functions for BFLOAT16
-template <> TL_DEVICE void AtomicAdd(bfloat16_t *address, bfloat16_t *val) {
-  atomicAdd(reinterpret_cast<__nv_bfloat16 *>(address),
-            static_cast<__nv_bfloat16>(*val));
-}
-
-// AtomicAdd Functions for BFLOAT16
-template <> TL_DEVICE void AtomicAdd(bfloat16_t *address, float val) {
-  atomicAdd(reinterpret_cast<__nv_bfloat16 *>(address), __float2bfloat16(val));
-}
-
-#endif
-
-// AtomicAdd Functions for FP16x2
-TL_DEVICE void AtomicAddx2(half_t *address, half_t *val) {
-  atomicAdd(reinterpret_cast<half2 *>(address),
-            static_cast<half2>(*reinterpret_cast<half2 *>(val)));
-}
-
-#if (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ > 750))
-
-// AtomicAdd Functions for BFLOAT16
-template <> TL_DEVICE void AtomicAdd(bfloat16_t *address, bfloat16_t val) {
-  atomicAdd(reinterpret_cast<__nv_bfloat16 *>(address),
-            static_cast<__nv_bfloat16>(val));
-}
-
-// AtomicAdd Functions for BFLOAT16x2
-TL_DEVICE void AtomicAddx2(bfloat16_t *address, bfloat16_t *val) {
-  atomicAdd(
-      reinterpret_cast<__nv_bfloat162 *>(address),
-      static_cast<__nv_bfloat162>(*reinterpret_cast<__nv_bfloat162 *>(val)));
-}
-#endif
-
-#if (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 900))
-// AtomicAdd Functions for FLOAT16x2
-TL_DEVICE void AtomicAddx2(float *address, float *val) {
-  atomicAdd(reinterpret_cast<float2 *>(address),
-            static_cast<float2>(*reinterpret_cast<float2 *>(val)));
-}
-// AtomicAdd Functions for FLOAT16x4
-TL_DEVICE void AtomicAddx4(float *address, float *val) {
-  atomicAdd(reinterpret_cast<float4 *>(address),
-            static_cast<float4>(*reinterpret_cast<float4 *>(val)));
-}
-#endif
-
 // DP4A
 template <typename InDatatype, typename OutDatatype>
-TL_DEVICE void DP4A(InDatatype *a, InDatatype *b, OutDatatype *c) {
+TL_DEVICE /**
+           * Compute a 4Ã—8-bit dot-product-accumulate using the CUDA DP4A
+           * intrinsic.
+           *
+           * Reads 32-bit packed values from `a` and `b` (each containing four
+           * signed 8-bit lanes), applies the __dp4a operation (dot product of
+           * the four lane pairs added to an accumulator), and stores the 32-bit
+           * integer result through `c`.
+           *
+           * @param a Pointer to a 32-bit packed input containing four signed
+           * 8-bit elements.
+           * @param b Pointer to a 32-bit packed input containing four signed
+           * 8-bit elements.
+           * @param c Pointer to a 32-bit accumulator; its current value is used
+           * as the initial accumulator and overwritten with the resulting int32
+           * sum.
+           */
+    void
+    DP4A(InDatatype *a, InDatatype *b, OutDatatype *c) {
   const int a_int = *((int *)a);
   const int b_int = *((int *)b);
   const int c_int = *((int *)c);
@@ -205,6 +169,101 @@ TL_DEVICE void DP4A(InDatatype *a, InDatatype *b, OutDatatype *c) {
 }
 
 namespace tl {
+/*!
+ * \brief PTX data type.
+ * \note
+ * PTX fundamental data types:
+ * https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#fundamental-types
+ * PTX matrix data types:
+ * https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-data-types
+ */
+enum class DataType : int {
+  kInt4 = 0,
+  kUInt4 = 1,
+  kInt8 = 2,
+  kUInt8 = 3,
+  kInt16 = 4,
+  kUInt16 = 5,
+  kInt32 = 6,
+  kUInt32 = 7,
+  kInt64 = 8,
+  kUInt64 = 9,
+  kFloat8_e4m3 = 10,
+  kFloat8_e5m2 = 11,
+  kFloat16 = 12,
+  kBFloat16 = 13,
+  kFloat16x2 = 14,
+  kFloat32 = 15,
+  kTensorFloat32 = 16,
+  kFloat64 = 17,
+  kBit1 = 18,
+  kBit8 = 19,
+  kBit16 = 20,
+  kBit32 = 21,
+  kBit64 = 22
+};
+
+union GmmaDescriptor {
+  CUTE_HOST_DEVICE constexpr GmmaDescriptor() noexcept : desc_(0) {}
+  CUTE_HOST_DEVICE constexpr GmmaDescriptor(uint64_t desc) noexcept
+      : desc_(desc) {}
+  CUTE_HOST_DEVICE constexpr GmmaDescriptor(GmmaDescriptor const &t) noexcept
+      : desc_(t.desc_) {}
+  CUTE_HOST_DEVICE constexpr GmmaDescriptor(GmmaDescriptor &&t) noexcept
+      : desc_(t.desc_) {}
+
+  CUTE_HOST_DEVICE constexpr GmmaDescriptor &
+  operator=(GmmaDescriptor const &t) noexcept {
+    desc_ = t.desc_;
+    return *this;
+  }
+
+  CUTE_HOST_DEVICE constexpr GmmaDescriptor &
+  operator=(GmmaDescriptor &&t) noexcept {
+    desc_ = t.desc_;
+    return *this;
+  }
+
+  uint64_t desc_;
+  uint32_t reg32_[2];
+  uint16_t reg16_[4];
+
+  // Bitfield implementation avoids the need for shifts in assignment
+  struct {
+    // start_address, bit [0,14), 4LSB not included
+    uint16_t start_address_ : 14, : 2; // 14 bits [0,14), 2 bits unused
+    // leading dimension byte offset, bit [16,30), 4LSB not included
+    // For N: This is the stride from the first col to the second col of the 8x2
+    // brick in INTERLEAVED
+    //   Unused for all SWIZZLE_* layouts (and assumed to be 1)
+    // For T: This is the stride from the first 8 rows to the next 8 rows.
+    uint16_t leading_byte_offset_ : 14, : 2; // 14 bits [0,14), 2 bits unused
+    // stride dimension byte offset, bit [32,46), 4LSB not included
+    // For N: This is the stride from the first 8 rows to the next 8 rows.
+    // For T: This is the stride fro mthe first 8 cols to the next 8 cols.
+    uint16_t stride_byte_offset_ : 14, : 2; // 14 bits [0,14), 2 bits unused
+    // base_offset, bit [49,52)
+    // Valid only for SWIZZLE_128B and SWIZZLE_64B
+    uint8_t : 1, base_offset_ : 3,
+        : 4; // 1 bit unused, 3 bits [1,4), 4 bits unused
+    // layout type, bit [62,64)
+    // SWIZZLE_NONE = 0, SWIZZLE_32B = 3, SWIZZLE_64B = 2, SWIZZLE_128B = 1
+    uint8_t : 6, layout_type_ : 2; // 6 bits unused, 2 bits [6,8)
+  } bitfield;
+
+  // Decay to a uint64_t
+  CUTE_HOST_DEVICE constexpr operator uint64_t() const noexcept {
+    return desc_;
+  }
+  template <typename T>
+  CUTE_HOST_DEVICE constexpr GmmaDescriptor operator+(const T &offset) const {
+    GmmaDescriptor ret;
+    ret.reg32_[0] = reg32_[0] + uint32_t(offset);
+    ret.reg32_[1] = reg32_[1];
+    return ret;
+  }
+};
+
 // Any
 template <typename T> TL_DEVICE bool Any(T *a, int size) {
   for (int i = 0; i < size; i++) {
@@ -236,9 +295,30 @@ template <int y = 1, typename T> TL_DEVICE T pow_of_int(T x) {
 
 // Thread partial barrier synchronization
 // https://docs.nvidia.com/cuda/parallel-thread-execution/#memory-consistency-model
-template <int barrier_id = 0, int thread_count = 0>
-TL_DEVICE void __sync_thread_partial() {
+TL_DEVICE void __sync_thread_partial(int barrier_id = 0, int thread_count = 0) {
   asm volatile("bar.sync %0, %1;" : : "r"(barrier_id), "r"(thread_count));
 }
+template <int layout_type = 0, int leading_byte_offset = 0,
+          int stride_byte_offset = 0, typename T>
+TL_DEVICE void initialize_descriptor(GmmaDescriptor &descriptor,
+                                     T *start_address) {
+  descriptor.bitfield.start_address_ =
+      cute::cast_smem_ptr_to_uint(start_address) >> 4;
+  descriptor.bitfield.layout_type_ = layout_type;
+  descriptor.bitfield.base_offset_ = 0;
+  descriptor.bitfield.leading_byte_offset_ = leading_byte_offset;
+  descriptor.bitfield.stride_byte_offset_ = stride_byte_offset;
+}
+
+template <typename T>
+TL_DEVICE void increase_descriptor_offset(GmmaDescriptor &descriptor,
+                                          T offset) {
+  descriptor.reg32_[0] += (offset >> 4);
+}
 
 } // namespace tl
+
+namespace cutlass {
+TL_DEVICE
+bfloat16_t fast_exp(bfloat16_t x) { return ::hexp(x); }
+} // namespace cutlass
diff --git a/src/tl_templates/cuda/compress_sm90.cu b/src/tl_templates/cuda/compress_sm90.cu
index 6635220c..8bb236dd 100644
--- a/src/tl_templates/cuda/compress_sm90.cu
+++ b/src/tl_templates/cuda/compress_sm90.cu
@@ -147,7 +147,7 @@ std::tuple<torch::Tensor, torch::Tensor> compress_impl(torch::Tensor A) {
       case torch::kChar:                                                                                   \
         return DISPATCH_BLOCK_K(int8_t, block_k, 2, A, TRANSPOSED);                                        \
       case torch::kByte:                                                                                   \
-        return DISPATCH_BLOCK_K(uint8_t, block_k, 2, A, TRANSPOSED);                                        \
+        return DISPATCH_BLOCK_K(uint8_t, block_k, 2, A, TRANSPOSED);                                       \
       default:                                                                                             \
         TORCH_CHECK(false, "Unsupported dtype");                                                           \
     }                                                                                                      \
diff --git a/src/tl_templates/cuda/copy.h b/src/tl_templates/cuda/copy.h
index bfb43055..df68287c 100644
--- a/src/tl_templates/cuda/copy.h
+++ b/src/tl_templates/cuda/copy.h
@@ -2,9 +2,14 @@
 
 #include "common.h"
 
-#if (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 900))
+#ifdef __CUDA_ARCH_LIST__
+#if __CUDA_ARCH_LIST__ >= 900
 #include "copy_sm90.h"
 #endif
+#if __CUDA_ARCH_LIST__ >= 1000
+#include "copy_sm100.h"
+#endif
+#endif
 
 namespace tl {
 
@@ -72,4 +77,329 @@ TL_DEVICE void cp_async_gs_conditional(void const *const smem_addr,
   }
 }
 
+template <int kBytes> struct VecInt {};
+template <> struct VecInt<1> {
+  using vec_t = int8_t;
+};
+template <> struct VecInt<2> {
+  using vec_t = int16_t;
+};
+template <> struct VecInt<4> {
+  using vec_t = int;
+};
+template <> struct VecInt<8> {
+  using vec_t = int64_t;
+};
+template <> struct VecInt<16> {
+  using vec_t = int4;
+};
+
+#define LD_NC_FUNC "ld.nc.global"
+#define ST_NA_FUNC "st.global"
+
+template <typename dtype_t> TL_DEVICE dtype_t ld_nc_global(const dtype_t *ptr) {
+  auto ret = ld_nc_global(
+      reinterpret_cast<const typename VecInt<sizeof(dtype_t)>::vec_t *>(ptr));
+  return *reinterpret_cast<dtype_t *>(&ret);
+}
+
+template <> TL_DEVICE uint8_t ld_nc_global(const uint8_t *ptr) {
+  uint16_t ret;
+  // NOTES: we must use `uint16_t` as inline ASM does not support 8-bit
+  // constraint letter (`h` below means unsigned 16-bit)
+  asm volatile(LD_NC_FUNC ".u8 %0, [%1];" : "=h"(ret) : "l"(ptr));
+  return static_cast<uint8_t>(ret);
+}
+
+template <> TL_DEVICE int16_t ld_nc_global(const int16_t *ptr) {
+  uint16_t ret;
+  asm volatile(LD_NC_FUNC ".s16 %0, [%1];" : "=h"(ret) : "l"(ptr));
+  return static_cast<int16_t>(ret);
+}
+
+template <> TL_DEVICE uint16_t ld_nc_global(const uint16_t *ptr) {
+  uint16_t ret;
+  asm volatile(LD_NC_FUNC ".u16 %0, [%1];" : "=h"(ret) : "l"(ptr));
+  return ret;
+}
+
+template <> TL_DEVICE int ld_nc_global(const int *ptr) {
+  int ret;
+  asm volatile(LD_NC_FUNC ".s32 %0, [%1];" : "=r"(ret) : "l"(ptr));
+  return ret;
+}
+
+template <> TL_DEVICE int64_t ld_nc_global(const int64_t *ptr) {
+  int64_t ret;
+  asm volatile(LD_NC_FUNC ".s64 %0, [%1];" : "=l"(ret) : "l"(ptr));
+  return ret;
+}
+
+template <> TL_DEVICE float ld_nc_global(const float *ptr) {
+  float ret;
+  asm volatile(LD_NC_FUNC ".f32 %0, [%1];" : "=f"(ret) : "l"(ptr));
+  return ret;
+}
+
+template <> TL_DEVICE int2 ld_nc_global(const int2 *ptr) {
+  int2 ret;
+  asm volatile(LD_NC_FUNC ".v2.s32 {%0, %1}, [%2];"
+               : "=r"(ret.x), "=r"(ret.y)
+               : "l"(ptr));
+  return ret;
+}
+
+template <> TL_DEVICE int4 ld_nc_global(const int4 *ptr) {
+  int4 ret;
+  asm volatile(LD_NC_FUNC ".v4.s32 {%0, %1, %2, %3}, [%4];"
+               : "=r"(ret.x), "=r"(ret.y), "=r"(ret.z), "=r"(ret.w)
+               : "l"(ptr));
+  return ret;
+}
+
+template <typename dtype_t>
+TL_DEVICE void st_na_global(const dtype_t *ptr, const dtype_t &value) {
+  st_na_global(
+      reinterpret_cast<const typename VecInt<sizeof(dtype_t)>::vec_t *>(ptr),
+      *reinterpret_cast<const typename VecInt<sizeof(dtype_t)>::vec_t *>(
+          &value));
+}
+
+template <>
+TL_DEVICE void st_na_global(const int16_t *ptr, const int16_t &value) {
+  asm volatile(ST_NA_FUNC ".s16 [%0], %1;" ::"l"(ptr), "h"(value));
+}
+
+template <>
+TL_DEVICE void st_na_global(const uint16_t *ptr, const uint16_t &value) {
+  asm volatile(ST_NA_FUNC ".u16 [%0], %1;" ::"l"(ptr), "h"(value));
+}
+
+template <> TL_DEVICE void st_na_global(const int *ptr, const int &value) {
+  asm volatile(ST_NA_FUNC ".s32 [%0], %1;" ::"l"(ptr), "r"(value));
+}
+
+template <>
+TL_DEVICE void st_na_global(const int64_t *ptr, const int64_t &value) {
+  asm volatile(ST_NA_FUNC ".s64 [%0], %1;" ::"l"(ptr), "l"(value));
+}
+
+template <> TL_DEVICE void st_na_global(const float *ptr, const float &value) {
+  asm volatile(ST_NA_FUNC ".f32 [%0], %1;" ::"l"(ptr), "f"(value));
+}
+
+template <> TL_DEVICE void st_na_global(const int4 *ptr, const int4 &value) {
+  asm volatile(ST_NA_FUNC ".v4.s32 [%0], {%1, %2, %3, %4};" ::"l"(ptr),
+               "r"(value.x), "r"(value.y), "r"(value.z), "r"(value.w));
+}
+
+#define LD_FUNC(ptr) ld_nc_global(ptr)
+#define ST_FUNC(ptr, value) st_na_global(ptr, value)
+
+template <int N, int UNROLL_FACTOR, typename dtype_t>
+TL_DEVICE void cp_warp_impl(dtype_t const *const dst_addr,
+                            dtype_t const *const src_addr) {
+  int lane_id;
+  asm("mov.s32 %0, %laneid;" : "=r"(lane_id));
+  constexpr int kLoopStride = 32 * (UNROLL_FACTOR);
+  typename std::remove_reference<decltype(LD_FUNC((src_addr) + 0))>::type
+      unrolled_values[(UNROLL_FACTOR)];
+  auto __src = (src_addr);
+  auto __dst = (dst_addr);
+  for (int __i = (lane_id); __i < ((N) / kLoopStride) * kLoopStride;
+       __i += kLoopStride) {
+    _Pragma("unroll") for (int __j = 0; __j < (UNROLL_FACTOR); ++__j)
+        unrolled_values[__j] = LD_FUNC(__src + __i + __j * 32);
+    _Pragma("unroll") for (int __j = 0; __j < (UNROLL_FACTOR); ++__j)
+        ST_FUNC(__dst + __i + __j * 32, unrolled_values[__j]);
+  }
+  for (int __i = ((N) / kLoopStride) * kLoopStride + (lane_id); __i < (N);
+       __i += 32)
+    ST_FUNC(__dst + __i, LD_FUNC(__src + __i));
+}
+
+/**
+ * @param enable_aggressive_vectorize If set to true, the copy will be performed
+ * with aggressive vectorization (e.g., using int4 for aligned and sized
+ * transfers), which requires that both source and destination addresses are
+ * 16-byte aligned and N*sizeof(dtype_t) is a multiple of 16 for optimal memory
+ * access and throughput. If false, performs a standard element-wise copy.
+ */
+// todo: support more auto-vectorize later
+template <int N, int UNROLL_FACTOR, bool enable_aggressive_vectorize = false,
+          typename dtype_t>
+TL_DEVICE void cp_warp(dtype_t const *const dst_addr,
+                       dtype_t const *const src_addr) {
+  if constexpr (enable_aggressive_vectorize) {
+    int4 *__restrict__ dst_addr_int4 = (int4 *)dst_addr;
+    const int4 *__restrict__ src_addr_int4 = (const int4 *)src_addr;
+    constexpr int N_int4 = sizeof(dtype_t) * N / 16;
+    cp_warp_impl<N_int4, UNROLL_FACTOR>(dst_addr_int4, src_addr_int4);
+  } else {
+    cp_warp_impl<N, UNROLL_FACTOR>(dst_addr, src_addr);
+  }
+}
+
+template <int N, int UNROLL_FACTOR, bool enable_aggressive_vectorize = false,
+          typename dtype_t>
+TL_DEVICE void cp_warp(uint64_t dst_addr_uint64,
+                       dtype_t const *const src_addr) {
+  dtype_t *dst_addr = reinterpret_cast<dtype_t *>(dst_addr_uint64);
+  if constexpr (enable_aggressive_vectorize) {
+    int4 *__restrict__ dst_addr_int4 = (int4 *)dst_addr;
+    const int4 *__restrict__ src_addr_int4 = (const int4 *)src_addr;
+    constexpr int N_int4 = sizeof(dtype_t) * N / 16;
+    cp_warp_impl<N_int4, UNROLL_FACTOR>(dst_addr_int4, src_addr_int4);
+  } else {
+    cp_warp_impl<N, UNROLL_FACTOR>(dst_addr, src_addr);
+  }
+}
+
+template <int N, int UNROLL_FACTOR, bool enable_aggressive_vectorize = false,
+          typename dtype_t>
+TL_DEVICE void cp_warp(dtype_t *const dst_addr, uint64_t src_addr_uint64) {
+  const dtype_t *src_addr = reinterpret_cast<const dtype_t *>(src_addr_uint64);
+  if constexpr (enable_aggressive_vectorize) {
+    int4 *__restrict__ dst_addr_int4 = (int4 *)dst_addr;
+    const int4 *__restrict__ src_addr_int4 = (const int4 *)src_addr;
+    constexpr int N_int4 = sizeof(dtype_t) * N / 16;
+    cp_warp_impl<N_int4, UNROLL_FACTOR>(dst_addr_int4, src_addr_int4);
+  } else {
+    cp_warp_impl<N, UNROLL_FACTOR>(dst_addr, src_addr);
+  }
+}
+
+/**
+ * Check:
+ * nvshmem_src/src/include/non_abi/device/common/nvshmemi_common_device.cuh::nvshmemi_memcpy_threadgroup()
+ */
+template <int N, typename dtype_t>
+TL_DEVICE void nvshmem_cp_threadgroup(dtype_t *__restrict__ _dst,
+                                      const dtype_t *__restrict__ _src,
+                                      int myIdx, int groupSize) {
+  size_t len = N * sizeof(dtype_t);
+  void *dst = _dst;
+  const void *src = _src;
+
+  /*
+   * If src and dst are 16B aligned copy as much as possible using 16B chunks
+   */
+  if ((uintptr_t)dst % 16 == 0 && (uintptr_t)src % 16 == 0) {
+    const size_t nelems = len / 16;
+
+    int4 *__restrict__ dst_p = (int4 *)dst;
+    const int4 *__restrict__ src_p = (const int4 *)src;
+    for (size_t i = myIdx; i < nelems; i += groupSize)
+      dst_p[i] = src_p[i];
+
+    len -= nelems * 16;
+
+    if (0 == len)
+      return;
+
+    dst = (void *)(dst_p + nelems);
+    src = (void *)(src_p + nelems);
+  }
+
+  /*
+   * If src and dst are 8B aligned copy as much as possible using 8B chunks
+   */
+  if ((uintptr_t)dst % 8 == 0 && (uintptr_t)src % 8 == 0) {
+    uint64_t *__restrict__ dst_p = (uint64_t *)dst;
+    const uint64_t *__restrict__ src_p = (const uint64_t *)src;
+    const size_t nelems = len / 8;
+
+    for (size_t i = myIdx; i < nelems; i += groupSize)
+      dst_p[i] = src_p[i];
+
+    len -= nelems * 8;
+
+    if (0 == len)
+      return;
+
+    dst = (void *)(dst_p + nelems);
+    src = (void *)(src_p + nelems);
+  }
+
+  /*
+   * If src and dst are 4B aligned copy as much as possible using 4B chunks
+   */
+  if ((uintptr_t)dst % 4 == 0 && (uintptr_t)src % 4 == 0) {
+    uint32_t *__restrict__ dst_p = (uint32_t *)dst;
+    const uint32_t *__restrict__ src_p = (const uint32_t *)src;
+    const size_t nelems = len / 4;
+
+    for (size_t i = myIdx; i < nelems; i += groupSize)
+      dst_p[i] = src_p[i];
+
+    len -= nelems * 4;
+
+    if (0 == len)
+      return;
+
+    dst = (void *)(dst_p + nelems);
+    src = (void *)(src_p + nelems);
+  }
+
+  /*
+   * If src and dst are 2B aligned copy as much as possible using 2B chunks
+   */
+  if ((uintptr_t)dst % 2 == 0 && (uintptr_t)src % 2 == 0) {
+    uint16_t *__restrict__ dst_p = (uint16_t *)dst;
+    const uint16_t *__restrict__ src_p = (const uint16_t *)src;
+    const size_t nelems = len / 2;
+
+    for (size_t i = myIdx; i < nelems; i += groupSize)
+      dst_p[i] = src_p[i];
+
+    len -= nelems * 2;
+
+    if (0 == len)
+      return;
+
+    dst = (void *)(dst_p + nelems);
+    src = (void *)(src_p + nelems);
+  }
+
+  unsigned char *__restrict__ dst_c = (unsigned char *)dst;
+  const unsigned char *__restrict__ src_c = (const unsigned char *)src;
+
+  for (size_t i = myIdx; i < len; i += groupSize)
+    dst_c[i] = src_c[i];
+}
+
+template <int N, typename dtype_t>
+TL_DEVICE void nvshmem_cp_warp(dtype_t *__restrict__ dst,
+                               const dtype_t *__restrict__ src) {
+  int lane_id;
+  asm("mov.s32 %0, %laneid;" : "=r"(lane_id));
+  nvshmem_cp_threadgroup<N>(dst, src, lane_id, 32);
+}
+
+template <int N, typename dtype_t>
+TL_DEVICE void nvshmem_cp_block(dtype_t *__restrict__ dst,
+                                const dtype_t *__restrict__ src) {
+  int thread_id = threadIdx.x + threadIdx.y * blockDim.x +
+                  threadIdx.z * blockDim.x * blockDim.y;
+  int block_size = blockDim.x * blockDim.y * blockDim.z;
+  nvshmem_cp_threadgroup<N>(dst, src, thread_id, block_size);
+}
+
+template <int N, typename dtype_t>
+TL_DEVICE void cp_block(dtype_t *dst_addr, const dtype_t *src_addr) {
+  nvshmem_cp_block<N>(dst_addr, src_addr);
+}
+
+template <int N, typename dtype_t>
+TL_DEVICE void cp_block(uint64_t dst_addr_uint64, const dtype_t *src_addr) {
+  dtype_t *dst_addr = reinterpret_cast<dtype_t *>(dst_addr_uint64);
+  nvshmem_cp_block<N>(dst_addr, src_addr);
+}
+
+template <int N, typename dtype_t>
+TL_DEVICE void cp_block(dtype_t *dst_addr, const uint64_t src_addr_uint64) {
+  const dtype_t *src_addr = reinterpret_cast<const dtype_t *>(src_addr_uint64);
+  nvshmem_cp_block<N>(dst_addr, src_addr);
+}
+
 } // namespace tl
diff --git a/src/tl_templates/cuda/copy_sm100.h b/src/tl_templates/cuda/copy_sm100.h
new file mode 100644
index 00000000..c4047c34
--- /dev/null
+++ b/src/tl_templates/cuda/copy_sm100.h
@@ -0,0 +1,134 @@
+#pragma once
+#include "cuda_fp8.h"
+#include "tcgen_05.h"
+#include "tcgen_05_ld.h"
+
+namespace tl {
+
+__device__ __forceinline__ longlong4 ld_global_256(const longlong4 *ptr) {
+  longlong4 ret;
+  asm volatile("ld.global.v4.s64 {%0, %1, %2, %3}, [%4];"
+               : "=l"(ret.x), "=l"(ret.y), "=l"(ret.z), "=l"(ret.w)
+               : "l"(ptr));
+  return ret;
+}
+
+__device__ __forceinline__ void st_global_256(longlong4 *ptr, longlong4 &val) {
+  asm volatile("st.global.v4.s64 [%0], {%1, %2, %3, %4};"
+               :
+               : "l"(ptr), "l"(val.x), "l"(val.y), "l"(val.z), "l"(val.w));
+}
+
+__device__ __forceinline__ ulonglong4 ld_global_256(const ulonglong4 *ptr) {
+  ulonglong4 ret;
+  asm volatile("ld.global.v4.u64 {%0, %1, %2, %3}, [%4];"
+               : "=l"(ret.x), "=l"(ret.y), "=l"(ret.z), "=l"(ret.w)
+               : "l"(ptr));
+  return ret;
+}
+
+// must be const &val, otherwise the compiler will generate a temporary variable
+// and compilation will fail if we have st_global_256(ptr, ld_global_256(ptr))
+__device__ __forceinline__ void st_global_256(ulonglong4 *ptr,
+                                              const ulonglong4 &val) {
+  asm volatile("st.global.v4.u64 [%0], {%1, %2, %3, %4};"
+               :
+               : "l"(ptr), "l"(val.x), "l"(val.y), "l"(val.z), "l"(val.w));
+}
+
+__device__ __forceinline__ ulonglong4 ld_global_256(const fp8_e4_32_t *ptr) {
+  ulonglong4 ret;
+  asm volatile("ld.global.v4.u64 {%0, %1, %2, %3}, [%4];"
+               : "=l"(ret.x), "=l"(ret.y), "=l"(ret.z), "=l"(ret.w)
+               : "l"(ptr));
+  return ret;
+}
+
+__device__ __forceinline__ void st_global_256(fp8_e4_32_t *ptr,
+                                              fp8_e4_32_t &val8) {
+  ulonglong4 &val = *((ulonglong4 *)&val8);
+  asm volatile("st.global.v4.u64 [%0], {%1, %2, %3, %4};"
+               :
+               : "l"(ptr), "l"(val.x), "l"(val.y), "l"(val.z), "l"(val.w));
+}
+
+__device__ __forceinline__ unsigned long long
+pack_bfloat16x4(const bfloat16_t x, const bfloat16_t y, const bfloat16_t z,
+                const bfloat16_t w) {
+  unsigned long long v0 = *((unsigned short *)&x);
+  unsigned long long v1 = *((unsigned short *)&y);
+  unsigned long long v2 = *((unsigned short *)&z);
+  unsigned long long v3 = *((unsigned short *)&w);
+  return (v0 | (v1 << 16) | (v2 << 32) | (v3 << 48));
+}
+
+__device__ __forceinline__ unsigned long long
+pack_float16x4(const half x, const half y, const half z, const half w) {
+  unsigned long long v0 = *((unsigned short *)&x);
+  unsigned long long v1 = *((unsigned short *)&y);
+  unsigned long long v2 = *((unsigned short *)&z);
+  unsigned long long v3 = *((unsigned short *)&w);
+  return (v0 | (v1 << 16) | (v2 << 32) | (v3 << 48));
+}
+
+// Helper function to find the largest K that 2**K <= N
+// Requires N > 0
+template <int N, int K = 0>
+__device__ __forceinline__ constexpr int get_floor_log2() {
+  static_assert(N > 0);
+  if constexpr ((1 << (K + 1)) > N)
+    return K;
+  else
+    return get_floor_log2<N, K + 1>();
+}
+
+template <typename target_call_cls, int MAX_LOGN, int N, typename dst_t>
+__device__ __forceinline__ void tcgen05_ld_core(uint32_t const &tmem_start_col,
+                                                dst_t *dst_ptr) {
+  static_assert(N > 0);
+  constexpr int LOG_N = get_floor_log2<N>();
+  constexpr int CUR_SEGMENT_LEN = 1 << (LOG_N > MAX_LOGN ? MAX_LOGN : LOG_N);
+  target_call_cls::copy<CUR_SEGMENT_LEN>(tmem_start_col, (uint32_t *)dst_ptr);
+  if constexpr (N - CUR_SEGMENT_LEN > 0) {
+    tcgen05_ld_core<target_call_cls, MAX_LOGN, N - CUR_SEGMENT_LEN>(
+        tmem_start_col + CUR_SEGMENT_LEN, dst_ptr + CUR_SEGMENT_LEN);
+  }
+}
+
+template <int N, typename dst_t>
+__device__ __forceinline__ void
+tcgen05_ld_32dp32bNx(uint32_t const &tmem_start_col,
+                     uint32_t const &tmem_col_offset, dst_t *dst_ptr) {
+  tcgen05_ld_core<tl::tmem_ld_32dp32bNx, 7, N>(tmem_start_col + tmem_col_offset,
+                                               dst_ptr);
+  tl::fence_view_async_tmem_load();
+}
+
+template <int N, typename dst_t>
+__device__ __forceinline__ void
+tcgen05_ld_32dp64bNx(uint32_t const &tmem_start_col,
+                     uint32_t const &tmem_col_offset, dst_t *dst_ptr) {
+  tcgen05_ld_core<tl::tmem_ld_32dp64bNx, 7, N>(tmem_start_col + tmem_col_offset,
+                                               dst_ptr);
+  tl::fence_view_async_tmem_load();
+}
+
+template <int N, typename dst_t>
+__device__ __forceinline__ void
+tcgen05_ld_32dp128bNx(uint32_t const &tmem_start_col,
+                      uint32_t const &tmem_col_offset, dst_t *dst_ptr) {
+  tcgen05_ld_core<tl::tmem_ld_32dp128bNx, 6, N>(
+      tmem_start_col + tmem_col_offset, dst_ptr);
+  tl::fence_view_async_tmem_load();
+}
+
+template <int N, typename dst_t>
+__device__ __forceinline__ void
+tcgen05_ld_32dp256bNx(uint32_t const &tmem_start_col,
+                      uint32_t const &tmem_col_offset, dst_t *dst_ptr) {
+  tcgen05_ld_core<tl::tmem_ld_32dp256bNx, 5, N>(
+      tmem_start_col + tmem_col_offset, dst_ptr);
+  tl::fence_view_async_tmem_load();
+}
+
+} // namespace tl
diff --git a/src/tl_templates/cuda/copy_sm90.h b/src/tl_templates/cuda/copy_sm90.h
index 10f9bc1e..b8b174dc 100644
--- a/src/tl_templates/cuda/copy_sm90.h
+++ b/src/tl_templates/cuda/copy_sm90.h
@@ -4,13 +4,21 @@
 #include <cuda.h>
 #endif
 
+#include "barrier.h"
 #include "common.h"
 
 namespace tl {
-
-TL_DEVICE void tma_load(void *smem_ptr, void *gmem_ptr, uint64_t &smem_mbar,
+enum class CacheHintSm90 : uint64_t {
+  EVICT_NORMAL = 0x1000000000000000,
+  EVICT_FIRST = 0x12F0000000000000,
+  EVICT_LAST = 0x14F0000000000000,
+};
+
+template <typename BarrierType = uint64_t>
+TL_DEVICE void tma_load(void *smem_ptr, void *gmem_ptr, BarrierType &smem_mbar,
                         uint32_t size) {
-  uint32_t smem_int_mbar = smem_ptr_to_uint(&smem_mbar);
+  uint32_t smem_int_mbar =
+      smem_ptr_to_uint(reinterpret_cast<uint64_t *>(&smem_mbar));
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
   asm volatile("cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::"
                "bytes [%0], [%1], %2, [%3]; \n" ::"r"(smem_int_ptr),
@@ -30,263 +38,233 @@ TL_DEVICE void tma_load_multicast(void *smem_ptr, void *gmem_ptr,
       :);
 }
 
-TL_DEVICE void tma_load(const CUtensorMap &descriptor, uint64_t &smem_mbar,
+template <CacheHintSm90 cache_hint = CacheHintSm90::EVICT_NORMAL,
+          typename BarrierType = uint64_t>
+TL_DEVICE void tma_load(const CUtensorMap &descriptor, BarrierType &smem_mbar,
                         void const *const smem_ptr, int32_t const &crd0) {
   uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(&descriptor);
-  uint32_t smem_int_mbar = smem_ptr_to_uint(&smem_mbar);
+  uint32_t smem_int_mbar;
+  if constexpr (std::is_pointer_v<BarrierType>) {
+    smem_int_mbar = smem_ptr_to_uint(reinterpret_cast<uint64_t *>(smem_mbar));
+  } else {
+    smem_int_mbar = smem_ptr_to_uint(reinterpret_cast<uint64_t *>(&smem_mbar));
+  }
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
   asm volatile("cp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::"
-               "complete_tx::bytes"
-               " [%0], [%1, {%3}], [%2];"
+               "complete_tx::bytes.L2::cache_hint"
+               " [%0], [%1, {%3}], [%2], %4;"
                :
                : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
-                 "r"(crd0)
+                 "r"(crd0), "l"(cache_hint)
                : "memory");
 }
 
-TL_DEVICE void tma_load(const CUtensorMap &descriptor, uint64_t &smem_mbar,
+template <CacheHintSm90 cache_hint = CacheHintSm90::EVICT_NORMAL,
+          typename BarrierType = uint64_t>
+TL_DEVICE void tma_load(const CUtensorMap &descriptor, BarrierType &smem_mbar,
                         void const *const smem_ptr, int32_t const &crd0,
                         int32_t const &crd1) {
   uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(&descriptor);
-  uint32_t smem_int_mbar = smem_ptr_to_uint(&smem_mbar);
+  uint32_t smem_int_mbar;
+  if constexpr (std::is_pointer_v<BarrierType>) {
+    smem_int_mbar = smem_ptr_to_uint(reinterpret_cast<uint64_t *>(smem_mbar));
+  } else {
+    smem_int_mbar = smem_ptr_to_uint(reinterpret_cast<uint64_t *>(&smem_mbar));
+  }
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
   asm volatile("cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::"
-               "complete_tx::bytes"
-               " [%0], [%1, {%3, %4}], [%2];"
+               "complete_tx::bytes.L2::cache_hint"
+               " [%0], [%1, {%3, %4}], [%2], %5;"
                :
                : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
-                 "r"(crd0), "r"(crd1)
+                 "r"(crd0), "r"(crd1), "l"(cache_hint)
                : "memory");
 }
 
-TL_DEVICE void tma_load(const CUtensorMap &descriptor, uint64_t &smem_mbar,
+template <CacheHintSm90 cache_hint = CacheHintSm90::EVICT_NORMAL,
+          typename BarrierType = uint64_t>
+TL_DEVICE void tma_load(const CUtensorMap &descriptor, BarrierType &smem_mbar,
                         void const *const smem_ptr, int32_t const &crd0,
                         int32_t const &crd1, int32_t const &crd2) {
   uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(&descriptor);
-  uint32_t smem_int_mbar = smem_ptr_to_uint(&smem_mbar);
+  uint32_t smem_int_mbar;
+  if constexpr (std::is_pointer_v<BarrierType>) {
+    smem_int_mbar = smem_ptr_to_uint(reinterpret_cast<uint64_t *>(smem_mbar));
+  } else {
+    smem_int_mbar = smem_ptr_to_uint(reinterpret_cast<uint64_t *>(&smem_mbar));
+  }
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
   asm volatile("cp.async.bulk.tensor.3d.shared::cluster.global.mbarrier::"
-               "complete_tx::bytes"
-               " [%0], [%1, {%3, %4, %5}], [%2];"
+               "complete_tx::bytes.L2::cache_hint"
+               " [%0], [%1, {%3, %4, %5}], [%2], %6;"
                :
                : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
-                 "r"(crd0), "r"(crd1), "r"(crd2)
+                 "r"(crd0), "r"(crd1), "r"(crd2), "l"(cache_hint)
                : "memory");
 }
-
-TL_DEVICE void tma_load(const CUtensorMap &descriptor, uint64_t &smem_mbar,
+template <CacheHintSm90 cache_hint = CacheHintSm90::EVICT_NORMAL,
+          typename BarrierType = uint64_t>
+TL_DEVICE void tma_load(const CUtensorMap &descriptor, BarrierType &smem_mbar,
                         void const *const smem_ptr, int32_t const &crd0,
                         int32_t const &crd1, int32_t const &crd2,
                         int32_t const &crd3) {
   uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(&descriptor);
-  uint32_t smem_int_mbar = smem_ptr_to_uint(&smem_mbar);
+  uint32_t smem_int_mbar;
+  if constexpr (std::is_pointer_v<BarrierType>) {
+    smem_int_mbar = smem_ptr_to_uint(reinterpret_cast<uint64_t *>(smem_mbar));
+  } else {
+    smem_int_mbar = smem_ptr_to_uint(reinterpret_cast<uint64_t *>(&smem_mbar));
+  }
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
   asm volatile("cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier::"
-               "complete_tx::bytes"
-               " [%0], [%1, {%3, %4, %5, %6}], [%2];"
+               "complete_tx::bytes.L2::cache_hint"
+               " [%0], [%1, {%3, %4, %5, %6}], [%2], %7;"
                :
                : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
-                 "r"(crd0), "r"(crd1), "r"(crd2), "r"(crd3)
+                 "r"(crd0), "r"(crd1), "r"(crd2), "r"(crd3), "l"(cache_hint)
                : "memory");
 }
 
-TL_DEVICE void tma_load(const CUtensorMap &descriptor, uint64_t &smem_mbar,
+template <CacheHintSm90 cache_hint = CacheHintSm90::EVICT_NORMAL,
+          typename BarrierType = uint64_t>
+TL_DEVICE void tma_load(const CUtensorMap &descriptor, BarrierType &smem_mbar,
                         void const *const smem_ptr, int32_t const &crd0,
                         int32_t const &crd1, int32_t const &crd2,
                         int32_t const &crd3, int32_t const &crd4) {
   uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(&descriptor);
-  uint32_t smem_int_mbar = smem_ptr_to_uint(&smem_mbar);
+  uint32_t smem_int_mbar;
+  if constexpr (std::is_pointer_v<BarrierType>) {
+    smem_int_mbar = smem_ptr_to_uint(reinterpret_cast<uint64_t *>(smem_mbar));
+  } else {
+    smem_int_mbar = smem_ptr_to_uint(reinterpret_cast<uint64_t *>(&smem_mbar));
+  }
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
   asm volatile("cp.async.bulk.tensor.5d.shared::cluster.global.mbarrier::"
-               "complete_tx::bytes"
-               " [%0], [%1, {%3, %4, %5, %6, %7}], [%2];"
+               "complete_tx::bytes.L2::cache_hint"
+               " [%0], [%1, {%3, %4, %5, %6, %7}], [%2], %8;"
                :
                : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
-                 "r"(crd0), "r"(crd1), "r"(crd2), "r"(crd3), "r"(crd4)
+                 "r"(crd0), "r"(crd1), "r"(crd2), "r"(crd3), "r"(crd4),
+                 "l"(cache_hint)
                : "memory");
 }
 
-TL_DEVICE void tma_load_im2col(const CUtensorMap &descriptor,
-                               uint64_t &smem_mbar, void const *const smem_ptr,
-                               int32_t const &coord_c, int32_t const &coord_w,
-                               int32_t const &coord_h, int32_t const &coord_n,
-                               uint16_t const &offset_w,
-                               uint16_t const &offset_h) {
+template <CacheHintSm90 cache_hint = CacheHintSm90::EVICT_NORMAL,
+          typename BarrierType = uint64_t>
+TL_DEVICE void
+tma_load_im2col(const CUtensorMap &descriptor, BarrierType &smem_mbar,
+                void const *const smem_ptr, int32_t const &coord_c,
+                int32_t const &coord_w, int32_t const &coord_h,
+                int32_t const &coord_n, uint16_t const &offset_w,
+                uint16_t const &offset_h) {
   uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(&descriptor);
-  uint32_t smem_int_mbar = smem_ptr_to_uint(&smem_mbar);
+  uint32_t smem_int_mbar =
+      smem_ptr_to_uint(reinterpret_cast<uint64_t *>(&smem_mbar));
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
   asm volatile("cp.async.bulk.tensor.4d.shared::cluster.global.im2col.mbarrier:"
-               ":complete_tx::bytes"
-               " [%0], [%1, {%3, %4, %5, %6}], [%2], {%7, %8};"
+               ":complete_tx::bytes.L2::cache_hint"
+               " [%0], [%1, {%3, %4, %5, %6}], [%2], {%7, %8}, %9;"
                :
                : "r"(smem_int_ptr), "l"(gmem_int_desc), "r"(smem_int_mbar),
                  "r"(coord_c), "r"(coord_w), "r"(coord_h), "r"(coord_n),
-                 "h"(offset_w), "h"(offset_h)
+                 "h"(offset_w), "h"(offset_h), "l"(cache_hint)
                : "memory");
 }
 
-TL_DEVICE void tma_store(void *dst_gmem_ptr, void *smem_ptr, uint32_t size) {
+template <CacheHintSm90 cache_hint = CacheHintSm90::EVICT_NORMAL>
+TL_DEVICE void tma_store(void *gmem_ptr, void *smem_ptr, uint32_t size) {
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
-  asm volatile(
-      "cp.async.bulk.global.shared::cta.bulk_group [%1], [%0], %2; \n" ::"r"(
-          smem_int_ptr),
-      "l"(dst_gmem_ptr), "r"(size)
-      :);
+  asm volatile("cp.async.bulk.global.shared::cta.bulk_group"
+               ".L2::cache_hint [%0], [%1], %2, %3;"
+               :
+               : "l"(gmem_ptr), "r"(smem_int_ptr), "r"(size), "l"(cache_hint)
+               :);
 }
 
+template <CacheHintSm90 cache_hint = CacheHintSm90::EVICT_NORMAL>
 TL_DEVICE void tma_store(const CUtensorMap &descriptor,
                          void const *const smem_ptr, int32_t const &crd0) {
   uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(&descriptor);
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
-
-  asm volatile(
-      "cp.async.bulk.tensor.1d.global.shared::cta.bulk_group [%0, {%2}], [%1];"
-      :
-      : "l"(gmem_int_desc), "r"(smem_int_ptr), "r"(crd0)
-      : "memory");
+  asm volatile("cp.async.bulk.tensor.1d.global.shared::cta.bulk_group "
+               ".L2::cache_hint [%0, {%2}], [%1], %3;"
+               :
+               : "l"(gmem_int_desc), "r"(smem_int_ptr), "r"(crd0),
+                 "l"(cache_hint)
+               : "memory");
 }
 
+template <CacheHintSm90 cache_hint = CacheHintSm90::EVICT_NORMAL>
 TL_DEVICE void tma_store(const CUtensorMap &descriptor,
                          void const *const smem_ptr, int32_t const &crd0,
                          int32_t const &crd1) {
   uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(&descriptor);
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
-
-  asm volatile("cp.async.bulk.tensor.2d.global.shared::cta.bulk_group [%0, "
-               "{%2, %3}], [%1];"
+  asm volatile("cp.async.bulk.tensor.2d.global.shared::cta.bulk_group "
+               ".L2::cache_hint [%0, {%2, %3}], [%1], %4;"
                :
-               : "l"(gmem_int_desc), "r"(smem_int_ptr), "r"(crd0), "r"(crd1)
+               : "l"(gmem_int_desc), "r"(smem_int_ptr), "r"(crd0), "r"(crd1),
+                 "l"(cache_hint)
                : "memory");
 }
 
+template <CacheHintSm90 cache_hint = CacheHintSm90::EVICT_NORMAL>
 TL_DEVICE void tma_store(const CUtensorMap &descriptor,
                          void const *const smem_ptr, int32_t const &crd0,
                          int32_t const &crd1, int32_t const &crd2) {
   uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(&descriptor);
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
-
-  asm volatile("cp.async.bulk.tensor.3d.global.shared::cta.bulk_group [%0, "
-               "{%2, %3, %4}], [%1];"
+  asm volatile("cp.async.bulk.tensor.3d.global.shared::cta.bulk_group "
+               ".L2::cache_hint [%0, {%2, %3, %4}], [%1], %5;"
                :
                : "l"(gmem_int_desc), "r"(smem_int_ptr), "r"(crd0), "r"(crd1),
-                 "r"(crd2)
+                 "r"(crd2), "l"(cache_hint)
                : "memory");
 }
 
+template <CacheHintSm90 cache_hint = CacheHintSm90::EVICT_NORMAL>
 TL_DEVICE void tma_store(const CUtensorMap &descriptor,
                          void const *const smem_ptr, int32_t const &crd0,
                          int32_t const &crd1, int32_t const &crd2,
                          int32_t const &crd3) {
   uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(&descriptor);
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
-
-  asm volatile("cp.async.bulk.tensor.4d.global.shared::cta.bulk_group [%0, "
-               "{%2, %3, %4, %5}], [%1];"
+  asm volatile("cp.async.bulk.tensor.4d.global.shared::cta.bulk_group "
+               ".L2::cache_hint [%0, {%2, %3, %4, %5}], [%1], %6;"
                :
                : "l"(gmem_int_desc), "r"(smem_int_ptr), "r"(crd0), "r"(crd1),
-                 "r"(crd2), "r"(crd3)
+                 "r"(crd2), "r"(crd3), "l"(cache_hint)
                : "memory");
 }
 
+template <CacheHintSm90 cache_hint = CacheHintSm90::EVICT_NORMAL>
 TL_DEVICE void tma_store(const CUtensorMap &descriptor,
                          void const *const smem_ptr, int32_t const &crd0,
                          int32_t const &crd1, int32_t const &crd2,
                          int32_t const &crd3, int32_t const &crd4) {
   uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(&descriptor);
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
-
-  asm volatile("cp.async.bulk.tensor.5d.global.shared::cta.bulk_group [%0, "
-               "{%2, %3, %4, %5, %6}], [%1];"
+  asm volatile("cp.async.bulk.tensor.5d.global.shared::cta.bulk_group "
+               ".L2::cache_hint [%0, {%2, %3, %4, %5, %6}], [%1], %7;"
                :
                : "l"(gmem_int_desc), "r"(smem_int_ptr), "r"(crd0), "r"(crd1),
-                 "r"(crd2), "r"(crd3), "r"(crd4)
+                 "r"(crd2), "r"(crd3), "r"(crd4), "l"(cache_hint)
                : "memory");
 }
 
-TL_DEVICE void prefetch_tma_descriptor(const CUtensorMap &descriptor) {
-  uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(&descriptor);
-  asm volatile("prefetch.tensormap [%0];" : : "l"(gmem_int_desc) : "memory");
-}
-
-TL_DEVICE void mbarrier_init(uint64_t &smem_barrier, uint32_t arrive_count) {
-  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
-  asm volatile("mbarrier.init.shared.b64 [%1], %0;"
-               :
-               : "r"(arrive_count), "r"(smem_int_ptr));
-}
-
-TL_DEVICE void mbarrier_wait(uint64_t &smem_barrier, int phase_bit) {
-  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
-  asm volatile("{\n"
-               ".reg .pred                P1;\n"
-               "LAB_WAIT:\n"
-               "mbarrier.try_wait.parity.shared.b64 P1, [%0], %1;\n"
-               "@!P1                      bra.uni LAB_WAIT;\n"
-               "}\n" ::"r"(smem_int_ptr),
-               "r"(phase_bit));
-}
-
-TL_DEVICE void mbarrier_arrive(uint64_t &smem_barrier) {
-  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
-  asm volatile("mbarrier.arrive.shared.b64 _, [%0];" : : "r"(smem_int_ptr));
-}
-
-TL_DEVICE void mbarrier_expect_tx(uint64_t &smem_barrier,
-                                  uint32_t transaction_bytes) {
-  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
-  asm volatile("mbarrier.expect_tx.shared.b64 [%1], %0;"
-               :
-               : "r"(transaction_bytes), "r"(smem_int_ptr));
-}
-
-TL_DEVICE void mbarrier_arrive_expect_tx(uint64_t &smem_barrier,
-                                         uint32_t transaction_bytes) {
-  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
-  asm volatile("mbarrier.arrive.expect_tx.shared.b64 _, [%1], %0;"
-               :
-               : "r"(transaction_bytes), "r"(smem_int_ptr));
-}
-
-TL_DEVICE void mbarrier_cp_async_arrive(uint64_t &smem_barrier) {
-  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
-  asm volatile("cp.async.mbarrier.arrive.shared.b64 [%0];"
-               :
-               : "r"(smem_int_ptr));
-}
-
-TL_DEVICE void fence_proxy_async() {
-  asm volatile("fence.proxy.async.shared::cta;" : :);
-}
-
-// Indicate arrival of warp issuing TMA_STORE
-TL_DEVICE void tma_store_arrive() {
-  asm volatile("cp.async.bulk.commit_group;");
-}
-
-template <int Count> TL_DEVICE void tma_store_wait() {
-  asm volatile("cp.async.bulk.wait_group.read %0;" : : "n"(Count) : "memory");
-}
-
-TL_DEVICE void syncthreads_partial(uint64_t &smem_barrier) {
-  uint32_t smem_int_ptr = smem_ptr_to_uint(&smem_barrier);
-  uint64_t state = 0;
-  asm volatile("{\n"
-               ".reg .pred                P1;\n"
-               "mbarrier.arrive.shared.b64 %1, [%0];\n"
-               "LAB_WAIT:\n"
-               "mbarrier.try_wait.shared.b64 P1, [%0], %1;\n"
-               "@!P1                      bra.uni LAB_WAIT;\n"
-               "}\n"
+TL_DEVICE void tma_store_add(float *const smem_ptr, float *gmem_ptr,
+                             int32_t const &store_bytes) {
+  uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
+  asm volatile("cp.reduce.async.bulk.global.shared::cta.bulk_group.add.f32 "
+               "[%0], [%1], %2;\n"
                :
-               : "r"(smem_int_ptr), "l"(state));
-}
-
-template <uint32_t RegCount> TL_DEVICE void warpgroup_reg_alloc() {
-  asm volatile("setmaxnreg.inc.sync.aligned.u32 %0;\n" : : "n"(RegCount));
+               : "l"(gmem_ptr), "r"(smem_int_ptr), "r"(store_bytes)
+               : "memory");
 }
 
-template <uint32_t RegCount> TL_DEVICE void warpgroup_reg_dealloc() {
-  asm volatile("setmaxnreg.dec.sync.aligned.u32 %0;\n" : : "n"(RegCount));
+TL_DEVICE void prefetch_tma_descriptor(const CUtensorMap &descriptor) {
+  uint64_t gmem_int_desc = reinterpret_cast<uint64_t>(&descriptor);
+  asm volatile("prefetch.tensormap [%0];" : : "l"(gmem_int_desc) : "memory");
 }
 
-} // namespace tl
\ No newline at end of file
+} // namespace tl
diff --git a/src/tl_templates/cuda/cuda_bf16_fallbacks.cuh b/src/tl_templates/cuda/cuda_bf16_fallbacks.cuh
new file mode 100644
index 00000000..f5641f61
--- /dev/null
+++ b/src/tl_templates/cuda/cuda_bf16_fallbacks.cuh
@@ -0,0 +1,257 @@
+// Downloaded from from FasterTransformer v5.2.1
+// https://github.com/NVIDIA/FasterTransformer/blob/release/v5.2.1_tag/src/fastertransformer/utils/cuda_bf16_fallbacks.cuh
+/*
+ * Copyright (c) 2019-2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "cuda_bf16_wrapper.h"
+#include <cuda_fp16.h>
+
+namespace fastertransformer {
+
+#ifdef ENABLE_BF16
+inline __device__ float2 bf1622float2(const __nv_bfloat162 val) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    float2 f_val;
+    f_val.x = __low2float(val);
+    f_val.y = __high2float(val);
+    return f_val;
+#else
+    return __bfloat1622float2(val);
+#endif
+}
+
+inline __device__ int16_t bf1622int16(__nv_bfloat162 val) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    float2 f_val;
+    f_val.x = max(min(__low2float(val), 127.f), -128.f);
+    f_val.y = max(min(__high2float(val), 127.f), -128.f);
+    union { int8_t int8[2]; int16_t int16; };
+    int8[0] = static_cast<int8_t>(static_cast<short>(f_val.x));
+    int8[1] = static_cast<int8_t>(static_cast<short>(f_val.y));
+    return int16;
+#else
+    val = __hmin2(val, make_bfloat162(127., 127.));
+    val = __hmax2(val, make_bfloat162(-128., -128.));
+    union { int8_t int8[2]; int16_t int16; };
+    int8[0] = static_cast<int8_t>(static_cast<short>(val.x));
+    int8[1] = static_cast<int8_t>(static_cast<short>(val.y));
+    return int16;
+#endif
+}
+
+inline __device__ __nv_bfloat162 float22bf162(const float2 val) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    return __floats2bfloat162_rn(val.x, val.y);
+#else
+    return __float22bfloat162_rn(val);
+#endif
+}
+
+inline __device__ __nv_bfloat162 bf162bf162(const __nv_bfloat16 val) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    __nv_bfloat162 val2;
+    val2.x = val;
+    val2.y = val;
+    return val2;
+#else
+    return __bfloat162bfloat162(val);
+#endif
+}
+
+inline __device__ __nv_bfloat162 bf16hadd2(const __nv_bfloat162 x, const __nv_bfloat162 y) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    float fxl, fxh, fyl, fyh;
+    fxl = __low2float(x);
+    fxh = __high2float(x);
+    fyl = __low2float(y);
+    fyh = __high2float(y);
+    return __floats2bfloat162_rn(fxl + fyl, fxh + fyh);
+#else
+    return __hadd2(x, y);
+#endif
+}
+
+inline __device__ __nv_bfloat16 bf16hadd(const __nv_bfloat16 x, const __nv_bfloat16 y) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    return __float2bfloat16( __bfloat162float(x) + __bfloat162float(y) );
+#else
+    return __hadd(x, y);
+#endif
+}
+
+inline __device__ __nv_bfloat162 bf16hsub2(const __nv_bfloat162 x, const __nv_bfloat162 y) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    float fxl, fxh, fyl, fyh;
+    fxl = __low2float(x);
+    fxh = __high2float(x);
+    fyl = __low2float(y);
+    fyh = __high2float(y);
+    return __floats2bfloat162_rn(fxl - fyl, fxh - fyh);
+#else
+    return __hsub2(x, y);
+#endif
+}
+
+inline __device__ __nv_bfloat16 bf16hsub(const __nv_bfloat16 x, const __nv_bfloat16 y) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    return __float2bfloat16( __bfloat162float(x) - __bfloat162float(y) );
+#else
+    return __hsub(x, y);
+#endif
+}
+
+inline __device__ __nv_bfloat162 bf16hmul2(const __nv_bfloat162 x, const __nv_bfloat162 y) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    float fxl, fxh, fyl, fyh;
+    fxl = __low2float(x);
+    fxh = __high2float(x);
+    fyl = __low2float(y);
+    fyh = __high2float(y);
+    return __floats2bfloat162_rn(fxl * fyl, fxh * fyh);
+#else
+    return __hmul2(x, y);
+#endif
+}
+
+inline __device__ __nv_bfloat16 bf16hmul(const __nv_bfloat16 x, const __nv_bfloat16 y) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    return __float2bfloat16( __bfloat162float(x) * __bfloat162float(y) );
+#else
+    return __hmul(x, y);
+#endif
+}
+
+inline __device__ __nv_bfloat162 bf16hfma2(const __nv_bfloat162 x, const __nv_bfloat162 y, const __nv_bfloat162 z) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    float fxl, fxh, fyl, fyh, fzl, fzh;
+    fxl = __low2float(x);
+    fxh = __high2float(x);
+    fyl = __low2float(y);
+    fyh = __high2float(y);
+    fzl = __low2float(z);
+    fzh = __high2float(z);
+    return __floats2bfloat162_rn(fxl * fyl + fzl, fxh * fyh + fzh);
+#else
+    return __hfma2(x, y, z);
+#endif
+}
+
+inline __device__ __nv_bfloat16 bf16hfma(const __nv_bfloat16 x, const __nv_bfloat16 y, const __nv_bfloat16 z) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    return __float2bfloat16( __bfloat162float(x) * __bfloat162float(y) + __bfloat162float(z));
+#else
+    return __hfma(x, y, z);
+#endif
+}
+
+inline __device__ __nv_bfloat162 bf16exp2(const __nv_bfloat162 x) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    float fxl, fxh;
+    fxl = __low2float(x);
+    fxh = __high2float(x);;
+    return __floats2bfloat162_rn(expf(fxl), expf(fxh));
+#else
+    return h2exp(x);
+#endif
+}
+
+#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800)
+inline __device__ __nv_bfloat162 operator*(const __nv_bfloat162 x, const __nv_bfloat162 y) { return bf16hmul2(x, y); };
+inline __device__ __nv_bfloat162 operator+(const __nv_bfloat162 x, const __nv_bfloat162 y) { return bf16hadd2(x, y); };
+
+inline __device__ __nv_bfloat162 make_bfloat162(const __nv_bfloat16 x, const __nv_bfloat16 y)
+{
+    __nv_bfloat162 t; t.x = x; t.y = y; return t;
+}
+
+#endif
+
+inline __device__ __nv_bfloat16 bf16hadd(__nv_bfloat16 a, __nv_bfloat16 b, __nv_bfloat16 c) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    return __float2bfloat16(__bfloat162float(a) + __bfloat162float(b) + __bfloat162float(c));
+#else
+    return a + b + c;
+#endif
+}
+
+inline __device__ __nv_bfloat16 bf16hadd(__nv_bfloat16 a, __nv_bfloat16 b, __nv_bfloat16 c, __nv_bfloat16 d) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    return __float2bfloat16(__bfloat162float(a) + __bfloat162float(b) + __bfloat162float(c) + __bfloat162float(d));
+#else
+    return (__nv_bfloat16)((float)a + (float)b + (float)c + (float)d);
+#endif
+}
+
+inline __device__ __nv_bfloat162 bf16hadd2(__nv_bfloat162 a, __nv_bfloat162 b, __nv_bfloat162 c) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    float fal, fah, fbl, fbh, fcl, fch;
+    fal = __low2float(a);
+    fah = __high2float(a);
+    fbl = __low2float(b);
+    fbh = __high2float(b);
+    fcl = __low2float(c);
+    fch = __high2float(c);
+    return __floats2bfloat162_rn(fal + fbl + fcl, fah + fbh + fch);
+#else
+    return a + b + c;
+#endif
+}
+
+inline __device__ __nv_bfloat16 bf16hmul(__nv_bfloat16 a, __nv_bfloat16 b, __nv_bfloat16 c) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    return __float2bfloat16(__bfloat162float(a) * __bfloat162float(b) * __bfloat162float(c));
+#else
+    return a * b * c;
+#endif
+}
+
+inline __device__ __nv_bfloat162 bf16hmul2(__nv_bfloat162 a, __nv_bfloat162 b, __nv_bfloat162 c) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    float fal, fah, fbl, fbh, fcl, fch;
+    fal = __low2float(a);
+    fah = __high2float(a);
+    fbl = __low2float(b);
+    fbh = __high2float(b);
+    fcl = __low2float(c);
+    fch = __high2float(c);
+    return __floats2bfloat162_rn(fal * fbl * fcl, fah * fbh * fch);
+#else
+    return a * b * c;
+#endif
+}
+
+inline __device__ __nv_bfloat162 bf16hfma2(__nv_bfloat162 a, __nv_bfloat162 b, __nv_bfloat162 c, __nv_bfloat162 d) {
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 800
+    float fal, fah, fbl, fbh, fcl, fch, fdl, fdh;
+    fal = __low2float(a);
+    fah = __high2float(a);
+    fbl = __low2float(b);
+    fbh = __high2float(b);
+    fcl = __low2float(c);
+    fch = __high2float(c);
+    fdl = __low2float(d);
+    fdh = __high2float(d);
+    return __floats2bfloat162_rn(fal * fbl * fcl + fdl, fah * fbh * fch + fdh);
+#else
+    return a * b * c + d;
+#endif
+}
+
+#endif // ENABLE_BF16
+
+}  // namespace fastertransformer
diff --git a/src/tl_templates/cuda/cuda_bf16_wrapper.h b/src/tl_templates/cuda/cuda_bf16_wrapper.h
new file mode 100644
index 00000000..efb6e798
--- /dev/null
+++ b/src/tl_templates/cuda/cuda_bf16_wrapper.h
@@ -0,0 +1,23 @@
+// Downloaded from from FasterTransformer v5.2.1
+// https://github.com/NVIDIA/FasterTransformer/blob/release/v5.2.1_tag/src/fastertransformer/utils/cuda_bf16_wrapper.h
+/*
+ * Copyright (c) 2019-2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#ifdef ENABLE_BF16
+#include <cuda_bf16.h>
+#endif
diff --git a/src/tl_templates/cuda/cuda_fp8.h b/src/tl_templates/cuda/cuda_fp8.h
index 3b610b27..8d216582 100644
--- a/src/tl_templates/cuda/cuda_fp8.h
+++ b/src/tl_templates/cuda/cuda_fp8.h
@@ -1,5 +1,6 @@
 #pragma once
 
+#include <cuda_fp8.h>
 #include <cute/numeric/numeric_types.hpp>
 
 using fp8_e4_t = cute::float_e4m3_t;
@@ -27,6 +28,19 @@ struct __CUDA_ALIGN__(16) fp8_e4_16_t {
   fp8_e4_8_t y;
 };
 
+struct __CUDA_ALIGN__(32) fp8_e4_32_t {
+  fp8_e4_16_t x;
+  fp8_e4_16_t y;
+
+  __device__ __forceinline__ fp8_e4_32_t &operator=(const ulonglong4 &rhs) {
+    x.x = *(fp8_e4_8_t *)&rhs.x;
+    x.y = *(fp8_e4_8_t *)&rhs.y;
+    y.x = *(fp8_e4_8_t *)&rhs.z;
+    y.y = *(fp8_e4_8_t *)&rhs.w;
+    return *this;
+  }
+};
+
 struct __CUDA_ALIGN__(2) fp8_e5_2_t {
   fp8_e5_t x;
   fp8_e5_t y;
@@ -48,3 +62,138 @@ struct __CUDA_ALIGN__(16) fp8_e5_16_t {
   fp8_e5_8_t x;
   fp8_e5_8_t y;
 };
+
+struct __CUDA_ALIGN__(32) fp8_e5_32_t {
+  fp8_e5_16_t x;
+  fp8_e5_16_t y;
+
+  __device__ __forceinline__ fp8_e5_32_t &operator=(const ulonglong4 &rhs) {
+    x.x = *(fp8_e5_8_t *)&rhs.x;
+    x.y = *(fp8_e5_8_t *)&rhs.y;
+    y.x = *(fp8_e5_8_t *)&rhs.z;
+    y.y = *(fp8_e5_8_t *)&rhs.w;
+    return *this;
+  }
+};
+
+// Pack two fp8_e4_t values.
+__forceinline__ __device__ fp8_e4_2_t make_fp8_e4_2_t(fp8_e4_t x, fp8_e4_t y) {
+  fp8_e4_2_t result;
+  result.x = x;
+  result.y = y;
+  return result;
+}
+
+// Pack four fp8_e4_t values.
+__forceinline__ __device__ fp8_e4_4_t make_fp8_e4_4_t(fp8_e4_t x0, fp8_e4_t x1,
+                                                      fp8_e4_t x2,
+                                                      fp8_e4_t x3) {
+  fp8_e4_4_t result;
+  result.x = x0;
+  result.y = x1;
+  result.z = x2;
+  result.w = x3;
+  return result;
+}
+
+// Pack eight fp8_e4_t values.
+__forceinline__ __device__ fp8_e4_8_t make_fp8_e4_8_t(fp8_e4_t x0, fp8_e4_t x1,
+                                                      fp8_e4_t x2, fp8_e4_t x3,
+                                                      fp8_e4_t x4, fp8_e4_t x5,
+                                                      fp8_e4_t x6,
+                                                      fp8_e4_t x7) {
+  fp8_e4_8_t result;
+  result.x = make_fp8_e4_4_t(x0, x1, x2, x3);
+  result.y = make_fp8_e4_4_t(x4, x5, x6, x7);
+  return result;
+}
+
+// Pack sixteen fp8_e4_t values.
+__forceinline__ __device__ fp8_e4_16_t
+make_fp8_e4_16_t(fp8_e4_t x0, fp8_e4_t x1, fp8_e4_t x2, fp8_e4_t x3,
+                 fp8_e4_t x4, fp8_e4_t x5, fp8_e4_t x6, fp8_e4_t x7,
+                 fp8_e4_t y0, fp8_e4_t y1, fp8_e4_t y2, fp8_e4_t y3,
+                 fp8_e4_t y4, fp8_e4_t y5, fp8_e4_t y6, fp8_e4_t y7) {
+  fp8_e4_16_t result;
+  result.x = make_fp8_e4_8_t(x0, x1, x2, x3, x4, x5, x6, x7);
+  result.y = make_fp8_e4_8_t(y0, y1, y2, y3, y4, y5, y6, y7);
+  return result;
+}
+
+// Pack thirty-two fp8_e4_t values.
+__forceinline__ __device__ fp8_e4_32_t make_fp8_e4_32_t(
+    fp8_e4_t x0, fp8_e4_t x1, fp8_e4_t x2, fp8_e4_t x3, fp8_e4_t x4,
+    fp8_e4_t x5, fp8_e4_t x6, fp8_e4_t x7, fp8_e4_t x8, fp8_e4_t x9,
+    fp8_e4_t x10, fp8_e4_t x11, fp8_e4_t x12, fp8_e4_t x13, fp8_e4_t x14,
+    fp8_e4_t x15, fp8_e4_t y0, fp8_e4_t y1, fp8_e4_t y2, fp8_e4_t y3,
+    fp8_e4_t y4, fp8_e4_t y5, fp8_e4_t y6, fp8_e4_t y7, fp8_e4_t y8,
+    fp8_e4_t y9, fp8_e4_t y10, fp8_e4_t y11, fp8_e4_t y12, fp8_e4_t y13,
+    fp8_e4_t y14, fp8_e4_t y15) {
+  fp8_e4_32_t result;
+  result.x = make_fp8_e4_16_t(x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11,
+                              x12, x13, x14, x15);
+  result.y = make_fp8_e4_16_t(y0, y1, y2, y3, y4, y5, y6, y7, y8, y9, y10, y11,
+                              y12, y13, y14, y15);
+  return result;
+}
+
+// Pack two fp8_e5_t values.
+__forceinline__ __device__ fp8_e5_2_t make_fp8_e5_2_t(fp8_e5_t x, fp8_e5_t y) {
+  fp8_e5_2_t result;
+  result.x = x;
+  result.y = y;
+  return result;
+}
+
+// Pack four fp8_e5_t values.
+__forceinline__ __device__ fp8_e5_4_t make_fp8_e5_4_t(fp8_e5_t x0, fp8_e5_t x1,
+                                                      fp8_e5_t x2,
+                                                      fp8_e5_t x3) {
+  fp8_e5_4_t result;
+  result.x = x0;
+  result.y = x1;
+  result.z = x2;
+  result.w = x3;
+  return result;
+}
+
+// Pack eight fp8_e5_t values.
+__forceinline__ __device__ fp8_e5_8_t make_fp8_e5_8_t(fp8_e5_t x0, fp8_e5_t x1,
+                                                      fp8_e5_t x2, fp8_e5_t x3,
+                                                      fp8_e5_t x4, fp8_e5_t x5,
+                                                      fp8_e5_t x6,
+                                                      fp8_e5_t x7) {
+  fp8_e5_8_t result;
+  result.x = make_fp8_e5_4_t(x0, x1, x2, x3);
+  result.y = make_fp8_e5_4_t(x4, x5, x6, x7);
+  return result;
+}
+
+// Pack sixteen fp8_e5_t values.
+__forceinline__ __device__ fp8_e5_16_t
+make_fp8_e5_16_t(fp8_e5_t x0, fp8_e5_t x1, fp8_e5_t x2, fp8_e5_t x3,
+                 fp8_e5_t x4, fp8_e5_t x5, fp8_e5_t x6, fp8_e5_t x7,
+                 fp8_e5_t y0, fp8_e5_t y1, fp8_e5_t y2, fp8_e5_t y3,
+                 fp8_e5_t y4, fp8_e5_t y5, fp8_e5_t y6, fp8_e5_t y7) {
+  fp8_e5_16_t result;
+  result.x = make_fp8_e5_8_t(x0, x1, x2, x3, x4, x5, x6, x7);
+  result.y = make_fp8_e5_8_t(y0, y1, y2, y3, y4, y5, y6, y7);
+  return result;
+}
+
+// Pack thirty-two fp8_e5_t values.
+__forceinline__ __device__ fp8_e5_32_t make_fp8_e5_32_t(
+    fp8_e5_t x0, fp8_e5_t x1, fp8_e5_t x2, fp8_e5_t x3, fp8_e5_t x4,
+    fp8_e5_t x5, fp8_e5_t x6, fp8_e5_t x7, fp8_e5_t x8, fp8_e5_t x9,
+    fp8_e5_t x10, fp8_e5_t x11, fp8_e5_t x12, fp8_e5_t x13, fp8_e5_t x14,
+    fp8_e5_t x15, fp8_e5_t y0, fp8_e5_t y1, fp8_e5_t y2, fp8_e5_t y3,
+    fp8_e5_t y4, fp8_e5_t y5, fp8_e5_t y6, fp8_e5_t y7, fp8_e5_t y8,
+    fp8_e5_t y9, fp8_e5_t y10, fp8_e5_t y11, fp8_e5_t y12, fp8_e5_t y13,
+    fp8_e5_t y14, fp8_e5_t y15) {
+  fp8_e5_32_t result;
+  result.x = make_fp8_e5_16_t(x0, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11,
+                              x12, x13, x14, x15);
+  result.y = make_fp8_e5_16_t(y0, y1, y2, y3, y4, y5, y6, y7, y8, y9, y10, y11,
+                              y12, y13, y14, y15);
+  return result;
+}
diff --git a/src/tl_templates/cuda/debug.h b/src/tl_templates/cuda/debug.h
index 07eabe69..3a71f60b 100644
--- a/src/tl_templates/cuda/debug.h
+++ b/src/tl_templates/cuda/debug.h
@@ -10,6 +10,15 @@
 // Template declaration for device-side debug printing (variable only)
 template <typename T> __device__ void debug_print_var(const char *msg, T var);
 
+// Overload for pointer type (supports any cv-qualified T*)
+template <typename T> __device__ void debug_print_var(const char *msg, T *var) {
+  printf(
+      "msg='%s' BlockIdx=(%d, %d, %d), ThreadIdx=(%d, %d, %d): dtype=pointer "
+      "value=%p\n",
+      msg, blockIdx.x, blockIdx.y, blockIdx.z, threadIdx.x, threadIdx.y,
+      threadIdx.z, var);
+}
+
 // Specialization for signed char type
 template <>
 __device__ void debug_print_var<signed char>(const char *msg, signed char var) {
@@ -39,6 +48,16 @@ template <> __device__ void debug_print_var<int>(const char *msg, int var) {
          threadIdx.z, var);
 }
 
+// Specialization for unsigned integer type
+template <>
+__device__ void debug_print_var<unsigned int>(const char *msg,
+                                              unsigned int var) {
+  printf("msg='%s' BlockIdx=(%d, %d, %d), ThreadIdx=(%d, %d, %d): dtype=int "
+         "value=%u\n",
+         msg, blockIdx.x, blockIdx.y, blockIdx.z, threadIdx.x, threadIdx.y,
+         threadIdx.z, var);
+}
+
 // Specialization for float type
 template <> __device__ void debug_print_var<float>(const char *msg, float var) {
   printf("msg='%s' BlockIdx=(%d, %d, %d), ThreadIdx=(%d, %d, %d): dtype=float "
@@ -118,7 +137,7 @@ debug_print_buffer_value<signed char>(const char *msg, const char *buf_name,
          threadIdx.z, buf_name, index, var);
 }
 
-// Specialization for unsiged char type
+// Specialization for unsigned char type
 template <>
 __device__ void
 debug_print_buffer_value<unsigned char>(const char *msg, const char *buf_name,
@@ -140,6 +159,17 @@ __device__ void debug_print_buffer_value<int>(const char *msg,
          threadIdx.z, buf_name, index, var);
 }
 
+// Specialization for unsigned integer type
+template <>
+__device__ void
+debug_print_buffer_value<unsigned int>(const char *msg, const char *buf_name,
+                                       int index, unsigned int var) {
+  printf("msg='%s' BlockIdx=(%d, %d, %d), ThreadIdx=(%d, %d, %d): buffer=%s, "
+         "index=%d, dtype=int value=%u\n",
+         msg, blockIdx.x, blockIdx.y, blockIdx.z, threadIdx.x, threadIdx.y,
+         threadIdx.z, buf_name, index, var);
+}
+
 // Specialization for float type
 template <>
 __device__ void debug_print_buffer_value<float>(const char *msg,
@@ -216,3 +246,27 @@ __device__ void debug_print_buffer_value<fp8_e5_t>(const char *msg,
          msg, blockIdx.x, blockIdx.y, blockIdx.z, threadIdx.x, threadIdx.y,
          threadIdx.z, buf_name, index, (float)var);
 }
+
+// Specialization for int16 type
+template <>
+__device__ void debug_print_buffer_value<int16_t>(const char *msg,
+                                                  const char *buf_name,
+                                                  int index, int16_t var) {
+  printf("msg='%s' BlockIdx=(%d, %d, %d), ThreadIdx=(%d, %d, %d): buffer=%s, "
+         "index=%d, dtype=int16_t value=%d\n",
+         msg, blockIdx.x, blockIdx.y, blockIdx.z, threadIdx.x, threadIdx.y,
+         threadIdx.z, buf_name, index, (int32_t)var);
+}
+
+// Specialization for msg-only debug print
+__device__ void debug_print_msg(const char *msg) {
+  printf("msg='%s' BlockIdx=(%d, %d, %d), ThreadIdx=(%d, %d, %d)\n", msg,
+         blockIdx.x, blockIdx.y, blockIdx.z, threadIdx.x, threadIdx.y,
+         threadIdx.z);
+}
+
+__device__ uint64_t get_clock() {
+  uint64_t gpu_clock;
+  asm volatile("mov.u64 %0, %%clock64;\n" : "=l"(gpu_clock) : : "memory");
+  return gpu_clock;
+}
diff --git a/src/tl_templates/cuda/distributed.h b/src/tl_templates/cuda/distributed.h
new file mode 100644
index 00000000..7eca3a70
--- /dev/null
+++ b/src/tl_templates/cuda/distributed.h
@@ -0,0 +1,21 @@
+#pragma once
+
+#include "common.h"
+
+namespace tl {
+
+extern "C" extern __device__ uint64_t meta_data[1024];
+
+TL_DEVICE uint64_t get_rank() { return meta_data[0]; }
+
+TL_DEVICE uint64_t get_num_ranks() { return meta_data[1]; }
+
+TL_DEVICE uint64_t get_remote_base_ptr(uint64_t rank) {
+  return meta_data[2 + rank];
+}
+
+template <typename dtype_t> TL_DEVICE uint64_t get_uintptr_t(dtype_t *ptr) {
+  return reinterpret_cast<uint64_t>(ptr);
+}
+
+} // namespace tl
diff --git a/src/tl_templates/cuda/gemm.h b/src/tl_templates/cuda/gemm.h
index 500b9717..b0b2a1b4 100644
--- a/src/tl_templates/cuda/gemm.h
+++ b/src/tl_templates/cuda/gemm.h
@@ -1,5 +1,11 @@
 #pragma once
-#if (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 900))
+
+#if (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 1200))
+#include "gemm_sm120.h"
+#elif (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 1000))
+#include "gemm_sm100.h"
+#elif (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 900))
+#include "./instruction/wgmma.h"
 #include "gemm_sm90.h"
 #elif (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 890))
 #include "gemm_sm89.h"
@@ -8,5 +14,5 @@
 #elif (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 700))
 #include "gemm_sm70.h"
 #else
-
+// No matching architecture found
 #endif
diff --git a/src/tl_templates/cuda/gemm_mma.h b/src/tl_templates/cuda/gemm_mma.h
new file mode 100644
index 00000000..9462514f
--- /dev/null
+++ b/src/tl_templates/cuda/gemm_mma.h
@@ -0,0 +1,482 @@
+#pragma once
+
+#include <cute/algorithm/clear.hpp>
+#include <cute/arch/mma_sm120.hpp>
+#include <cute/arch/mma_sm80.hpp>
+#include <cute/arch/mma_sm89.hpp>
+#include <cute/atom/mma_atom.hpp>
+#include <cute/underscore.hpp>
+
+#include "common.h"
+#include "cuda_fp8.h"
+#include "intrin.h"
+
+namespace cute::tl_mma {
+
+template <typename A_type, typename B_type, typename C_type, int num_warp_m,
+          int num_warp_n, int N>
+struct DispatchInstruction;
+
+using _X = Underscore;
+
+} // namespace cute::tl_mma
+
+#define TL_DISPATCH_MMA(A_type, B_type, C_type, MMA_instr)                     \
+  namespace cute::tl_mma {                                                     \
+  template <int num_warp_m, int num_warp_n, int N>                             \
+  struct DispatchInstruction<A_type, B_type, C_type, num_warp_m, num_warp_n,   \
+                             N> {                                              \
+    using MMA = MMA_Atom<MMA_instr>;                                           \
+    using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;         \
+  };                                                                           \
+  }
+#define TL_DISPATCH_MMA_TEMPLATE(A_type, B_type, C_type, MMA_instr)            \
+  namespace cute::tl_mma {                                                     \
+  template <int num_warp_m, int num_warp_n, int N>                             \
+  struct DispatchInstruction<A_type, B_type, C_type, num_warp_m, num_warp_n,   \
+                             N> {                                              \
+    using MMA = MMA_Atom<MMA_instr<A_type, B_type, C_type>>;                   \
+    using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;         \
+  };                                                                           \
+  }
+
+#ifdef __CUDA_ARCH_LIST__
+#if __CUDA_ARCH_LIST__ >= 1200
+#include "cuda_fp8.h"
+#include <cute/arch/mma_sm120.hpp>
+#include <cute/arch/mma_sm80.hpp>
+TL_DISPATCH_MMA_TEMPLATE(fp8_e4_t, fp8_e4_t, float, SM120_16x8x32_TN)
+TL_DISPATCH_MMA_TEMPLATE(fp8_e5_t, fp8_e5_t, float, SM120_16x8x32_TN)
+TL_DISPATCH_MMA(half_t, half_t, half_t, SM80_16x8x16_F16F16F16F16_TN)
+TL_DISPATCH_MMA(half_t, half_t, float, SM80_16x8x16_F32F16F16F32_TN)
+TL_DISPATCH_MMA(bfloat16_t, bfloat16_t, float, SM80_16x8x16_F32BF16BF16F32_TN)
+TL_DISPATCH_MMA(tfloat32_t, tfloat32_t, float, SM80_16x8x8_F32TF32TF32F32_TN)
+TL_DISPATCH_MMA(int8_t, int8_t, int, SM80_16x8x32_S32S8S8S32_TN)
+TL_DISPATCH_MMA(double, double, double, SM80_8x8x4_F64F64F64F64_TN)
+#elif __CUDA_ARCH_LIST__ >= 1000
+#include "cuda_fp8.h"
+#include <cute/arch/mma_sm100.hpp>
+#include <cute/arch/mma_sm80.hpp>
+#include <cute/arch/mma_sm89.hpp>
+TL_DISPATCH_MMA(fp8_e4_t, fp8_e4_t, float, SM89_16x8x32_F32E4M3E4M3F32_TN)
+TL_DISPATCH_MMA(fp8_e5_t, fp8_e5_t, float, SM89_16x8x32_F32E5M2E5M2F32_TN)
+TL_DISPATCH_MMA(half_t, half_t, half_t, SM80_16x8x16_F16F16F16F16_TN)
+TL_DISPATCH_MMA(half_t, half_t, float, SM80_16x8x16_F32F16F16F32_TN)
+TL_DISPATCH_MMA(bfloat16_t, bfloat16_t, float, SM80_16x8x16_F32BF16BF16F32_TN)
+TL_DISPATCH_MMA(tfloat32_t, tfloat32_t, float, SM80_16x8x8_F32TF32TF32F32_TN)
+TL_DISPATCH_MMA(int8_t, int8_t, int, SM80_16x8x32_S32S8S8S32_TN)
+TL_DISPATCH_MMA(double, double, double, SM80_8x8x4_F64F64F64F64_TN)
+#elif __CUDA_ARCH_LIST__ >= 900
+#include "cuda_fp8.h"
+#include <cute/arch/mma_sm80.hpp>
+#include <cute/arch/mma_sm89.hpp>
+TL_DISPATCH_MMA(fp8_e4_t, fp8_e4_t, float, SM89_16x8x32_F32E4M3E4M3F32_TN)
+TL_DISPATCH_MMA(fp8_e5_t, fp8_e5_t, float, SM89_16x8x32_F32E5M2E5M2F32_TN)
+TL_DISPATCH_MMA(half_t, half_t, half_t, SM80_16x8x16_F16F16F16F16_TN)
+TL_DISPATCH_MMA(half_t, half_t, float, SM80_16x8x16_F32F16F16F32_TN)
+TL_DISPATCH_MMA(bfloat16_t, bfloat16_t, float, SM80_16x8x16_F32BF16BF16F32_TN)
+TL_DISPATCH_MMA(tfloat32_t, tfloat32_t, float, SM80_16x8x8_F32TF32TF32F32_TN)
+TL_DISPATCH_MMA(int8_t, int8_t, int, SM80_16x8x32_S32S8S8S32_TN)
+TL_DISPATCH_MMA(double, double, double, SM80_8x8x4_F64F64F64F64_TN)
+#elif __CUDA_ARCH_LIST__ >= 890
+#include "cuda_fp8.h"
+#include <cute/arch/mma_sm80.hpp>
+#include <cute/arch/mma_sm89.hpp>
+TL_DISPATCH_MMA(fp8_e4_t, fp8_e4_t, float, SM89_16x8x32_F32E4M3E4M3F32_TN)
+TL_DISPATCH_MMA(fp8_e5_t, fp8_e5_t, float, SM89_16x8x32_F32E5M2E5M2F32_TN)
+TL_DISPATCH_MMA(half_t, half_t, half_t, SM80_16x8x16_F16F16F16F16_TN)
+TL_DISPATCH_MMA(half_t, half_t, float, SM80_16x8x16_F32F16F16F32_TN)
+TL_DISPATCH_MMA(bfloat16_t, bfloat16_t, float, SM80_16x8x16_F32BF16BF16F32_TN)
+TL_DISPATCH_MMA(tfloat32_t, tfloat32_t, float, SM80_16x8x8_F32TF32TF32F32_TN)
+TL_DISPATCH_MMA(int8_t, int8_t, int, SM80_16x8x32_S32S8S8S32_TN)
+TL_DISPATCH_MMA(double, double, double, SM80_8x8x4_F64F64F64F64_TN)
+#elif __CUDA_ARCH_LIST__ >= 800
+#include <cute/arch/mma_sm80.hpp>
+TL_DISPATCH_MMA(half_t, half_t, half_t, SM80_16x8x16_F16F16F16F16_TN)
+TL_DISPATCH_MMA(half_t, half_t, float, SM80_16x8x16_F32F16F16F32_TN)
+TL_DISPATCH_MMA(bfloat16_t, bfloat16_t, float, SM80_16x8x16_F32BF16BF16F32_TN)
+TL_DISPATCH_MMA(tfloat32_t, tfloat32_t, float, SM80_16x8x8_F32TF32TF32F32_TN)
+TL_DISPATCH_MMA(int8_t, int8_t, int, SM80_16x8x32_S32S8S8S32_TN)
+TL_DISPATCH_MMA(double, double, double, SM80_8x8x4_F64F64F64F64_TN)
+#elif __CUDA_ARCH_LIST__ >= 750
+TL_DISPATCH_MMA(half_t, half_t, float, SM75_16x8x8_F32F16F16F32_TN)
+#endif
+#endif
+#undef TL_DISPATCH_MMA
+#undef TL_DISPATCH_MMA_TEMPLATE
+
+namespace cute::tl_mma {
+
+template <int N, int num_warp_n, bool transpose> struct SelectCopy {
+  static constexpr int remainder = (N / num_warp_n) % 16;
+  using type = std::conditional_t<
+      remainder == 4 || remainder == 8 || remainder == 0,
+      std::conditional_t<
+          transpose,
+          std::conditional_t<
+              remainder == 4, SM75_U32x1_LDSM_N,
+              std::conditional_t<remainder == 8, SM75_U32x2_LDSM_N,
+                                 SM75_U32x4_LDSM_N>>,
+          std::conditional_t<
+              remainder == 4, SM75_U16x2_LDSM_T,
+              std::conditional_t<remainder == 8, SM75_U16x4_LDSM_T,
+                                 SM75_U16x8_LDSM_T>>>,
+      DefaultCopy>;
+};
+
+template <int Bits, int N, int K, bool K_inner, int num_warp_n, int leading_dim,
+          typename Enable = void>
+struct OperandTraits {
+  // Primary template, use padded layout and default copy
+  static constexpr int stride = leading_dim;
+  static constexpr int padded =
+      stride % (256 / Bits) == 0 ? stride + 128 / Bits : stride;
+  using Layout = typename std::conditional<
+      K_inner, Layout<Shape<Int<N>, Int<leading_dim>>, Shape<Int<padded>, _1>>,
+      Layout<Shape<Int<leading_dim>, Int<K>>, Shape<_1, Int<padded>>>>::type;
+  using Copy = DefaultCopy;
+};
+
+template <int N, int K, int num_warp_n, int leading_dim>
+struct OperandTraits<16, N, K, true, num_warp_n, leading_dim,
+                     typename std::enable_if<leading_dim % 64 == 32>::type> {
+  using LayoutAtom = decltype(composition(
+      Swizzle<2, 3, 3>{}, Layout<Shape<_8, _32>, Stride<_32, _1>>{}));
+  using Layout =
+      decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<leading_dim>>{}));
+  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
+};
+
+template <int N, int K, int num_warp_n, int leading_dim>
+struct OperandTraits<16, N, K, true, num_warp_n, leading_dim,
+                     typename std::enable_if<leading_dim % 64 == 0>::type> {
+  using LayoutAtom = decltype(composition(
+      Swizzle<3, 3, 3>{}, Layout<Shape<_8, _64>, Stride<_64, _1>>{}));
+  using Layout =
+      decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<leading_dim>>{}));
+  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
+};
+
+template <int N, int K, int num_warp_n, int leading_dim>
+struct OperandTraits<16, N, K, false, num_warp_n, leading_dim,
+                     typename std::enable_if<leading_dim % 64 == 32>::type> {
+  using LayoutAtom = decltype(composition(
+      Swizzle<2, 3, 3>{}, Layout<Shape<_32, _8>, Stride<_1, _32>>{}));
+  using Layout = decltype(tile_to_shape(
+      LayoutAtom{}, Shape<Int<leading_dim>, Int<K>>{}, Step<_2, _1>{}));
+  using Copy = typename SelectCopy<N, num_warp_n, false>::type;
+};
+
+template <int N, int K, int num_warp_n, int leading_dim>
+struct OperandTraits<16, N, K, false, num_warp_n, leading_dim,
+                     typename std::enable_if<leading_dim % 64 == 0>::type> {
+  using LayoutAtom = decltype(composition(
+      Swizzle<3, 3, 3>{}, Layout<Shape<_64, _8>, Stride<_1, _64>>{}));
+  using Layout = decltype(tile_to_shape(
+      LayoutAtom{}, Shape<Int<leading_dim>, Int<K>>{}, Step<_2, _1>{}));
+  using Copy = typename SelectCopy<N, num_warp_n, false>::type;
+};
+
+template <int N, int K, int num_warp_n, int leading_dim>
+struct OperandTraits<32, N, K, true, num_warp_n, leading_dim,
+                     typename std::enable_if<leading_dim % 32 == 0>::type> {
+  using LayoutAtom = decltype(composition(
+      Swizzle<3, 2, 3>{}, Layout<Shape<_8, _32>, Stride<_32, _1>>{}));
+  using Layout =
+      decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<leading_dim>>{}));
+  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
+};
+
+template <int N, int K, int num_warp_n, int leading_dim>
+struct OperandTraits<32, N, K, true, num_warp_n, leading_dim,
+                     typename std::enable_if<leading_dim % 32 == 16>::type> {
+  using LayoutAtom = decltype(composition(
+      Swizzle<2, 2, 3>{}, Layout<Shape<_8, _16>, Stride<_16, _1>>{}));
+  using Layout =
+      decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<leading_dim>>{}));
+  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
+};
+
+template <int N, int K, int num_warp_n, int leading_dim>
+struct OperandTraits<32, N, K, false, num_warp_n, leading_dim,
+                     typename std::enable_if<leading_dim % 32 == 0>::type> {
+  using LayoutAtom = decltype(composition(
+      Swizzle<3, 2, 3>{}, Layout<Shape<_32, _8>, Stride<_1, _32>>{}));
+  using Layout = decltype(tile_to_shape(
+      LayoutAtom{}, Shape<Int<leading_dim>, Int<K>>{}, Step<_2, _1>{}));
+  using Copy = UniversalCopy<tfloat32_t>;
+};
+
+template <int N, int K, int num_warp_n, int leading_dim>
+struct OperandTraits<32, N, K, false, num_warp_n, leading_dim,
+                     typename std::enable_if<leading_dim % 32 == 16>::type> {
+  using LayoutAtom = decltype(composition(
+      Swizzle<2, 2, 3>{}, Layout<Shape<_16, _8>, Stride<_1, _16>>{}));
+  using Layout = decltype(tile_to_shape(
+      LayoutAtom{}, Shape<Int<leading_dim>, Int<K>>{}, Step<_2, _1>{}));
+  using Copy = UniversalCopy<tfloat32_t>;
+};
+
+template <int N, int K, int num_warp_n, int leading_dim>
+struct OperandTraits<8, N, K, true, num_warp_n, leading_dim,
+                     typename std::enable_if<leading_dim % 128 == 64>::type> {
+  using LayoutAtom = decltype(composition(
+      Swizzle<2, 4, 3>{}, Layout<Shape<_8, _64>, Stride<_64, _1>>{}));
+  using Layout =
+      decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<leading_dim>>{}));
+  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
+};
+
+template <int N, int K, int num_warp_n, int leading_dim>
+struct OperandTraits<8, N, K, true, num_warp_n, leading_dim,
+                     typename std::enable_if<leading_dim % 128 == 0>::type> {
+  using LayoutAtom = decltype(composition(
+      Swizzle<3, 4, 3>{}, Layout<Shape<_8, _128>, Stride<_128, _1>>{}));
+  using Layout =
+      decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<leading_dim>>{}));
+  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
+};
+
+template <int N, int K, int num_warp_n, int leading_dim>
+struct OperandTraits<64, N, K, true, num_warp_n, leading_dim,
+                     typename std::enable_if<leading_dim % 16 == 0>::type> {
+  using LayoutAtom = decltype(composition(
+      Swizzle<2, 0, 4>{}, Layout<Shape<_4, _16>, Stride<_16, _1>>{}));
+  using Layout =
+      decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<leading_dim>>{}));
+  using Copy = DefaultCopy;
+};
+
+template <int N, int K, int num_warp_n, int leading_dim>
+struct OperandTraits<64, N, K, false, num_warp_n, leading_dim,
+                     typename std::enable_if<leading_dim % 16 == 0>::type> {
+  using LayoutAtom = decltype(composition(
+      Swizzle<2, 2, 2>{}, Layout<Shape<_16, _4>, Stride<_1, _16>>{}));
+  using Layout = decltype(tile_to_shape(
+      LayoutAtom{}, Shape<Int<leading_dim>, Int<K>>{}, Step<_2, _1>{}));
+  using Copy = DefaultCopy;
+};
+
+template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
+          bool trans_B, bool clear_accum, int lda, int ldb, int offset_a,
+          int offset_b, typename A_type_raw, typename B_type_raw,
+          typename C_type_raw>
+class GemmTensorOp {
+public:
+  using A_type =
+      typename std::conditional<std::is_same<A_type_raw, float>::value,
+                                tfloat32_t, A_type_raw>::type;
+  using B_type =
+      typename std::conditional<std::is_same<B_type_raw, float>::value,
+                                tfloat32_t, A_type_raw>::type;
+  using C_type = C_type_raw;
+
+  using Instruction =
+      DispatchInstruction<A_type, B_type, C_type, num_warp_m, num_warp_n, N>;
+
+  using OperandATraits = OperandTraits<sizeof_bits<A_type>::value, M, K,
+                                       !trans_A, num_warp_m, lda>;
+  using OperandBTraits =
+      OperandTraits<sizeof_bits<B_type>::value, N, K, trans_B, num_warp_n, ldb>;
+
+  using SmemLayoutA = typename OperandATraits::Layout;
+  using SmemLayoutB = typename OperandBTraits::Layout;
+  using SmemCopyA = Copy_Atom<typename OperandATraits::Copy, A_type>;
+  using SmemCopyB = Copy_Atom<typename OperandBTraits::Copy, B_type>;
+
+  using TileMma = TiledMMA<typename Instruction::MMA,
+                           Layout<Shape<Int<num_warp_m>, Int<num_warp_n>, _1>>,
+                           typename Instruction::MMA_Group>;
+
+  template <class... Args>
+  static CUTE_DEVICE auto remove_swizzle(Layout<Args...> const &layout) {
+    return layout;
+  }
+  // In fp16, when layout is KxN and n_warp is 1 and N % 64 == 0
+  // the original layout fail to compile, currently using this as a workaround
+  template <class... Args>
+  static CUTE_DEVICE auto
+  remove_swizzle(ComposedLayout<Args...> const &layout) {
+    if constexpr (sizeof(A_type) == 2)
+      return layout.layout_b();
+    else
+      return layout;
+  }
+
+  template <int offset, int NN, int KK, bool trans, int lddim, typename Engine0,
+            typename Layout0>
+  static CUTE_DEVICE auto get_region_tensor(Tensor<Engine0, Layout0> &sa) {
+    if constexpr (offset == 0) {
+      return composition(
+          sa,
+          Layout<Shape<Int<NN>, Int<KK>>,
+                 Stride<_1, typename std::conditional<trans, Int<NN>,
+                                                      Int<lddim>>::type>>{});
+    } else {
+      if constexpr (trans) {
+        static_assert(offset % KK == 0, "Offset must be a multiple of K");
+        constexpr int offset_n = offset / KK;
+        return flat_divide(sa, Shape<Int<NN>, Int<KK>>{})(_, _, _0{},
+                                                          Int<offset_n>{});
+      } else {
+        static_assert(offset % NN == 0, "Offset must be a multiple of N");
+        constexpr int offset_n = offset / NN;
+        return flat_divide(sa, Shape<Int<NN>, Int<KK>>{})(_, _, Int<offset_n>{},
+                                                          _0{});
+      }
+    }
+  }
+
+  static CUTE_DEVICE void body(A_type_raw *pA, B_type_raw *pB, C_type_raw *pC) {
+    const int tid = threadIdx.x;
+    Tensor sA_all = make_tensor(make_smem_ptr(reinterpret_cast<A_type *>(pA)),
+                                SmemLayoutA{});
+    Tensor sB_all = make_tensor(make_smem_ptr(reinterpret_cast<B_type *>(pB)),
+                                SmemLayoutB{});
+    Tensor sA = get_region_tensor<offset_a, M, K, !trans_A, lda>(sA_all);
+    Tensor sB = get_region_tensor<offset_b, N, K, trans_B, ldb>(sB_all);
+    TileMma tiled_mma;
+    auto thr_mma = tiled_mma.get_thread_slice(tid);
+    auto tiled_copy_A = make_tiled_copy_A(SmemCopyA{}, tiled_mma);
+    auto tiled_copy_B = make_tiled_copy_B(SmemCopyB{}, tiled_mma);
+    auto thr_copy_A = tiled_copy_A.get_thread_slice(tid);
+    auto thr_copy_B = tiled_copy_B.get_thread_slice(tid);
+
+    Tensor tCrA = thr_mma.partition_fragment_A(sA);
+    Tensor tCrB = thr_mma.partition_fragment_B(sB);
+    Tensor tCsA = thr_copy_A.partition_S(sA);
+    Tensor tCsB = thr_copy_B.partition_S(sB);
+
+    Tensor tCrA_copy_view = thr_copy_A.retile_D(tCrA);
+    Tensor tCrB_copy_view = thr_copy_B.retile_D(tCrB);
+
+    Tensor acc =
+        make_tensor(make_rmem_ptr(reinterpret_cast<C_type *>(pC)),
+                    partition_shape_C(tiled_mma, Shape<Int<M>, Int<N>>{}));
+
+    // when layout is KxN and n_warp is 1, there seem to be a bug, use this as a
+    // workaround
+    auto tCrA_view = make_tensor(tCrA.data(), remove_swizzle(tCrA.layout()));
+    auto tCrB_view = make_tensor(tCrB.data(), remove_swizzle(tCrB.layout()));
+    if constexpr (clear_accum) {
+      clear(acc);
+    }
+    CUTE_UNROLL
+    for (int k = 0; k < size<2>(tCrA); ++k) {
+      copy(tiled_copy_A, tCsA(_, _, k), tCrA_copy_view(_, _, k));
+      copy(tiled_copy_B, tCsB(_, _, k), tCrB_copy_view(_, _, k));
+      gemm(tiled_mma, tCrA_view(_, _, k), tCrB_view(_, _, k), acc);
+    }
+  }
+
+  static CUTE_DEVICE void body_rs(A_type_raw *pA, B_type_raw *pB,
+                                  C_type_raw *pC) {
+    const int tid = threadIdx.x;
+    Tensor sB_all = make_tensor(make_smem_ptr(reinterpret_cast<B_type *>(pB)),
+                                SmemLayoutB{});
+    Tensor sB = get_region_tensor<offset_b, N, K, trans_B, ldb>(sB_all);
+    TileMma tiled_mma;
+    auto thr_mma = tiled_mma.get_thread_slice(tid);
+    auto tiled_copy_B = make_tiled_copy_B(SmemCopyB{}, tiled_mma);
+    auto thr_copy_B = tiled_copy_B.get_thread_slice(tid);
+
+    Tensor tCrB = thr_mma.partition_fragment_B(sB);
+    Tensor tCsB = thr_copy_B.partition_S(sB);
+
+    Tensor tCrB_copy_view = thr_copy_B.retile_D(tCrB);
+
+    Tensor acc =
+        make_tensor(make_rmem_ptr(reinterpret_cast<C_type *>(pC)),
+                    partition_shape_C(tiled_mma, Shape<Int<M>, Int<N>>{}));
+    Tensor tCrA =
+        make_tensor(make_rmem_ptr(reinterpret_cast<A_type *>(pA)),
+                    partition_shape_A(tiled_mma, Shape<Int<M>, Int<K>>{}));
+    auto tCrB_view = make_tensor(tCrB.data(), remove_swizzle(tCrB.layout()));
+    if constexpr (clear_accum) {
+      clear(acc);
+    }
+    copy(tiled_copy_B, tCsB(_, _, 0), tCrB_copy_view(_, _, 0));
+    CUTE_UNROLL
+    for (int k = 0; k < size<2>(tCrA); ++k) {
+      if (k < size<2>(tCrA) - 1) {
+        copy(tiled_copy_B, tCsB(_, _, k + 1), tCrB_copy_view(_, _, k + 1));
+      }
+      gemm(tiled_mma, tCrA(_, _, k), tCrB_view(_, _, k), acc);
+    }
+  }
+
+  static CUTE_DEVICE void body_sr(A_type_raw *pA, B_type_raw *pB,
+                                  C_type_raw *pC) {
+    const int tid = threadIdx.x;
+    Tensor sA_all = make_tensor(make_smem_ptr(reinterpret_cast<A_type *>(pA)),
+                                SmemLayoutA{});
+    Tensor sA = get_region_tensor<offset_a, M, K, !trans_A, lda>(sA_all);
+    TileMma tiled_mma;
+    auto thr_mma = tiled_mma.get_thread_slice(tid);
+    auto tiled_copy_A = make_tiled_copy_A(SmemCopyA{}, tiled_mma);
+    auto thr_copy_A = tiled_copy_A.get_thread_slice(tid);
+
+    Tensor tCrA = thr_mma.partition_fragment_A(sA);
+    Tensor tCsA = thr_copy_A.partition_S(sA);
+
+    Tensor tCrA_copy_view = thr_copy_A.retile_D(tCrA);
+
+    Tensor acc =
+        make_tensor(make_rmem_ptr(reinterpret_cast<C_type *>(pC)),
+                    partition_shape_C(tiled_mma, Shape<Int<M>, Int<N>>{}));
+    Tensor tCrB =
+        make_tensor(make_rmem_ptr(reinterpret_cast<B_type *>(pB)),
+                    partition_shape_B(tiled_mma, Shape<Int<N>, Int<K>>{}));
+    auto tCrA_view = make_tensor(tCrA.data(), remove_swizzle(tCrA.layout()));
+    if constexpr (clear_accum) {
+      clear(acc);
+    }
+    copy(tiled_copy_A, tCsA(_, _, 0), tCrA_copy_view(_, _, 0));
+    CUTE_UNROLL
+    for (int k = 0; k < size<2>(tCrA); ++k) {
+      if (k < size<2>(tCrA) - 1) {
+        copy(tiled_copy_A, tCsA(_, _, k + 1), tCrA_copy_view(_, _, k + 1));
+      }
+      gemm(tiled_mma, tCrA_view(_, _, k), tCrB(_, _, k), acc);
+    }
+  }
+};
+
+} // namespace cute::tl_mma
+
+namespace tl::tl_mma {
+
+template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
+          bool trans_B, bool clear_accum, int lda, int ldb, int offset_a,
+          int offset_b, typename A_type, typename B_type, typename C_type>
+CUTLASS_DEVICE void gemm_ss(A_type *pA, B_type *pB, C_type *accum) {
+  using MMA =
+      cute::tl_mma::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
+                                 trans_B, clear_accum, lda, ldb, offset_a,
+                                 offset_b, A_type, B_type, C_type>;
+  MMA::body(pA, pB, accum);
+}
+
+template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
+          bool trans_B, bool clear_accum, int lda, int ldb, int offset_a,
+          int offset_b, typename A_type, typename B_type, typename C_type>
+CUTLASS_DEVICE void gemm_rs(A_type *pA, B_type *pB, C_type *accum) {
+  using MMA =
+      cute::tl_mma::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
+                                 trans_B, clear_accum, lda, ldb, offset_a,
+                                 offset_b, A_type, B_type, C_type>;
+  MMA::body_rs(pA, pB, accum);
+}
+
+template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
+          bool trans_B, bool clear_accum, int lda, int ldb, int offset_a,
+          int offset_b, typename A_type, typename B_type, typename C_type>
+CUTLASS_DEVICE void gemm_sr(A_type *pA, B_type *pB, C_type *accum) {
+  using MMA =
+      cute::tl_mma::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
+                                 trans_B, clear_accum, lda, ldb, offset_a,
+                                 offset_b, A_type, B_type, C_type>;
+  MMA::body_sr(pA, pB, accum);
+}
+
+} // namespace tl::tl_mma
diff --git a/src/tl_templates/cuda/gemm_sm100.h b/src/tl_templates/cuda/gemm_sm100.h
new file mode 100644
index 00000000..5b50fe72
--- /dev/null
+++ b/src/tl_templates/cuda/gemm_sm100.h
@@ -0,0 +1,384 @@
+// Licensed under the MIT License.
+#pragma once
+
+#include "common.h"
+#include "gemm_mma.h"
+#include "intrin.h"
+
+#include <cutlass/arch/barrier.h>
+#include <cutlass/cutlass.h>
+#include <cutlass/gemm/collective/collective_builder.hpp>
+
+namespace cute {
+
+// Extensions to CuTe
+// CuTe don't support TCGEN5MMA with .ws, so we add it here
+// About why we need .ws, plz refer to comments in tl_tcgen5mma::GemmTensorOp
+
+template <class a_type, class b_type, class c_type, int M, int N,
+          UMMA::Major a_major, UMMA::Major b_major,
+          UMMA::ScaleIn a_neg = UMMA::ScaleIn::One,
+          UMMA::ScaleIn b_neg = UMMA::ScaleIn::One>
+struct SM100_MMA_F16BF16_WS_SS {
+  static_assert(M == 32 || M == 64 || M == 128,
+                "SM100_MMA_F16BF16 (with .ws) M-mode size should be 32, 64 or "
+                "128 for 1 CTA cluster MMA.");
+  static_assert(
+      N == 64 || N == 128 || N == 256,
+      "SM100_MMA_F16BF16 (with .ws) N-mode size should be 32, 64 or 128");
+
+  using DRegisters = void;
+  using ARegisters = uint64_t[1];
+  using BRegisters = uint64_t[1];
+  using CRegisters = uint32_t[1];
+
+  CUTE_HOST_DEVICE static void
+  fma(uint64_t const &desc_a, uint64_t const &desc_b, uint32_t const &tmem_c,
+      uint32_t const &scaleC, uint64_t const &idescE) {
+    if (cute::elect_one_sync()) {
+      asm volatile(
+          "{\n\t"
+          ".reg .pred p;\n\t"
+          "setp.ne.b32 p, %4, 0;\n\t"
+          "tcgen05.mma.ws.cta_group::1.kind::f16 [%0], %1, %2, %3, p, 0; \n\t"
+          "}\n"
+          :
+          : "r"(tmem_c), "l"(desc_a), "l"(desc_b), "r"(uint32_t(idescE >> 32)),
+            "r"(scaleC));
+    }
+  }
+};
+
+template <class a_type, class b_type, class c_type, int M, int N,
+          UMMA::Major a_major, UMMA::Major b_major, UMMA::ScaleIn a_neg,
+          UMMA::ScaleIn b_neg>
+struct MMA_Traits<SM100_MMA_F16BF16_WS_SS<a_type, b_type, c_type, M, N, a_major,
+                                          b_major, a_neg, b_neg>> {
+  using ValTypeD = c_type;
+  using ValTypeA = a_type;
+  using ValTypeB = b_type;
+  using ValTypeC = c_type;
+
+  static_assert(cute::sizeof_bits_v<a_type> == cute::sizeof_bits_v<b_type> &&
+                    cute::sizeof_bits_v<b_type> == 16,
+                "SM100_MMA_F16BF16_WS_SS supports 16bit types");
+
+  using FrgTypeA = UMMA::smem_desc<a_major>;
+  using FrgTypeB = UMMA::smem_desc<b_major>;
+  using FrgTypeC = UMMA::tmem_frg_ws_1sm<c_type>;
+
+  // Logical shape-K is always 256bits, transform to units of elements
+  static constexpr int K = 256 / cute::sizeof_bits<ValTypeA>::value;
+
+  using Shape_MNK = Shape<Int<M>, Int<N>, Int<K>>;
+  using ThrID = Layout<_1>;
+  using ALayout =
+      Layout<Shape<_1, Shape<Int<M>, Int<K>>>, Stride<_0, Stride<_1, Int<M>>>>;
+  using BLayout =
+      Layout<Shape<_1, Shape<Int<N>, Int<K>>>, Stride<_0, Stride<_1, Int<N>>>>;
+  using CLayout =
+      Layout<Shape<_1, Shape<Int<M>, Int<N>>>, Stride<_0, Stride<_1, Int<M>>>>;
+
+  UMMA::InstrDescriptor idesc_ =
+      UMMA::make_instr_desc<a_type, b_type, c_type, M, N, a_major, b_major,
+                            a_neg, b_neg>();
+
+  // Accumulate or overwrite C.   1: read C, 0: ignore C [clear accumulators]
+  UMMA::ScaleOut accumulate_ = UMMA::ScaleOut::One;
+
+  template <class TD, class DLayout, class TA, class ALayout, class TB,
+            class BLayout, class TC, class CLayout>
+  CUTE_HOST_DEVICE constexpr friend void
+  mma_unpack(MMA_Traits const &traits, Tensor<TD, DLayout> &D,
+             Tensor<TA, ALayout> const &A, Tensor<TB, BLayout> const &B,
+             Tensor<TC, CLayout> const &C) {
+    static_assert(is_tmem<TD>::value, "Expected tmem in MMA_Atom::call");
+    static_assert(is_rmem<TA>::value,
+                  "Expected desc registers in MMA_Atom::call");
+    static_assert(is_rmem<TB>::value,
+                  "Expected desc registers in MMA_Atom::call");
+    static_assert(is_tmem<TC>::value, "Expected tmem in MMA_Atom::call");
+
+    uint64_t desc_a = A[0];
+    uint64_t desc_b = B[0];
+    uint32_t tmem_c = raw_pointer_cast(D.data());
+    uint64_t idesc = UMMA::make_runtime_instr_desc<>(traits.idesc_);
+
+    SM100_MMA_F16BF16_WS_SS<a_type, b_type, c_type, M, N, a_major, b_major,
+                            a_neg, b_neg>::fma(desc_a, desc_b, tmem_c,
+                                               uint32_t(traits.accumulate_),
+                                               idesc);
+  }
+};
+
+struct SM100_MMA_F8F6F4_WS_SS {
+  using DRegisters = void;
+  using ARegisters = uint64_t[1];
+  using BRegisters = uint64_t[1];
+  using CRegisters = uint32_t[1];
+
+  CUTE_HOST_DEVICE static void
+  fma(uint64_t const &desc_a, uint64_t const &desc_b, uint32_t const &tmem_c,
+      uint32_t const &scaleC, uint64_t const &idescE) {
+    if (cute::elect_one_sync()) {
+      asm volatile("{\n\t"
+                   ".reg .pred p;\n\t"
+                   "setp.ne.b32 p, %4, 0;\n\t"
+                   "tcgen05.mma.ws.cta_group::1.kind::f8f6f4 [%0], %1, %2, %3, "
+                   "p, 0; \n\t"
+                   "}\n"
+                   :
+                   : "r"(tmem_c), "l"(desc_a), "l"(desc_b),
+                     "r"(uint32_t(idescE >> 32)), "r"(scaleC));
+    }
+  }
+};
+
+template <class a_type, class b_type, class c_type, int M, int N,
+          UMMA::Major a_major, UMMA::Major b_major, UMMA::ScaleIn a_neg,
+          UMMA::ScaleIn b_neg>
+struct MMA_Traits<SM100_MMA_F8F6F4_WS_SS, a_type, b_type, c_type, cute::C<M>,
+                  cute::C<N>, cute::integral_constant<UMMA::Major, a_major>,
+                  cute::integral_constant<UMMA::Major, b_major>,
+                  cute::integral_constant<UMMA::ScaleIn, a_neg>,
+                  cute::integral_constant<UMMA::ScaleIn, b_neg>> {
+  using ValTypeD = c_type;
+  using ValTypeA = a_type;
+  using ValTypeB = b_type;
+  using ValTypeC = c_type;
+  static_assert(cute::sizeof_bits_v<a_type> <= 8 &&
+                    cute::sizeof_bits_v<b_type> <= 8,
+                "SM100_MMA_F8F6F4_WS_SS supports types with leq 8bit types");
+  static_assert(M == 32 || M == 64 || M == 128,
+                "SM100_MMA_F8F6F4_WS_SS M-mode size should be 32, 64 or 128 "
+                "for 1 CTA cluster MMA.");
+  static_assert(
+      N == 64 || N == 128 || N == 256,
+      "SM100_MMA_F8F6F4_WS_SS (with .ws) N-mode size should be 32, 64 or 128");
+  using FrgTypeA = UMMA::smem_desc<a_major>;
+  using FrgTypeB = UMMA::smem_desc<b_major>;
+  using FrgTypeC = UMMA::tmem_frg_ws_1sm<c_type>;
+
+  static_assert(sizeof_bits_v<ValTypeA> <= sizeof_bits_v<uint8_t> &&
+                sizeof_bits_v<ValTypeB> <= sizeof_bits_v<uint8_t>);
+
+  // Logical shape-K is always 256bits, transform to units of elements
+  constexpr static int K = 32;
+
+  using Shape_MNK = Shape<Int<M>, Int<N>, Int<K>>;
+  using ThrID = Layout<_1>;
+  using ALayout =
+      Layout<Shape<_1, Shape<Int<M>, Int<K>>>, Stride<_0, Stride<_1, Int<M>>>>;
+  using BLayout =
+      Layout<Shape<_1, Shape<Int<N>, Int<K>>>, Stride<_0, Stride<_1, Int<N>>>>;
+  using CLayout =
+      Layout<Shape<_1, Shape<Int<M>, Int<N>>>, Stride<_0, Stride<_1, Int<M>>>>;
+
+  UMMA::InstrDescriptor idesc_ =
+      UMMA::make_instr_desc<a_type, b_type, c_type, M, N, a_major, b_major,
+                            a_neg, b_neg>();
+
+  // Accumulate or overwrite C.   1: read C, 0: ignore C [clear accumulators]
+  UMMA::ScaleOut accumulate_ = UMMA::ScaleOut::One;
+
+  template <class TD, class DLayout, class TA, class ALayout, class TB,
+            class BLayout, class TC, class CLayout>
+  CUTE_HOST_DEVICE constexpr friend void
+  mma_unpack(MMA_Traits const &traits, Tensor<TD, DLayout> &D,
+             Tensor<TA, ALayout> const &A, Tensor<TB, BLayout> const &B,
+             Tensor<TC, CLayout> const &C) {
+    static_assert(is_tmem<TD>::value, "Expected tmem in MMA_Atom::call");
+    static_assert(is_rmem<TA>::value,
+                  "Expected desc registers in MMA_Atom::call");
+    static_assert(is_rmem<TB>::value,
+                  "Expected desc registers in MMA_Atom::call");
+    static_assert(is_tmem<TC>::value, "Expected tmem in MMA_Atom::call");
+
+    uint64_t desc_a = A[0];
+    uint64_t desc_b = B[0];
+    uint32_t tmem_c = raw_pointer_cast(D.data());
+    uint64_t idesc = UMMA::make_runtime_instr_desc<>(traits.idesc_);
+
+    SM100_MMA_F8F6F4_WS_SS::fma(desc_a, desc_b, tmem_c,
+                                uint32_t(traits.accumulate_), idesc);
+  }
+};
+
+namespace tl_tcgen5mma {
+
+using cutlass::gemm::collective::detail::sm100_smem_selector;
+
+template <typename A_type, typename B_type, typename C_type, int M, int N,
+          int K, UMMA::Major a_major, UMMA::Major b_major,
+          typename Enable = void>
+struct DispatchInstruction;
+
+template <int M, int N, int K, UMMA::Major a_major, UMMA::Major b_major>
+struct DispatchInstruction<bfloat16_t, bfloat16_t, float, M, N, K, a_major,
+                           b_major, std::enable_if_t<M == 128 && K == 16>> {
+  using MMA = SM100_MMA_F16BF16_SS<bfloat16_t, bfloat16_t, float, M, N, a_major,
+                                   b_major>;
+};
+
+template <int M, int N, int K, UMMA::Major a_major, UMMA::Major b_major>
+struct DispatchInstruction<bfloat16_t, bfloat16_t, float, M, N, K, a_major,
+                           b_major,
+                           std::enable_if_t<(M == 64 || M == 32) && K == 16>> {
+  using MMA = SM100_MMA_F16BF16_WS_SS<bfloat16_t, bfloat16_t, float, M, N,
+                                      a_major, b_major>;
+};
+
+template <int M, int N, int K, UMMA::Major a_major, UMMA::Major b_major>
+struct DispatchInstruction<half_t, half_t, float, M, N, K, a_major, b_major,
+                           std::enable_if_t<M == 128 && K == 16>> {
+  using MMA =
+      SM100_MMA_F16BF16_SS<half_t, half_t, float, M, N, a_major, b_major>;
+};
+
+template <int M, int N, int K, UMMA::Major a_major, UMMA::Major b_major>
+struct DispatchInstruction<half_t, half_t, float, M, N, K, a_major, b_major,
+                           std::enable_if_t<(M == 64 || M == 32) && K == 16>> {
+  using MMA =
+      SM100_MMA_F16BF16_WS_SS<half_t, half_t, float, M, N, a_major, b_major>;
+};
+
+template <int M, int N, int K, UMMA::Major a_major, UMMA::Major b_major>
+struct DispatchInstruction<fp8_e4_t, fp8_e4_t, float, M, N, K, a_major, b_major,
+                           std::enable_if_t<M == 128 && K == 32>> {
+  using MMA = MMA_Traits<SM100_MMA_F8F6F4_SS, fp8_e4_t, fp8_e4_t, float, Int<M>,
+                         Int<N>, integral_constant<UMMA::Major, a_major>,
+                         integral_constant<UMMA::Major, b_major>,
+                         integral_constant<UMMA::ScaleIn, UMMA::ScaleIn::One>,
+                         integral_constant<UMMA::ScaleIn, UMMA::ScaleIn::One>>;
+};
+
+template <int M, int N, int K, UMMA::Major a_major, UMMA::Major b_major>
+struct DispatchInstruction<fp8_e4_t, fp8_e4_t, float, M, N, K, a_major, b_major,
+                           std::enable_if_t<(M == 64 || M == 32) && K == 32>> {
+  using MMA =
+      MMA_Traits<SM100_MMA_F8F6F4_WS_SS, fp8_e4_t, fp8_e4_t, float, Int<M>,
+                 Int<N>, integral_constant<UMMA::Major, a_major>,
+                 integral_constant<UMMA::Major, b_major>,
+                 integral_constant<UMMA::ScaleIn, UMMA::ScaleIn::One>,
+                 integral_constant<UMMA::ScaleIn, UMMA::ScaleIn::One>>;
+};
+
+template <int M, int N, int K, UMMA::Major a_major, UMMA::Major b_major>
+struct DispatchInstruction<fp8_e5_t, fp8_e5_t, float, M, N, K, a_major, b_major,
+                           std::enable_if_t<M == 128 && K == 32>> {
+  using MMA = MMA_Traits<SM100_MMA_F8F6F4_SS, fp8_e5_t, fp8_e5_t, float, Int<M>,
+                         Int<N>, integral_constant<UMMA::Major, a_major>,
+                         integral_constant<UMMA::Major, b_major>,
+                         integral_constant<UMMA::ScaleIn, UMMA::ScaleIn::One>,
+                         integral_constant<UMMA::ScaleIn, UMMA::ScaleIn::One>>;
+};
+
+template <int M, int N, int K, UMMA::Major a_major, UMMA::Major b_major>
+struct DispatchInstruction<fp8_e5_t, fp8_e5_t, float, M, N, K, a_major, b_major,
+                           std::enable_if_t<(M == 64 || M == 32) && K == 32>> {
+  using MMA =
+      MMA_Traits<SM100_MMA_F8F6F4_WS_SS, fp8_e5_t, fp8_e5_t, float, Int<M>,
+                 Int<N>, integral_constant<UMMA::Major, a_major>,
+                 integral_constant<UMMA::Major, b_major>,
+                 integral_constant<UMMA::ScaleIn, UMMA::ScaleIn::One>,
+                 integral_constant<UMMA::ScaleIn, UMMA::ScaleIn::One>>;
+};
+
+template <int M, int N, int K, int AtomM, int AtomN, int AtomK, bool trans_A,
+          bool trans_B, typename A_type_raw, typename B_type_raw,
+          typename C_type_raw>
+class GemmTensorOp {
+public:
+  using A_type =
+      typename std::conditional<std::is_same<A_type_raw, float>::value,
+                                tfloat32_t, A_type_raw>::type;
+  using B_type =
+      typename std::conditional<std::is_same<B_type_raw, float>::value,
+                                tfloat32_t, B_type_raw>::type;
+  using C_type = C_type_raw;
+
+  static_assert(AtomM == 128 || AtomM == 64 || AtomM == 32);
+
+  static constexpr UMMA::Major UmmaMajorA =
+      trans_A ? UMMA::Major::MN : UMMA::Major::K;
+  static constexpr UMMA::Major UmmaMajorB =
+      trans_B ? UMMA::Major::K : UMMA::Major::MN;
+
+  using SmemLayoutAtomA =
+      decltype(sm100_smem_selector<UmmaMajorA, A_type, Int<M>, Int<K>>());
+  using SmemLayoutAtomB =
+      decltype(sm100_smem_selector<UmmaMajorB, B_type, Int<N>, Int<K>>());
+
+  using SmemLayoutA = decltype(tile_to_shape(
+      SmemLayoutAtomA{}, Shape<Int<M>, Int<K>>{},
+      conditional_t<trans_A, Step<_2, _1>, Step<_1, _2>>{}));
+  using SmemLayoutB = decltype(tile_to_shape(
+      SmemLayoutAtomB{}, Shape<Int<N>, Int<K>>{},
+      conditional_t<trans_B, Step<_1, _2>, Step<_2, _1>>{}));
+
+  static CUTE_DEVICE void body_ss(A_type_raw *pA, B_type_raw *pB, uint32_t pC,
+                                  uint64_t *umma_bar_ptr, bool clear_accum) {
+    Tensor sA = make_tensor(make_smem_ptr(reinterpret_cast<A_type *>(pA)),
+                            SmemLayoutA{});
+    Tensor sB = make_tensor(make_smem_ptr(reinterpret_cast<B_type *>(pB)),
+                            SmemLayoutB{});
+
+    // TODO (lei): Normal TCGEN5MMA (the one w/o ws) don't saturate all 128
+    // lanes when M == 64
+    // (see layout F in
+    // https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-data-path-layout-f)
+    // So we use the .ws variant here
+    using MmaAtom =
+        typename DispatchInstruction<A_type, B_type, C_type, AtomM, AtomN,
+                                     AtomK, UmmaMajorA, UmmaMajorB>::MMA;
+    auto tiled_mma = make_tiled_mma(MmaAtom{}, Layout<Shape<_1>>{},
+                                    Tile<Int<M>, Int<N>, Int<K>>{});
+    auto thr_mma = tiled_mma.get_slice(_0{});
+    tiled_mma.accumulate_ =
+        clear_accum ? UMMA::ScaleOut::Zero : UMMA::ScaleOut::One;
+    Tensor acc = partition_fragment_C(tiled_mma, Shape<Int<M>, Int<N>>{});
+    acc.data() = pC;
+
+    Tensor sA_frag = thr_mma.partition_fragment_A(sA);
+    Tensor sB_frag = thr_mma.partition_fragment_B(sB);
+    CUTLASS_PRAGMA_UNROLL
+    for (int k_block = 0; k_block < size<2>(sA_frag); ++k_block) {
+      cute::gemm(tiled_mma, sA_frag(_, _, k_block), sB_frag(_, _, k_block),
+                 acc);
+      tiled_mma.accumulate_ = UMMA::ScaleOut::One;
+    }
+
+    cutlass::arch::umma_arrive(umma_bar_ptr);
+  }
+};
+
+} // namespace tl_tcgen5mma
+
+} // namespace cute
+
+namespace tl {
+
+using tl_mma::gemm_rs;
+using tl_mma::gemm_sr;
+using tl_mma::gemm_ss;
+
+// TODO (lei): Implement gemm_ts
+// template <int M, int N, int K, int warp_m, int warp_n, bool trans_A, bool
+// trans_B, bool clear_accum, typename A_type, typename B_type, typename C_type>
+// TL_DEVICE void gemm_ts(A_type *pA, B_type *pB, C_type *accum, uint64_t
+// *umma_bar_ptr) {
+// }
+
+template <int M, int N, int K, int AtomM, int AtomN, int AtomK, bool trans_A,
+          bool trans_B, typename C_type, typename A_type, typename B_type,
+          typename Barrier_type>
+TL_DEVICE void tcgen5mma_gemm_ss(A_type *pA, B_type *pB, uint32_t accum,
+                                 Barrier_type *umma_bar_ptr, bool clear_accum) {
+  using MMA =
+      cute::tl_tcgen5mma::GemmTensorOp<M, N, K, AtomM, AtomN, AtomK, trans_A,
+                                       trans_B, A_type, B_type, C_type>;
+  MMA::body_ss(pA, pB, accum, reinterpret_cast<uint64_t *>(umma_bar_ptr),
+               clear_accum);
+}
+
+} // namespace tl
diff --git a/src/tl_templates/cuda/gemm_sm120.h b/src/tl_templates/cuda/gemm_sm120.h
new file mode 100644
index 00000000..122f5664
--- /dev/null
+++ b/src/tl_templates/cuda/gemm_sm120.h
@@ -0,0 +1,9 @@
+#pragma once
+
+#include "gemm_mma.h"
+
+namespace tl {
+using tl_mma::gemm_rs;
+using tl_mma::gemm_sr;
+using tl_mma::gemm_ss;
+} // namespace tl
diff --git a/src/tl_templates/cuda/gemm_sm80.h b/src/tl_templates/cuda/gemm_sm80.h
index 826cb5ec..122f5664 100644
--- a/src/tl_templates/cuda/gemm_sm80.h
+++ b/src/tl_templates/cuda/gemm_sm80.h
@@ -1,389 +1,9 @@
 #pragma once
 
-#include <cute/algorithm/clear.hpp>
-#include <cute/arch/mma_sm80.hpp>
-#include <cute/atom/mma_atom.hpp>
-#include <cute/underscore.hpp>
-
-#include "common.h"
-
-namespace cute {
-
-template <typename A_type, typename B_type, typename C_type, int num_warp_m,
-          int num_warp_n, int N>
-struct DispatchInstruction;
-
-using _X = Underscore;
-
-#if (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 800))
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<half_t, half_t, half_t, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x16_F16F16F16F16_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<half_t, half_t, float, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x16_F32F16F16F32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<bfloat16_t, bfloat16_t, float, num_warp_m,
-                           num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x16_F32BF16BF16F32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<tfloat32_t, tfloat32_t, float, num_warp_m,
-                           num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x8_F32TF32TF32F32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<int8_t, int8_t, int, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x32_S32S8S8S32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<double, double, double, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_8x8x4_F64F64F64F64_TN>;
-  using MMA_Group = Tile<Int<num_warp_m * 16>, Int<num_warp_n * 16>, _X>;
-};
-#elif (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 750))
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<half_t, half_t, float, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM75_16x8x8_F32F16F16F32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _16>;
-};
-#endif
-
-template <int N, int num_warp_n, bool transpose> struct SelectCopy {
-  static constexpr int remainder = (N / num_warp_n) % 16;
-  using type = std::conditional_t<
-      remainder == 4 || remainder == 8 || remainder == 0,
-      std::conditional_t<
-          transpose,
-          std::conditional_t<
-              remainder == 4, SM75_U32x1_LDSM_N,
-              std::conditional_t<remainder == 8, SM75_U32x2_LDSM_N,
-                                 SM75_U32x4_LDSM_N>>,
-          std::conditional_t<
-              remainder == 4, SM75_U16x2_LDSM_T,
-              std::conditional_t<remainder == 8, SM75_U16x4_LDSM_T,
-                                 SM75_U16x8_LDSM_T>>>,
-      DefaultCopy>;
-};
-
-template <int Bits, int N, int K, bool K_inner, int num_warp_n,
-          typename Enable = void>
-struct OperandTraits {
-  // Primary template, use padded layout and default copy
-  static constexpr int stride = K_inner ? K : N;
-  static constexpr int padded =
-      stride % (256 / Bits) == 0 ? stride + 128 / Bits : stride;
-  using Layout = typename std::conditional<
-      K_inner, Layout<Shape<Int<N>, Int<K>>, Shape<Int<padded>, _1>>,
-      Layout<Shape<Int<N>, Int<K>>, Shape<_1, Int<padded>>>>::type;
-  using Copy = DefaultCopy;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<16, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 64 == 32>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 3, 3>{}, Layout<Shape<_8, _32>, Stride<_32, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<16, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 64 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 3, 3>{}, Layout<Shape<_8, _64>, Stride<_64, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<16, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 64 == 32>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 3, 3>{}, Layout<Shape<_32, _8>, Stride<_1, _32>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, false>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<16, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 64 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 3, 3>{}, Layout<Shape<_64, _8>, Stride<_1, _64>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, false>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<32, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 32 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 2, 3>{}, Layout<Shape<_8, _32>, Stride<_32, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<32, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 32 == 16>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 2, 3>{}, Layout<Shape<_8, _16>, Stride<_16, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<32, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 32 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 2, 3>{}, Layout<Shape<_32, _8>, Stride<_1, _32>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = UniversalCopy<tfloat32_t>;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<32, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 32 == 16>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 2, 3>{}, Layout<Shape<_16, _8>, Stride<_1, _16>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = UniversalCopy<tfloat32_t>;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<8, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 128 == 64>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 4, 3>{}, Layout<Shape<_8, _64>, Stride<_64, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<8, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 128 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 4, 3>{}, Layout<Shape<_8, _128>, Stride<_128, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<64, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 16 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 0, 4>{}, Layout<Shape<_4, _16>, Stride<_16, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = DefaultCopy;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<64, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 16 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 2, 2>{}, Layout<Shape<_16, _4>, Stride<_1, _16>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = DefaultCopy;
-};
-
-template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
-          bool trans_B, bool clear_accum, typename A_type_raw,
-          typename B_type_raw, typename C_type_raw>
-class GemmTensorOp {
-public:
-  using A_type =
-      typename std::conditional<std::is_same<A_type_raw, float>::value,
-                                tfloat32_t, A_type_raw>::type;
-  using B_type =
-      typename std::conditional<std::is_same<B_type_raw, float>::value,
-                                tfloat32_t, A_type_raw>::type;
-  using C_type = C_type_raw;
-
-  using Instruction =
-      DispatchInstruction<A_type, B_type, C_type, num_warp_m, num_warp_n, N>;
-
-  using OperandATraits =
-      OperandTraits<sizeof_bits<A_type>::value, M, K, !trans_A, num_warp_m>;
-  using OperandBTraits =
-      OperandTraits<sizeof_bits<B_type>::value, N, K, trans_B, num_warp_n>;
-
-  using SmemLayoutA = typename OperandATraits::Layout;
-  using SmemLayoutB = typename OperandBTraits::Layout;
-  using SmemCopyA = Copy_Atom<typename OperandATraits::Copy, A_type>;
-  using SmemCopyB = Copy_Atom<typename OperandBTraits::Copy, B_type>;
-
-  using TileMma = TiledMMA<typename Instruction::MMA,
-                           Layout<Shape<Int<num_warp_m>, Int<num_warp_n>, _1>>,
-                           typename Instruction::MMA_Group>;
-
-  template <class... Args>
-  static CUTE_DEVICE auto remove_swizzle(Layout<Args...> const &layout) {
-    return layout;
-  }
-  // In fp16, when layout is KxN and n_warp is 1 and N % 64 == 0
-  // the original layout fail to compile, currently using this as a workaround
-  template <class... Args>
-  static CUTE_DEVICE auto
-  remove_swizzle(ComposedLayout<Args...> const &layout) {
-    if constexpr (sizeof(A_type) == 2)
-      return layout.layout_b();
-    else
-      return layout;
-  }
-
-  static CUTE_DEVICE void body(A_type_raw *pA, B_type_raw *pB, C_type_raw *pC) {
-    const int tid = threadIdx.x;
-    Tensor sA = make_tensor(make_smem_ptr(reinterpret_cast<A_type *>(pA)),
-                            SmemLayoutA{});
-    Tensor sB = make_tensor(make_smem_ptr(reinterpret_cast<B_type *>(pB)),
-                            SmemLayoutB{});
-    TileMma tiled_mma;
-    auto thr_mma = tiled_mma.get_thread_slice(tid);
-    auto tiled_copy_A = make_tiled_copy_A(SmemCopyA{}, tiled_mma);
-    auto tiled_copy_B = make_tiled_copy_B(SmemCopyB{}, tiled_mma);
-    auto thr_copy_A = tiled_copy_A.get_thread_slice(tid);
-    auto thr_copy_B = tiled_copy_B.get_thread_slice(tid);
-
-    Tensor tCrA = thr_mma.partition_fragment_A(sA);
-    Tensor tCrB = thr_mma.partition_fragment_B(sB);
-    Tensor tCsA = thr_copy_A.partition_S(sA);
-    Tensor tCsB = thr_copy_B.partition_S(sB);
-
-    Tensor tCrA_copy_view = thr_copy_A.retile_D(tCrA);
-    Tensor tCrB_copy_view = thr_copy_B.retile_D(tCrB);
-
-    Tensor acc =
-        make_tensor(make_rmem_ptr(reinterpret_cast<C_type *>(pC)),
-                    partition_shape_C(tiled_mma, Shape<Int<M>, Int<N>>{}));
-
-    if constexpr (clear_accum) {
-      clear(acc);
-    }
-    // when layout is KxN and n_warp is 1, there seem to be a bug, use this as a
-    // workaround
-    auto tCrA_view = make_tensor(tCrA.data(), remove_swizzle(tCrA.layout()));
-    auto tCrB_view = make_tensor(tCrB.data(), remove_swizzle(tCrB.layout()));
-    CUTE_UNROLL
-    for (int k = 0; k < size<2>(tCrA); ++k) {
-      copy(tiled_copy_A, tCsA(_, _, k), tCrA_copy_view(_, _, k));
-      copy(tiled_copy_B, tCsB(_, _, k), tCrB_copy_view(_, _, k));
-      gemm(tiled_mma, tCrA_view(_, _, k), tCrB_view(_, _, k), acc);
-    }
-  }
-
-  static CUTE_DEVICE void body_rs(A_type_raw *pA, B_type_raw *pB,
-                                  C_type_raw *pC) {
-    const int tid = threadIdx.x;
-    Tensor sB = make_tensor(make_smem_ptr(reinterpret_cast<B_type *>(pB)),
-                            SmemLayoutB{});
-    TileMma tiled_mma;
-    auto thr_mma = tiled_mma.get_thread_slice(tid);
-    auto tiled_copy_B = make_tiled_copy_B(SmemCopyB{}, tiled_mma);
-    auto thr_copy_B = tiled_copy_B.get_thread_slice(tid);
-
-    Tensor tCrB = thr_mma.partition_fragment_B(sB);
-    Tensor tCsB = thr_copy_B.partition_S(sB);
-
-    Tensor tCrB_copy_view = thr_copy_B.retile_D(tCrB);
-
-    Tensor acc =
-        make_tensor(make_rmem_ptr(reinterpret_cast<C_type *>(pC)),
-                    partition_shape_C(tiled_mma, Shape<Int<M>, Int<N>>{}));
-    Tensor tCrA =
-        make_tensor(make_rmem_ptr(reinterpret_cast<A_type *>(pA)),
-                    partition_shape_A(tiled_mma, Shape<Int<M>, Int<K>>{}));
-    if constexpr (clear_accum) {
-      clear(acc);
-    }
-    auto tCrB_view = make_tensor(tCrB.data(), remove_swizzle(tCrB.layout()));
-    copy(tiled_copy_B, tCsB(_, _, 0), tCrB_copy_view(_, _, 0));
-    CUTE_UNROLL
-    for (int k = 0; k < size<2>(tCrA); ++k) {
-      if (k < size<2>(tCrA) - 1) {
-        copy(tiled_copy_B, tCsB(_, _, k + 1), tCrB_copy_view(_, _, k + 1));
-      }
-      gemm(tiled_mma, tCrA(_, _, k), tCrB_view(_, _, k), acc);
-    }
-  }
-
-  static CUTE_DEVICE void body_sr(A_type_raw *pA, B_type_raw *pB,
-                                  C_type_raw *pC) {
-    const int tid = threadIdx.x;
-    Tensor sA = make_tensor(make_smem_ptr(reinterpret_cast<A_type *>(pA)),
-                            SmemLayoutA{});
-    TileMma tiled_mma;
-    auto thr_mma = tiled_mma.get_thread_slice(tid);
-    auto tiled_copy_A = make_tiled_copy_A(SmemCopyA{}, tiled_mma);
-    auto thr_copy_A = tiled_copy_A.get_thread_slice(tid);
-
-    Tensor tCrA = thr_mma.partition_fragment_A(sA);
-    Tensor tCsA = thr_copy_A.partition_S(sA);
-
-    Tensor tCrA_copy_view = thr_copy_A.retile_D(tCrA);
-
-    Tensor acc =
-        make_tensor(make_rmem_ptr(reinterpret_cast<C_type *>(pC)),
-                    partition_shape_C(tiled_mma, Shape<Int<M>, Int<N>>{}));
-    Tensor tCrB =
-        make_tensor(make_rmem_ptr(reinterpret_cast<B_type *>(pB)),
-                    partition_shape_B(tiled_mma, Shape<Int<N>, Int<K>>{}));
-    if constexpr (clear_accum) {
-      clear(acc);
-    }
-    auto tCrA_view = make_tensor(tCrA.data(), remove_swizzle(tCrA.layout()));
-    copy(tiled_copy_A, tCsA(_, _, 0), tCrA_copy_view(_, _, 0));
-    CUTE_UNROLL
-    for (int k = 0; k < size<2>(tCrA); ++k) {
-      if (k < size<2>(tCrA) - 1) {
-        copy(tiled_copy_A, tCsA(_, _, k + 1), tCrA_copy_view(_, _, k + 1));
-      }
-      gemm(tiled_mma, tCrA_view(_, _, k), tCrB(_, _, k), acc);
-    }
-  }
-};
-
-} // namespace cute
+#include "gemm_mma.h"
 
 namespace tl {
-
-template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
-          bool trans_B, bool clear_accum, typename A_type, typename B_type,
-          typename C_type>
-CUTLASS_DEVICE void gemm_ss(A_type *pA, B_type *pB, C_type *accum) {
-  using MMA = cute::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
-                                 trans_B, clear_accum, A_type, B_type, C_type>;
-  MMA::body(pA, pB, accum);
-}
-
-template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
-          bool trans_B, bool clear_accum, typename A_type, typename B_type,
-          typename C_type>
-CUTLASS_DEVICE void gemm_rs(A_type *pA, B_type *pB, C_type *accum) {
-  using MMA = cute::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
-                                 trans_B, clear_accum, A_type, B_type, C_type>;
-  MMA::body_rs(pA, pB, accum);
-}
-
-template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
-          bool trans_B, bool clear_accum, typename A_type, typename B_type,
-          typename C_type>
-CUTLASS_DEVICE void gemm_sr(A_type *pA, B_type *pB, C_type *accum) {
-  using MMA = cute::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
-                                 trans_B, clear_accum, A_type, B_type, C_type>;
-  MMA::body_sr(pA, pB, accum);
-}
-
+using tl_mma::gemm_rs;
+using tl_mma::gemm_sr;
+using tl_mma::gemm_ss;
 } // namespace tl
diff --git a/src/tl_templates/cuda/gemm_sm89.h b/src/tl_templates/cuda/gemm_sm89.h
index 37504e59..d64ae9e2 100644
--- a/src/tl_templates/cuda/gemm_sm89.h
+++ b/src/tl_templates/cuda/gemm_sm89.h
@@ -1,409 +1,13 @@
 #pragma once
 
-#include <cute/algorithm/clear.hpp>
-#include <cute/arch/mma_sm80.hpp>
 #include <cute/arch/mma_sm89.hpp>
 
-#include <cute/atom/mma_atom.hpp>
-#include <cute/atom/mma_traits.hpp>
-#include <cute/underscore.hpp>
-
-#include "common.h"
 #include "cuda_fp8.h"
 
-namespace cute {
-
-template <typename A_type, typename B_type, typename C_type, int num_warp_m,
-          int num_warp_n, int N>
-struct DispatchInstruction;
-
-using _X = Underscore;
-
-#if (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 890))
-
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<fp8_e4_t, fp8_e4_t, float, num_warp_m, num_warp_n,
-                           N> {
-  using MMA = MMA_Atom<SM89_16x8x32_F32E4M3E4M3F32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<fp8_e5_t, fp8_e5_t, float, num_warp_m, num_warp_n,
-                           N> {
-  using MMA = MMA_Atom<SM89_16x8x32_F32E5M2E5M2F32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<half_t, half_t, half_t, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x16_F16F16F16F16_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<half_t, half_t, float, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x16_F32F16F16F32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<bfloat16_t, bfloat16_t, float, num_warp_m,
-                           num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x16_F32BF16BF16F32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<tfloat32_t, tfloat32_t, float, num_warp_m,
-                           num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x8_F32TF32TF32F32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<int8_t, int8_t, int, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x32_S32S8S8S32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<double, double, double, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_8x8x4_F64F64F64F64_TN>;
-  using MMA_Group = Tile<Int<num_warp_m * 16>, Int<num_warp_n * 16>, _X>;
-};
-#elif (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 750))
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<half_t, half_t, float, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM75_16x8x8_F32F16F16F32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _16>;
-};
-#endif
-
-template <int N, int num_warp_n, bool transpose> struct SelectCopy {
-  static constexpr int remainder = (N / num_warp_n) % 16;
-  using type = std::conditional_t<
-      remainder == 4 || remainder == 8 || remainder == 0,
-      std::conditional_t<
-          transpose,
-          std::conditional_t<
-              remainder == 4, SM75_U32x1_LDSM_N,
-              std::conditional_t<remainder == 8, SM75_U32x2_LDSM_N,
-                                 SM75_U32x4_LDSM_N>>,
-          std::conditional_t<
-              remainder == 4, SM75_U16x2_LDSM_T,
-              std::conditional_t<remainder == 8, SM75_U16x4_LDSM_T,
-                                 SM75_U16x8_LDSM_T>>>,
-      DefaultCopy>;
-};
-
-template <int Bits, int N, int K, bool K_inner, int num_warp_n,
-          typename Enable = void>
-struct OperandTraits {
-  // Primary template, use padded layout and default copy
-  static constexpr int stride = K_inner ? K : N;
-  static constexpr int padded =
-      stride % (256 / Bits) == 0 ? stride + 128 / Bits : stride;
-  using Layout = typename std::conditional<
-      K_inner, Layout<Shape<Int<N>, Int<K>>, Shape<Int<padded>, _1>>,
-      Layout<Shape<Int<N>, Int<K>>, Shape<_1, Int<padded>>>>::type;
-  using Copy = DefaultCopy;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<16, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 64 == 32>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 3, 3>{}, Layout<Shape<_8, _32>, Stride<_32, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<16, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 64 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 3, 3>{}, Layout<Shape<_8, _64>, Stride<_64, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<16, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 64 == 32>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 3, 3>{}, Layout<Shape<_32, _8>, Stride<_1, _32>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, false>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<16, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 64 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 3, 3>{}, Layout<Shape<_64, _8>, Stride<_1, _64>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, false>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<32, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 32 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 2, 3>{}, Layout<Shape<_8, _32>, Stride<_32, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<32, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 32 == 16>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 2, 3>{}, Layout<Shape<_8, _16>, Stride<_16, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<32, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 32 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 2, 3>{}, Layout<Shape<_32, _8>, Stride<_1, _32>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = UniversalCopy<tfloat32_t>;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<32, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 32 == 16>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 2, 3>{}, Layout<Shape<_16, _8>, Stride<_1, _16>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = UniversalCopy<tfloat32_t>;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<8, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 128 == 64>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 4, 3>{}, Layout<Shape<_8, _64>, Stride<_64, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<8, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 128 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 4, 3>{}, Layout<Shape<_8, _128>, Stride<_128, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<64, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 16 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 0, 4>{}, Layout<Shape<_4, _16>, Stride<_16, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = DefaultCopy;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<64, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 16 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 2, 2>{}, Layout<Shape<_16, _4>, Stride<_1, _16>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = DefaultCopy;
-};
-
-template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
-          bool trans_B, bool clear_accum, typename A_type_raw,
-          typename B_type_raw, typename C_type_raw>
-class GemmTensorOp {
-public:
-  using A_type =
-      typename std::conditional<std::is_same<A_type_raw, float>::value,
-                                tfloat32_t, A_type_raw>::type;
-  using B_type =
-      typename std::conditional<std::is_same<B_type_raw, float>::value,
-                                tfloat32_t, A_type_raw>::type;
-  using C_type = C_type_raw;
-
-  using Instruction =
-      DispatchInstruction<A_type, B_type, C_type, num_warp_m, num_warp_n, N>;
-
-  using OperandATraits =
-      OperandTraits<sizeof_bits<A_type>::value, M, K, !trans_A, num_warp_m>;
-  using OperandBTraits =
-      OperandTraits<sizeof_bits<B_type>::value, N, K, trans_B, num_warp_n>;
-
-  using SmemLayoutA = typename OperandATraits::Layout;
-  using SmemLayoutB = typename OperandBTraits::Layout;
-  using SmemCopyA = Copy_Atom<typename OperandATraits::Copy, A_type>;
-  using SmemCopyB = Copy_Atom<typename OperandBTraits::Copy, B_type>;
-
-  using TileMma = TiledMMA<typename Instruction::MMA,
-                           Layout<Shape<Int<num_warp_m>, Int<num_warp_n>, _1>>,
-                           typename Instruction::MMA_Group>;
-
-  template <class... Args>
-  static CUTE_DEVICE auto remove_swizzle(Layout<Args...> const &layout) {
-    return layout;
-  }
-  // In fp16, when layout is KxN and n_warp is 1 and N % 64 == 0
-  // the original layout fail to compile, currently using this as a workaround
-  template <class... Args>
-  static CUTE_DEVICE auto
-  remove_swizzle(ComposedLayout<Args...> const &layout) {
-    if constexpr (sizeof(A_type) == 2)
-      return layout.layout_b();
-    else
-      return layout;
-  }
-
-  static CUTE_DEVICE void body(A_type_raw *pA, B_type_raw *pB, C_type_raw *pC) {
-    const int tid = threadIdx.x;
-    Tensor sA = make_tensor(make_smem_ptr(reinterpret_cast<A_type *>(pA)),
-                            SmemLayoutA{});
-    Tensor sB = make_tensor(make_smem_ptr(reinterpret_cast<B_type *>(pB)),
-                            SmemLayoutB{});
-    TileMma tiled_mma;
-    auto thr_mma = tiled_mma.get_thread_slice(tid);
-    auto tiled_copy_A = make_tiled_copy_A(SmemCopyA{}, tiled_mma);
-    auto tiled_copy_B = make_tiled_copy_B(SmemCopyB{}, tiled_mma);
-    auto thr_copy_A = tiled_copy_A.get_thread_slice(tid);
-    auto thr_copy_B = tiled_copy_B.get_thread_slice(tid);
-
-    Tensor tCrA = thr_mma.partition_fragment_A(sA);
-    Tensor tCrB = thr_mma.partition_fragment_B(sB);
-    Tensor tCsA = thr_copy_A.partition_S(sA);
-    Tensor tCsB = thr_copy_B.partition_S(sB);
-
-    Tensor tCrA_copy_view = thr_copy_A.retile_D(tCrA);
-    Tensor tCrB_copy_view = thr_copy_B.retile_D(tCrB);
-
-    Tensor acc =
-        make_tensor(make_rmem_ptr(reinterpret_cast<C_type *>(pC)),
-                    partition_shape_C(tiled_mma, Shape<Int<M>, Int<N>>{}));
-
-    if constexpr (clear_accum) {
-      clear(acc);
-    }
-    // when layout is KxN and n_warp is 1, there seem to be a bug, use this as a
-    // workaround
-    auto tCrA_view = make_tensor(tCrA.data(), remove_swizzle(tCrA.layout()));
-    auto tCrB_view = make_tensor(tCrB.data(), remove_swizzle(tCrB.layout()));
-
-    CUTE_UNROLL
-    for (int k = 0; k < size<2>(tCrA); ++k) {
-      copy(tiled_copy_A, tCsA(_, _, k), tCrA_copy_view(_, _, k));
-      copy(tiled_copy_B, tCsB(_, _, k), tCrB_copy_view(_, _, k));
-      gemm(tiled_mma, tCrA_view(_, _, k), tCrB_view(_, _, k), acc);
-    }
-  }
-
-  static CUTE_DEVICE void body_rs(A_type_raw *pA, B_type_raw *pB,
-                                  C_type_raw *pC) {
-    const int tid = threadIdx.x;
-    Tensor sB = make_tensor(make_smem_ptr(reinterpret_cast<B_type *>(pB)),
-                            SmemLayoutB{});
-    TileMma tiled_mma;
-    auto thr_mma = tiled_mma.get_thread_slice(tid);
-    auto tiled_copy_B = make_tiled_copy_B(SmemCopyB{}, tiled_mma);
-    auto thr_copy_B = tiled_copy_B.get_thread_slice(tid);
-
-    Tensor tCrB = thr_mma.partition_fragment_B(sB);
-    Tensor tCsB = thr_copy_B.partition_S(sB);
-
-    Tensor tCrB_copy_view = thr_copy_B.retile_D(tCrB);
-
-    Tensor acc =
-        make_tensor(make_rmem_ptr(reinterpret_cast<C_type *>(pC)),
-                    partition_shape_C(tiled_mma, Shape<Int<M>, Int<N>>{}));
-    Tensor tCrA =
-        make_tensor(make_rmem_ptr(reinterpret_cast<A_type *>(pA)),
-                    partition_shape_A(tiled_mma, Shape<Int<M>, Int<K>>{}));
-
-    if constexpr (clear_accum) {
-      clear(acc);
-    }
-    auto tCrB_view = make_tensor(tCrB.data(), remove_swizzle(tCrB.layout()));
-    copy(tiled_copy_B, tCsB(_, _, 0), tCrB_copy_view(_, _, 0));
-    CUTE_UNROLL
-    for (int k = 0; k < size<2>(tCrA); ++k) {
-      if (k < size<2>(tCrA) - 1) {
-        copy(tiled_copy_B, tCsB(_, _, k + 1), tCrB_copy_view(_, _, k + 1));
-      }
-      gemm(tiled_mma, tCrA(_, _, k), tCrB_view(_, _, k), acc);
-    }
-  }
-
-  static CUTE_DEVICE void body_sr(A_type_raw *pA, B_type_raw *pB,
-                                  C_type_raw *pC) {
-    const int tid = threadIdx.x;
-    Tensor sA = make_tensor(make_smem_ptr(reinterpret_cast<A_type *>(pA)),
-                            SmemLayoutA{});
-    TileMma tiled_mma;
-    auto thr_mma = tiled_mma.get_thread_slice(tid);
-    auto tiled_copy_A = make_tiled_copy_A(SmemCopyA{}, tiled_mma);
-    auto thr_copy_A = tiled_copy_A.get_thread_slice(tid);
-
-    Tensor tCrA = thr_mma.partition_fragment_A(sA);
-    Tensor tCsA = thr_copy_A.partition_S(sA);
-
-    Tensor tCrA_copy_view = thr_copy_A.retile_D(tCrA);
-
-    Tensor acc =
-        make_tensor(make_rmem_ptr(reinterpret_cast<C_type *>(pC)),
-                    partition_shape_C(tiled_mma, Shape<Int<M>, Int<N>>{}));
-    Tensor tCrB =
-        make_tensor(make_rmem_ptr(reinterpret_cast<B_type *>(pB)),
-                    partition_shape_B(tiled_mma, Shape<Int<N>, Int<K>>{}));
-    if constexpr (clear_accum) {
-      clear(acc);
-    }
-    auto tCrA_view = make_tensor(tCrA.data(), remove_swizzle(tCrA.layout()));
-    copy(tiled_copy_A, tCsA(_, _, 0), tCrA_copy_view(_, _, 0));
-    CUTE_UNROLL
-    for (int k = 0; k < size<2>(tCrA); ++k) {
-      if (k < size<2>(tCrA) - 1) {
-        copy(tiled_copy_A, tCsA(_, _, k + 1), tCrA_copy_view(_, _, k + 1));
-      }
-      gemm(tiled_mma, tCrA_view(_, _, k), tCrB(_, _, k), acc);
-    }
-  }
-};
-
-} // namespace cute
+#include "gemm_mma.h"
 
 namespace tl {
-
-template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
-          bool trans_B, bool clear_accum, typename A_type, typename B_type,
-          typename C_type>
-CUTLASS_DEVICE void gemm_ss(A_type *pA, B_type *pB, C_type *accum) {
-  using MMA = cute::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
-                                 trans_B, clear_accum, A_type, B_type, C_type>;
-  MMA::body(pA, pB, accum);
-}
-
-template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
-          bool trans_B, bool clear_accum, typename A_type, typename B_type,
-          typename C_type>
-CUTLASS_DEVICE void gemm_rs(A_type *pA, B_type *pB, C_type *accum) {
-  using MMA = cute::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
-                                 trans_B, clear_accum, A_type, B_type, C_type>;
-  MMA::body_rs(pA, pB, accum);
-}
-
-template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
-          bool trans_B, bool clear_accum, typename A_type, typename B_type,
-          typename C_type>
-CUTLASS_DEVICE void gemm_sr(A_type *pA, B_type *pB, C_type *accum) {
-  using MMA = cute::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
-                                 trans_B, clear_accum, A_type, B_type, C_type>;
-  MMA::body_sr(pA, pB, accum);
-}
-
+using tl_mma::gemm_rs;
+using tl_mma::gemm_sr;
+using tl_mma::gemm_ss;
 } // namespace tl
diff --git a/src/tl_templates/cuda/gemm_sm90.h b/src/tl_templates/cuda/gemm_sm90.h
index 0555ab91..1aa3ecff 100644
--- a/src/tl_templates/cuda/gemm_sm90.h
+++ b/src/tl_templates/cuda/gemm_sm90.h
@@ -1,14 +1,13 @@
 #pragma once
 
-#include <cute/arch/mma_sm80.hpp>
-#include <cute/arch/mma_sm90.hpp>
-#include <cute/atom/mma_atom.hpp>
+#include "common.h"
+#include "gemm_mma.h"
+#include "intrin.h"
+
 #include <cutlass/arch/barrier.h>
 #include <cutlass/cutlass.h>
 #include <cutlass/gemm/collective/collective_builder.hpp>
 
-#include "common.h"
-
 namespace cute {
 
 using namespace SM90;
@@ -145,430 +144,206 @@ public:
 
 } // namespace tl_wgmma
 
-namespace tl_mma {
-
-template <typename A_type, typename B_type, typename C_type, int num_warp_m,
-          int num_warp_n, int N>
-struct DispatchInstruction;
-
-using _X = Underscore;
-
-#if (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 800))
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<half_t, half_t, half_t, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x16_F16F16F16F16_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<half_t, half_t, float, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x16_F32F16F16F32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<bfloat16_t, bfloat16_t, float, num_warp_m,
-                           num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x16_F32BF16BF16F32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<tfloat32_t, tfloat32_t, float, num_warp_m,
-                           num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x8_F32TF32TF32F32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<int8_t, int8_t, int, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_16x8x32_S32S8S8S32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _X>;
-};
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<double, double, double, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM80_8x8x4_F64F64F64F64_TN>;
-  using MMA_Group = Tile<Int<num_warp_m * 16>, Int<num_warp_n * 16>, _X>;
-};
-#elif (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 750))
-template <int num_warp_m, int num_warp_n, int N>
-struct DispatchInstruction<half_t, half_t, float, num_warp_m, num_warp_n, N> {
-  using MMA = MMA_Atom<SM75_16x8x8_F32F16F16F32_TN>;
-  using MMA_Group = Tile<_X, Int<std::min(num_warp_n * 16, N)>, _16>;
-};
-#endif
-
-template <int Bits, int N, int K, bool K_inner, int num_warp_n,
-          typename Enable = void>
-struct OperandTraits {
-  // Primary template, use padded layout and default copy
-  static constexpr int stride = K_inner ? K : N;
-  static constexpr int padded =
-      stride % (256 / Bits) == 0 ? stride + 128 / Bits : stride;
-  using Layout = typename std::conditional<
-      K_inner, Layout<Shape<Int<N>, Int<K>>, Shape<Int<padded>, _1>>,
-      Layout<Shape<Int<N>, Int<K>>, Shape<_1, Int<padded>>>>::type;
-  using Copy = DefaultCopy;
-};
-
-template <int N, int num_warp_n, bool transpose> struct SelectCopy {
-  static constexpr int remainder = (N / num_warp_n) % 16;
-  using type = std::conditional_t<
-      remainder == 4 || remainder == 8 || remainder == 0,
-      std::conditional_t<
-          transpose,
-          std::conditional_t<
-              remainder == 4, SM75_U32x1_LDSM_N,
-              std::conditional_t<remainder == 8, SM75_U32x2_LDSM_N,
-                                 SM75_U32x4_LDSM_N>>,
-          std::conditional_t<
-              remainder == 4, SM75_U16x2_LDSM_T,
-              std::conditional_t<remainder == 8, SM75_U16x4_LDSM_T,
-                                 SM75_U16x8_LDSM_T>>>,
-      DefaultCopy>;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<16, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 64 == 32>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 3, 3>{}, Layout<Shape<_8, _32>, Stride<_32, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<16, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 64 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 3, 3>{}, Layout<Shape<_8, _64>, Stride<_64, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<16, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 64 == 32>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 3, 3>{}, Layout<Shape<_32, _8>, Stride<_1, _32>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, false>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<16, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 64 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 3, 3>{}, Layout<Shape<_64, _8>, Stride<_1, _64>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, false>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<32, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 32 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 2, 3>{}, Layout<Shape<_8, _32>, Stride<_32, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<32, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 32 == 16>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 2, 3>{}, Layout<Shape<_8, _16>, Stride<_16, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename SelectCopy<N, num_warp_n, true>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<32, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 32 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 2, 3>{}, Layout<Shape<_32, _8>, Stride<_1, _32>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = UniversalCopy<tfloat32_t>;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<32, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 32 == 16>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 2, 3>{}, Layout<Shape<_16, _8>, Stride<_1, _16>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = UniversalCopy<tfloat32_t>;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<8, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 128 == 64>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 4, 3>{}, Layout<Shape<_8, _64>, Stride<_64, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename std::conditional<N == 8 * num_warp_n, SM75_U32x2_LDSM_N,
-                                         SM75_U32x4_LDSM_N>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<8, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 128 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<3, 4, 3>{}, Layout<Shape<_8, _128>, Stride<_128, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = typename std::conditional<N == 8 * num_warp_n, SM75_U32x2_LDSM_N,
-                                         SM75_U32x4_LDSM_N>::type;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<64, N, K, true, num_warp_n,
-                     typename std::enable_if<K % 16 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 0, 4>{}, Layout<Shape<_4, _16>, Stride<_16, _1>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{}));
-  using Copy = DefaultCopy;
-};
-
-template <int N, int K, int num_warp_n>
-struct OperandTraits<64, N, K, false, num_warp_n,
-                     typename std::enable_if<N % 16 == 0>::type> {
-  using LayoutAtom = decltype(composition(
-      Swizzle<2, 2, 2>{}, Layout<Shape<_16, _4>, Stride<_1, _16>>{}));
-  using Layout = decltype(tile_to_shape(LayoutAtom{}, Shape<Int<N>, Int<K>>{},
-                                        Step<_2, _1>{}));
-  using Copy = DefaultCopy;
-};
-
-template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
-          bool trans_B, bool clear_accum, typename A_type_raw,
-          typename B_type_raw, typename C_type_raw>
-class GemmTensorOp {
-public:
-  using A_type =
-      typename std::conditional<std::is_same<A_type_raw, float>::value,
-                                tfloat32_t, A_type_raw>::type;
-  using B_type =
-      typename std::conditional<std::is_same<B_type_raw, float>::value,
-                                tfloat32_t, A_type_raw>::type;
-  using C_type = C_type_raw;
-
-  using Instruction =
-      DispatchInstruction<A_type, B_type, C_type, num_warp_m, num_warp_n, N>;
-
-  using OperandATraits =
-      OperandTraits<sizeof_bits<A_type>::value, M, K, !trans_A, num_warp_m>;
-  using OperandBTraits =
-      OperandTraits<sizeof_bits<B_type>::value, N, K, trans_B, num_warp_n>;
-  using SmemLayoutA = typename OperandATraits::Layout;
-  using SmemLayoutB = typename OperandBTraits::Layout;
-  using SmemCopyA = Copy_Atom<typename OperandATraits::Copy, A_type>;
-  using SmemCopyB = Copy_Atom<typename OperandBTraits::Copy, B_type>;
-
-  using TileMma = TiledMMA<typename Instruction::MMA,
-                           Layout<Shape<Int<num_warp_m>, Int<num_warp_n>, _1>>,
-                           typename Instruction::MMA_Group>;
-
-  template <class... Args>
-  static CUTE_DEVICE auto remove_swizzle(Layout<Args...> const &layout) {
-    return layout;
-  }
-  // In fp16, when layout is KxN and n_warp is 1 and N % 64 == 0
-  // the original layout fail to compile, currently using this as a workaround
-  template <class... Args>
-  static CUTE_DEVICE auto
-  remove_swizzle(ComposedLayout<Args...> const &layout) {
-    if constexpr (sizeof(A_type) == 2)
-      return layout.layout_b();
-    else
-      return layout;
-  }
-
-  static CUTE_DEVICE void body(A_type_raw *pA, B_type_raw *pB, C_type_raw *pC) {
-    const int tid = threadIdx.x;
-    Tensor sA = make_tensor(make_smem_ptr(reinterpret_cast<A_type *>(pA)),
-                            SmemLayoutA{});
-    Tensor sB = make_tensor(make_smem_ptr(reinterpret_cast<B_type *>(pB)),
-                            SmemLayoutB{});
-    TileMma tiled_mma;
-    auto thr_mma = tiled_mma.get_thread_slice(tid);
-    auto tiled_copy_A = make_tiled_copy_A(SmemCopyA{}, tiled_mma);
-    auto tiled_copy_B = make_tiled_copy_B(SmemCopyB{}, tiled_mma);
-    auto thr_copy_A = tiled_copy_A.get_thread_slice(tid);
-    auto thr_copy_B = tiled_copy_B.get_thread_slice(tid);
-
-    Tensor tCrA = thr_mma.partition_fragment_A(sA);
-    Tensor tCrB = thr_mma.partition_fragment_B(sB);
-    Tensor tCsA = thr_copy_A.partition_S(sA);
-    Tensor tCsB = thr_copy_B.partition_S(sB);
-
-    Tensor tCrA_copy_view = thr_copy_A.retile_D(tCrA);
-    Tensor tCrB_copy_view = thr_copy_B.retile_D(tCrB);
-
-    Tensor acc =
-        make_tensor(make_rmem_ptr(reinterpret_cast<C_type *>(pC)),
-                    partition_shape_C(tiled_mma, Shape<Int<M>, Int<N>>{}));
-
-    // when layout is KxN and n_warp is 1, there seem to be a bug, use this as a
-    // workaround
-    auto tCrA_view = make_tensor(tCrA.data(), remove_swizzle(tCrA.layout()));
-    auto tCrB_view = make_tensor(tCrB.data(), remove_swizzle(tCrB.layout()));
-    if constexpr (clear_accum) {
-      clear(acc);
-    }
-    CUTE_UNROLL
-    for (int k = 0; k < size<2>(tCrA); ++k) {
-      copy(tiled_copy_A, tCsA(_, _, k), tCrA_copy_view(_, _, k));
-      copy(tiled_copy_B, tCsB(_, _, k), tCrB_copy_view(_, _, k));
-      gemm(tiled_mma, tCrA_view(_, _, k), tCrB_view(_, _, k), acc);
-    }
-  }
-
-  static CUTE_DEVICE void body_rs(A_type_raw *pA, B_type_raw *pB,
-                                  C_type_raw *pC) {
-    const int tid = threadIdx.x;
-    Tensor sB = make_tensor(make_smem_ptr(reinterpret_cast<B_type *>(pB)),
-                            SmemLayoutB{});
-    TileMma tiled_mma;
-    auto thr_mma = tiled_mma.get_thread_slice(tid);
-    auto tiled_copy_B = make_tiled_copy_B(SmemCopyB{}, tiled_mma);
-    auto thr_copy_B = tiled_copy_B.get_thread_slice(tid);
-
-    Tensor tCrB = thr_mma.partition_fragment_B(sB);
-    Tensor tCsB = thr_copy_B.partition_S(sB);
-
-    Tensor tCrB_copy_view = thr_copy_B.retile_D(tCrB);
-
-    Tensor acc =
-        make_tensor(make_rmem_ptr(reinterpret_cast<C_type *>(pC)),
-                    partition_shape_C(tiled_mma, Shape<Int<M>, Int<N>>{}));
-    Tensor tCrA =
-        make_tensor(make_rmem_ptr(reinterpret_cast<A_type *>(pA)),
-                    partition_shape_A(tiled_mma, Shape<Int<M>, Int<K>>{}));
-    auto tCrB_view = make_tensor(tCrB.data(), remove_swizzle(tCrB.layout()));
-    if constexpr (clear_accum) {
-      clear(acc);
-    }
-    copy(tiled_copy_B, tCsB(_, _, 0), tCrB_copy_view(_, _, 0));
-    CUTE_UNROLL
-    for (int k = 0; k < size<2>(tCrA); ++k) {
-      if (k < size<2>(tCrA) - 1) {
-        copy(tiled_copy_B, tCsB(_, _, k + 1), tCrB_copy_view(_, _, k + 1));
-      }
-      gemm(tiled_mma, tCrA(_, _, k), tCrB_view(_, _, k), acc);
-    }
-  }
-
-  static CUTE_DEVICE void body_sr(A_type_raw *pA, B_type_raw *pB,
-                                  C_type_raw *pC) {
-    const int tid = threadIdx.x;
-    Tensor sA = make_tensor(make_smem_ptr(reinterpret_cast<A_type *>(pA)),
-                            SmemLayoutA{});
-    TileMma tiled_mma;
-    auto thr_mma = tiled_mma.get_thread_slice(tid);
-    auto tiled_copy_A = make_tiled_copy_A(SmemCopyA{}, tiled_mma);
-    auto thr_copy_A = tiled_copy_A.get_thread_slice(tid);
-
-    Tensor tCrA = thr_mma.partition_fragment_A(sA);
-    Tensor tCsA = thr_copy_A.partition_S(sA);
-
-    Tensor tCrA_copy_view = thr_copy_A.retile_D(tCrA);
-
-    Tensor acc =
-        make_tensor(make_rmem_ptr(reinterpret_cast<C_type *>(pC)),
-                    partition_shape_C(tiled_mma, Shape<Int<M>, Int<N>>{}));
-    Tensor tCrB =
-        make_tensor(make_rmem_ptr(reinterpret_cast<B_type *>(pB)),
-                    partition_shape_B(tiled_mma, Shape<Int<N>, Int<K>>{}));
-    auto tCrA_view = make_tensor(tCrA.data(), remove_swizzle(tCrA.layout()));
-    if constexpr (clear_accum) {
-      clear(acc);
-    }
-    copy(tiled_copy_A, tCsA(_, _, 0), tCrA_copy_view(_, _, 0));
-    CUTE_UNROLL
-    for (int k = 0; k < size<2>(tCrA); ++k) {
-      if (k < size<2>(tCrA) - 1) {
-        copy(tiled_copy_A, tCsA(_, _, k + 1), tCrA_copy_view(_, _, k + 1));
-      }
-      gemm(tiled_mma, tCrA_view(_, _, k), tCrB(_, _, k), acc);
-    }
-  }
-};
-
-} // namespace tl_mma
-
 } // namespace cute
+/**
+ * Execute a tiled GEMM where A is read from global memory and B is staged in
+ * shared memory.
+ *
+ * Dispatches to tl_mma::GemmTensorOp<M,N,K,...>::body_rs to perform the
+ * computation.
+ *
+ * @param pA Pointer to the A tile region (device memory).
+ * @param pB Pointer to the B tile region (device memory).
+ * @param accum Pointer to the accumulator/output tile region (device memory).
+ */
+/**
+ * Execute a tiled GEMM where A is staged in shared memory and B is read from
+ * global memory.
+ *
+ * Dispatches to tl_mma::GemmTensorOp<M,N,K,...>::body_sr to perform the
+ * computation.
+ *
+ * @param pA Pointer to the A tile region (device memory).
+ * @param pB Pointer to the B tile region (device memory).
+ * @param accum Pointer to the accumulator/output tile region (device memory).
+ */
+/**
+ * Perform a tiled GEMM (both operands in shared memory or selected backend) and
+ * write to accum.
+ *
+ * If use_wgmma is true, validates wgmma constraints (strides and offsets) and
+ * dispatches to the Hopper wgmma implementation; otherwise dispatches to the
+ * tl_mma implementation.
+ *
+ * @param pA Pointer to the A tile region (device memory).
+ * @param pB Pointer to the B tile region (device memory).
+ * @param accum Pointer to the accumulator/output tile region (device memory).
+ */
+/**
+ * Perform a tiled GEMM with A in global memory and B in shared memory (or
+ * selected backend).
+ *
+ * If use_wgmma is true, validates wgmma constraints (strides and offsets) and
+ * dispatches to the Hopper wgmma read-share implementation; otherwise
+ * dispatches to the tl_mma read-share.
+ *
+ * @param pA Pointer to the A tile region (device memory).
+ * @param pB Pointer to the B tile region (device memory).
+ * @param accum Pointer to the accumulator/output tile region (device memory).
+ */
+/**
+ * Perform a tiled GEMM with A staged in shared memory and B in global memory
+ * (tl_mma only).
+ *
+ * wgmma does not support this variant; caller must set use_wgmma == false.
+ * Dispatches to tl_mma::GemmTensorOp<M,N,K,...>::body_sr.
+ *
+ * @param pA Pointer to the A tile region (device memory).
+ * @param pB Pointer to the B tile region (device memory).
+ * @param accum Pointer to the accumulator/output tile region (device memory).
+ */
+/**
+ * Wait for a warp-group of WMMA/MMA warps to complete.
+ *
+ * Wrapper around cute::warpgroup_wait for the specified number of MMA warps.
+ */
+/**
+ * Synchronize a named barrier across NumMmaThreads MMA threads.
+ *
+ * Calls cutlass::arch::NamedBarrier::sync with the canonical warp-group id.
+ */
+/**
+ * Arrive at a named barrier for NumMmaThreads MMA threads using
+ * architecture-aware mapping.
+ *
+ * Supported NumMmaThreads values: 256 or 384. The function issues one or two
+ * barrier arrives depending on the thread-group topology to ensure proper
+ * rendezvous ordering.
+ */
+/**
+ * Initialize named-barrier state for multi-warp MMA execution.
+ *
+ * For NumMmaThreads == 256 or 384, performs the required initial barrier
+ * arrivals for non-zero canonical warp-group indices to set up subsequent
+ * barrier synchronization.
+ */
 
 namespace tl {
 
-namespace tl_mma {
-
-template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
-          bool trans_B, bool clear_accum, typename A_type, typename B_type,
-          typename C_type>
-CUTLASS_DEVICE void gemm_ss(A_type *pA, B_type *pB, C_type *accum) {
-  using MMA =
-      cute::tl_mma::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
-                                 trans_B, clear_accum, A_type, B_type, C_type>;
-  MMA::body(pA, pB, accum);
-}
-
-template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
-          bool trans_B, bool clear_accum, typename A_type, typename B_type,
-          typename C_type>
-CUTLASS_DEVICE void gemm_rs(A_type *pA, B_type *pB, C_type *accum) {
-  using MMA =
-      cute::tl_mma::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
-                                 trans_B, clear_accum, A_type, B_type, C_type>;
-  MMA::body_rs(pA, pB, accum);
-}
-
 template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
-          bool trans_B, bool clear_accum, typename A_type, typename B_type,
-          typename C_type>
-CUTLASS_DEVICE void gemm_sr(A_type *pA, B_type *pB, C_type *accum) {
-  using MMA =
-      cute::tl_mma::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
-                                 trans_B, clear_accum, A_type, B_type, C_type>;
-  MMA::body_sr(pA, pB, accum);
-}
-
-} // namespace tl_mma
-
-template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
-          bool trans_B, bool clear_accum = false, bool use_wgmma = true,
+          bool trans_B, bool clear_accum = false, int lda = 0, int ldb = 0,
+          int offset_a = 0, int offset_b = 0, bool use_wgmma = true,
           int wg_wait = 0, typename A_type, typename B_type, typename C_type>
 TL_DEVICE void gemm_ss(A_type *pA, B_type *pB, C_type *accum) {
   if constexpr (use_wgmma) {
+    static_assert((trans_A && lda == M) || (!trans_A && lda == K),
+                  "Hopper wgmma doesn't support custom stride for A");
+    static_assert((trans_B && ldb == K) || (!trans_B && ldb == N),
+                  "Hopper wgmma doesn't support custom stride for B");
+    static_assert(offset_a == 0 && offset_b == 0,
+                  "offset_a and offset_b must be zero for wgmma");
     using MMA = cute::tl_wgmma::GemmTensorOp<M, N, K, num_warp_m, num_warp_n,
                                              trans_A, trans_B, clear_accum,
                                              A_type, B_type, C_type>;
     MMA::body<wg_wait>(pA, pB, accum);
   } else {
-    using MMA = cute::tl_mma::GemmTensorOp<M, N, K, num_warp_m, num_warp_n,
-                                           trans_A, trans_B, clear_accum,
-                                           A_type, B_type, C_type>;
+    using MMA =
+        cute::tl_mma::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
+                                   trans_B, clear_accum, lda, ldb, offset_a,
+                                   offset_b, A_type, B_type, C_type>;
     MMA::body(pA, pB, accum);
   }
 }
 
 template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
-          bool trans_B, bool clear_accum = false, bool use_wgmma = true,
+          bool trans_B, bool clear_accum = false, int lda = 0, int ldb = 0,
+          int offset_a = 0, int offset_b = 0, bool use_wgmma = true,
           int wg_wait = 0, typename A_type, typename B_type, typename C_type>
-TL_DEVICE void gemm_rs(A_type *pA, B_type *pB, C_type *accum) {
+TL_DEVICE /**
+           * Perform a read-share (B in shared memory, A in global) tiled GEMM
+           * and accumulate into `accum`.
+           *
+           * Dispatches at compile time to either the Hopper wgmma
+           * implementation or the fallback MMA implementation depending on
+           * `use_wgmma`. The selected GemmTensorOp::body_rs performs the
+           * region-tiled GEMM loop and updates the accumulator in-place.
+           *
+           * When `use_wgmma == true`, this function enforces wgmma constraints
+           * at compile time:
+           * - A's leading dimension must equal (trans_A ? M : K)
+           * - B's leading dimension must equal (trans_B ? K : N)
+           * - offset_a and offset_b must be zero
+           *
+           * @param pA Pointer to operand A (global memory). Layout/stride
+           * expectations depend on template parameters.
+           * @param pB Pointer to operand B (base for shared-memory staging).
+           * Layout/stride expectations depend on template parameters.
+           * @param accum Pointer to the accumulator/output C buffer updated
+           * in-place.
+           */
+    void
+    gemm_rs(A_type *pA, B_type *pB, C_type *accum) {
   if constexpr (use_wgmma) {
+    static_assert((trans_A && lda == M) || (!trans_A && lda == K),
+                  "Hopper wgmma doesn't support custom stride for A");
+    static_assert((trans_B && ldb == K) || (!trans_B && ldb == N),
+                  "Hopper wgmma doesn't support custom stride for B");
+    static_assert(offset_a == 0 && offset_b == 0,
+                  "offset_a and offset_b must be zero for wgmma");
     using MMA = cute::tl_wgmma::GemmTensorOp<M, N, K, num_warp_m, num_warp_n,
                                              trans_A, trans_B, clear_accum,
                                              A_type, B_type, C_type>;
     MMA::body_rs<wg_wait>(pA, pB, accum);
   } else {
-    using MMA = cute::tl_mma::GemmTensorOp<M, N, K, num_warp_m, num_warp_n,
-                                           trans_A, trans_B, clear_accum,
-                                           A_type, B_type, C_type>;
+    using MMA =
+        cute::tl_mma::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
+                                   trans_B, clear_accum, lda, ldb, offset_a,
+                                   offset_b, A_type, B_type, C_type>;
     MMA::body_rs(pA, pB, accum);
   }
 }
 
-template <int num_mma> TL_DEVICE void wait_wgmma() {
+template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
+          bool trans_B, bool clear_accum = false, int lda = 0, int ldb = 0,
+          int offset_a = 0, int offset_b = 0, bool use_wgmma = true,
+          int wg_wait = 0, typename A_type, typename B_type, typename C_type>
+TL_DEVICE /**
+           * Perform a non-wgmma tiled GEMM where A regions are staged into
+           * shared memory and B is read directly from global memory,
+           * accumulating into `accum`.
+           *
+           * This overload dispatches to the tl_mma::GemmTensorOp::body_sr
+           * implementation. Must be instantiated with `use_wgmma = false`
+           * (enforced via static_assert).
+           *
+           * @param pA Pointer to the A operand in global memory (source that
+           * will be staged to shared memory).
+           * @param pB Pointer to the B operand in global memory (read
+           * directly).
+           * @param accum Pointer to the output accumulator matrix in global
+           * memory.
+           */
+    void
+    gemm_sr(A_type *pA, B_type *pB, C_type *accum) {
+  static_assert(!use_wgmma, "wgmma doesn't support gemm_sr");
+  using MMA =
+      cute::tl_mma::GemmTensorOp<M, N, K, num_warp_m, num_warp_n, trans_A,
+                                 trans_B, clear_accum, lda, ldb, offset_a,
+                                 offset_b, A_type, B_type, C_type>;
+  MMA::body_sr(pA, pB, accum);
+}
+
+template <int num_mma>
+TL_DEVICE /**
+           * Wait for all WMMA/MMA warps in the current warp-group to
+           * synchronize.
+           *
+           * Blocks until the warp-group-wide rendezvous for `num_mma` MMA lanes
+           * completes, ensuring all participating warps have arrived before
+           * proceeding.
+           */
+    void
+    wait_wgmma() {
   cute::warpgroup_wait<num_mma>();
 }
 
diff --git a/src/tl_templates/cuda/gemm_sp.h b/src/tl_templates/cuda/gemm_sp.h
index bd9cadcd..f40a7bd0 100644
--- a/src/tl_templates/cuda/gemm_sp.h
+++ b/src/tl_templates/cuda/gemm_sp.h
@@ -1,6 +1,6 @@
 #pragma once
 #if (defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 900))
 #include "gemm_sp_sm90.h"
-#else
-
+#else(defined(__CUDA_ARCH_LIST__) && (__CUDA_ARCH_LIST__ >= 800))
+#include "gemm_sp_sm80.h"
 #endif
diff --git a/src/tl_templates/cuda/gemm_sp_sm80.h b/src/tl_templates/cuda/gemm_sp_sm80.h
new file mode 100644
index 00000000..f1fc8600
--- /dev/null
+++ b/src/tl_templates/cuda/gemm_sp_sm80.h
@@ -0,0 +1,270 @@
+#include <cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h>
+#include <stdio.h>
+
+namespace tl {
+
+static int const kSparse = 2;
+template <typename T, typename Shape> struct ShapeCheck {
+  static constexpr bool value = false;
+};
+
+template <typename Shape> struct ShapeCheck<cutlass::half_t, Shape> {
+  static constexpr bool value =
+      (Shape::kM % 32 == 0) && (Shape::kN % 32 == 0) && (Shape::kK % 32 == 0);
+};
+
+template <typename Shape> struct ShapeCheck<cutlass::bfloat16_t, Shape> {
+  static constexpr bool value =
+      ShapeCheck<cutlass::half_t, Shape>::value; // Same as half
+};
+
+template <typename Shape> struct ShapeCheck<int8_t, Shape> {
+  static constexpr bool value =
+      (Shape::kM % 16 == 0) && (Shape::kN % 16 == 0) && (Shape::kK % 64 == 0);
+};
+
+template <typename Shape> struct ShapeCheck<uint8_t, Shape> {
+  static constexpr bool value =
+      (Shape::kM % 16 == 0) && (Shape::kN % 16 == 0) && (Shape::kK % 64 == 0);
+};
+
+// ref:
+// https://github.com/NVIDIA/cutlass/blob/main/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h
+template <typename T> struct DispatchInstructionShape {
+  static_assert(!std::is_same_v<T, T>,
+                "Unsupported type for DispatchInstructionShape");
+};
+
+template <> struct DispatchInstructionShape<cutlass::half_t> {
+  using Shape = cutlass::gemm::GemmShape<16, 8, 32>;
+  using Operator = cutlass::arch::OpMultiplyAdd;
+};
+
+template <> struct DispatchInstructionShape<cutlass::bfloat16_t> {
+  using Shape = cutlass::gemm::GemmShape<16, 8, 32>;
+  using Operator = cutlass::arch::OpMultiplyAdd;
+};
+
+// TODO: Not supported for now
+// template<>
+// struct DispatchInstructionShape<cutlass::tfloat32_t> {
+//   using Shape = cutlass::gemm::GemmShape<16, 8, 16>;
+//   using Operator = cutlass::arch::OpMultiplyAdd;
+// };
+
+template <> struct DispatchInstructionShape<int8_t> {
+  using Shape = cutlass::gemm::GemmShape<16, 8, 64>;
+  using Operator = cutlass::arch::OpMultiplyAddSaturate;
+};
+
+template <> struct DispatchInstructionShape<uint8_t> {
+  using Shape = cutlass::gemm::GemmShape<16, 8, 64>;
+  using Operator = cutlass::arch::OpMultiplyAddSaturate;
+};
+
+// TODO: Not supported for now
+// template<>
+// struct DispatchInstructionShape<cutlass::int4b_t> {
+//   using Shape = cutlass::gemm::GemmShape<16, 8, 128>;
+//   using Operator = cutlass::arch::OpMultiplyAddSaturate;
+// };
+
+template <typename T, bool transpose, int M, int K>
+struct DispatchSharedMemoryLayoutA;
+
+template <typename T, int M, int K>
+struct DispatchSharedMemoryLayoutA<T, false, M, K> {
+  using SmemLayoutA = cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<
+      cutlass::sizeof_bits<T>::value, K / kSparse>;
+};
+
+template <typename T, int M, int K>
+struct DispatchSharedMemoryLayoutA<T, true, M, K> {
+  static int const Crosswise_A =
+      cutlass::platform::min(int(128 / sizeof(T)), M);
+  using SmemLayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous<
+      cutlass::sizeof_bits<T>::value, Crosswise_A>;
+};
+
+template <typename T, bool transpose, int N, int K>
+struct DispatchSharedMemoryLayoutB;
+
+template <typename T, int N, int K>
+struct DispatchSharedMemoryLayoutB<T, false, N, K> {
+  static_assert(
+      cutlass::sizeof_bits<T>::value != 8,
+      "int8, uint8, float8 only support column major layout for matrix B");
+  static int const Crosswise_B =
+      cutlass::platform::min(int(128 / sizeof(T)), N);
+  using SmemLayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous<
+      cutlass::sizeof_bits<T>::value, Crosswise_B>;
+};
+
+template <typename T, int N, int K>
+struct DispatchSharedMemoryLayoutB<T, true, N, K> {
+  static int const kCrosswiseB = (K > (1024 / cutlass::sizeof_bits<T>::value))
+                                     ? (1024 / cutlass::sizeof_bits<T>::value)
+                                     : K;
+  using SmemLayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise<
+      cutlass::sizeof_bits<T>::value, kCrosswiseB>;
+};
+
+template <typename T> struct DispatchType {
+  static_assert(std::is_same<T, void>::value, "Unsupported dtype");
+};
+
+template <> struct DispatchType<cutlass::half_t> {
+  using Type = cutlass::half_t;
+};
+
+template <> struct DispatchType<cutlass::bfloat16_t> {
+  using Type = cutlass::bfloat16_t;
+};
+
+template <> struct DispatchType<unsigned char> {
+  using Type = uint8_t;
+};
+
+template <> struct DispatchType<signed char> {
+  using Type = int8_t;
+};
+
+template <typename Shape, int num_warp_m, int num_warp_n, bool trans_A,
+          bool trans_B, bool clear_accum, typename A_type_raw,
+          typename B_type_raw, typename C_type_raw>
+class GemmTensorOp {
+public:
+  static_assert(Shape::kM % num_warp_m == 0);
+  static_assert(Shape::kN % num_warp_n == 0);
+  using ElementA = typename DispatchType<A_type_raw>::Type;
+  using ElementB = typename DispatchType<B_type_raw>::Type;
+  using ElementC = C_type_raw;
+
+  static_assert(std::is_same_v<ElementA, ElementB>,
+                "A and B are not the same type");
+  static_assert(ShapeCheck<ElementA, Shape>::value,
+                "Invalid shape for ElementA");
+
+  using LayoutA =
+      typename std::conditional_t<trans_A, cutlass::layout::ColumnMajor,
+                                  cutlass::layout::RowMajor>;
+  using LayoutB =
+      typename std::conditional_t<trans_B, cutlass::layout::ColumnMajor,
+                                  cutlass::layout::RowMajor>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ThreadblockShape = Shape;
+  using SmemLayoutA =
+      typename DispatchSharedMemoryLayoutA<ElementA, trans_A,
+                                           ThreadblockShape::kM,
+                                           ThreadblockShape::kK>::SmemLayoutA;
+  using SmemLayoutB =
+      typename DispatchSharedMemoryLayoutB<ElementB, trans_B,
+                                           ThreadblockShape::kN,
+                                           ThreadblockShape::kK>::SmemLayoutB;
+
+  using WarpShape = cutlass::gemm::GemmShape<ThreadblockShape::kM / num_warp_m,
+                                             ThreadblockShape::kN / num_warp_n,
+                                             ThreadblockShape::kK>;
+  using InstructionShape = typename DispatchInstructionShape<ElementA>::Shape;
+  using Operator = typename DispatchInstructionShape<ElementA>::Operator;
+  static_assert(WarpShape::kK % InstructionShape::kK == 0,
+                "K dimension must be divisible by instruction shape K.");
+
+  // instruction/warp config
+  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
+      cutlass::arch::SparseMma<InstructionShape, 32, ElementA,
+                               cutlass::layout::RowMajor, ElementB,
+                               cutlass::layout::ColumnMajor, ElementC,
+                               cutlass::layout::RowMajor, Operator>,
+      cutlass::MatrixShape<1, 1>>;
+  using MmaWarp =
+      cutlass::gemm::warp::SparseMmaTensorOp<WarpShape, ElementA, SmemLayoutA,
+                                             ElementB, SmemLayoutB, ElementC,
+                                             LayoutC, Policy>;
+  static_assert(kSparse == MmaWarp::kSparse, "not 2:4 structured sparse");
+
+  using SmemLayoutE = typename MmaWarp::LayoutE;
+  static_assert(std::is_same_v<SmemLayoutE, cutlass::layout::ColumnMajor>,
+                "Meta data layout must be ColumnMajor for sparse mma.");
+
+  // other traits
+  using FragmentA = typename MmaWarp::FragmentA;
+  using FragmentB = typename MmaWarp::FragmentB;
+  using FragmentC = typename MmaWarp::FragmentC;
+  using FragmentE = typename MmaWarp::FragmentE;
+
+  using IteratorA = typename MmaWarp::IteratorA;
+  using IteratorB = typename MmaWarp::IteratorB;
+  using IteratorE = typename MmaWarp::IteratorE;
+
+  using TensorRefA = typename IteratorA::TensorRef;
+  using TensorRefB = typename IteratorB::TensorRef;
+  using TensorRefE = typename IteratorE::TensorRef;
+  using ElementE = typename TensorRefE::Element;
+
+  static int const kElementsPerElementE = MmaWarp::kElementsPerElementE;
+  static_assert(kSparse == MmaWarp::kSparse, "not 2:4 structured sparse");
+
+  using ShapeA = cutlass::MatrixShape<Shape::kM, Shape::kK / kSparse>;
+  using ShapeB = cutlass::MatrixShape<Shape::kK, Shape::kN>;
+  using ShapeE =
+      cutlass::MatrixShape<Shape::kM * 2,
+                           Shape::kK / kSparse / kElementsPerElementE / 2>;
+
+  static int constexpr kKgroups = WarpShape::kK / InstructionShape::kK;
+
+  template <typename E_type_raw>
+  static CUTLASS_DEVICE void
+  body(A_type_raw *pA, E_type_raw *pE, B_type_raw *pB, FragmentC &accum,
+       const int warp_idx_m, const int warp_idx_n, const int lane_id) {
+    MmaWarp mma_op;
+    FragmentA frag_a;
+    FragmentB frag_b;
+    FragmentE frag_e;
+    const TensorRefA ref_A(
+        (ElementA *)pA,
+        MmaWarp::LayoutA::packed({ShapeA::kRow, ShapeA::kColumn}));
+    const TensorRefE ref_E(
+        (ElementE *)pE,
+        MmaWarp::LayoutE::packed({ShapeE::kRow, ShapeE::kColumn}));
+    const TensorRefB ref_B(
+        (ElementB *)pB,
+        MmaWarp::LayoutB::packed({ShapeB::kRow, ShapeB::kColumn}));
+    IteratorA iter_A(ref_A, lane_id);
+    IteratorE iter_E(ref_E, lane_id);
+    IteratorB iter_B(ref_B, lane_id);
+    iter_A.add_tile_offset({warp_idx_m, 0});
+    iter_E.add_tile_offset({warp_idx_m, 0});
+    iter_B.add_tile_offset({0, warp_idx_n});
+    if constexpr (clear_accum) {
+      accum.clear();
+    }
+    CUTLASS_PRAGMA_UNROLL
+    for (int k = 0; k < kKgroups; ++k) {
+      iter_A.load(frag_a);
+      iter_E.load(frag_e);
+      iter_B.load(frag_b);
+      ++iter_A;
+      ++iter_E;
+      ++iter_B;
+      mma_op(accum, frag_a, frag_b, accum, frag_e);
+    }
+  }
+};
+
+template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
+          bool trans_B, bool clear_accum = false, typename A_type,
+          typename B_type, typename C_type, typename E_type>
+TL_DEVICE void gemm_sp_ss(A_type *pA, B_type *pB, C_type *accum, E_type *pE) {
+  using MMA =
+      GemmTensorOp<cutlass::gemm::GemmShape<M, N, K>, num_warp_m, num_warp_n,
+                   trans_A, trans_B, clear_accum, A_type, B_type, C_type>;
+  using FragmentC = typename MMA::FragmentC;
+
+  int warp_id = threadIdx.x / 32;
+  int lane_id = threadIdx.x % 32;
+  MMA::body(pA, pE, pB, *(FragmentC *)(accum), warp_id % num_warp_m,
+            warp_id / num_warp_m, lane_id);
+}
+
+} // namespace tl
diff --git a/src/tl_templates/cuda/gemm_sp_sm90.h b/src/tl_templates/cuda/gemm_sp_sm90.h
index dc2bb4a0..db55a21e 100644
--- a/src/tl_templates/cuda/gemm_sp_sm90.h
+++ b/src/tl_templates/cuda/gemm_sp_sm90.h
@@ -217,14 +217,14 @@ namespace tl {
 template <int M, int N, int K, int num_warp_m, int num_warp_n, bool trans_A,
           bool trans_B, bool clear_accum = false, bool use_wgmma = true,
           int wg_wait = 0, typename A_type, typename B_type, typename C_type,
-          typename MMA = cute::tl_wgmma_sp::GemmTensorOp<
+          typename GMMA = cute::tl_wgmma_sp::GemmTensorOp<
               M, N, K, num_warp_m, num_warp_n, trans_A, trans_B, clear_accum,
               A_type, B_type, C_type>,
-          typename E_type = typename MMA::ElementEMma::raw_type>
+          typename E_type = typename GMMA::ElementEMma::raw_type>
 TL_DEVICE void gemm_sp_ss(A_type *pA, B_type *pB, C_type *accum, E_type *pE) {
   static_assert(use_wgmma, "only wgmma is supported for now");
   if constexpr (use_wgmma) {
-    MMA::body<wg_wait>(pA, pB, accum, pE);
+    GMMA::body<wg_wait>(pA, pB, accum, pE);
   } else {
     CUTE_GCC_UNREACHABLE;
   }
diff --git a/src/tl_templates/cuda/instruction/wgmma.h b/src/tl_templates/cuda/instruction/wgmma.h
new file mode 100644
index 00000000..0e971728
--- /dev/null
+++ b/src/tl_templates/cuda/instruction/wgmma.h
@@ -0,0 +1,647 @@
+#pragma once
+#include "../common.h"
+#include "cute/arch/mma_sm90_gmma.hpp"
+
+namespace tl {
+
+template <class> inline constexpr bool always_false_v = false;
+
+// ä¸»ç±»æ¨¡æ¿ - ç§»é™¤é»˜è®¤å‚æ•°ï¼Œå› ä¸ºç‰¹åŒ–ä¸èƒ½æœ‰é»˜è®¤å‚æ•°
+template <DataType A_type, DataType B_type, DataType C_type, int M, int N,
+          int K, bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    printf("DEBUG: WgmmaSSImpl fallback - A_type=%d (kFloat16=%d), B_type=%d, "
+           "C_type=%d, M=%d, N=%d, K=%d, tnspA=%d, tnspB=%d, scaleA=%d, "
+           "scaleB=%d\n",
+           (int)A_type, (int)DataType::kFloat16, (int)B_type, (int)C_type, M, N,
+           K, (int)tnspA, (int)tnspB, scaleA, scaleB);
+    // æš‚æ—¶æ³¨é‡ŠæŽ‰ static_assert æ¥çœ‹è°ƒè¯•è¾“å‡º
+    // static_assert(always_false_v<decltype(c)>,
+    //     "wgmma_ss: No specialization available for given template
+    //     parameters!");
+  };
+};
+
+// ================================= F16 x F16 -> F16
+// =================================
+
+// M64N8K16 F16
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat16, DataType::kFloat16, DataType::kFloat16,
+                   64, 8, 16, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %4, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n8k16.f16.f16.f16 "
+                 "{%0, %1}, %2, %3, p, %5, %6, %7, %8;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// M64N16K16 F16
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat16, DataType::kFloat16, DataType::kFloat16,
+                   64, 16, 16, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %6, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n16k16.f16.f16.f16 "
+                 "{%0, %1, %2, %3}, %4, %5, p, %7, %8, %9, %10;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// M64N32K16 F16
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat16, DataType::kFloat16, DataType::kFloat16,
+                   64, 32, 16, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile(
+        "{\n"
+        ".reg .pred p;\n"
+        "setp.ne.b32 p, %10, 0;\n"
+        "wgmma.mma_async.sync.aligned.m64n32k16.f16.f16.f16 "
+        "{%0, %1, %2, %3, %4, %5, %6, %7}, %8, %9, p, %11, %12, %13, %14;\n"
+        "}\n"
+        : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3]), "+r"(c[4]),
+          "+r"(c[5]), "+r"(c[6]), "+r"(c[7])
+        : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+          "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)),
+          "n"(int32_t(tnspB)));
+  }
+};
+
+// M64N64K16 F16
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat16, DataType::kFloat16, DataType::kFloat16,
+                   64, 64, 16, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %18, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n64k16.f16.f16.f16 "
+                 "{%0,  %1,  %2,  %3,  %4,  %5,  %6,  %7,  "
+                 "%8,  %9, %10, %11, %12, %13, %14, %15},"
+                 " %16, %17, p, %19, %20, %21, %22;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3]), "+r"(c[4]),
+                   "+r"(c[5]), "+r"(c[6]), "+r"(c[7]), "+r"(c[8]), "+r"(c[9]),
+                   "+r"(c[10]), "+r"(c[11]), "+r"(c[12]), "+r"(c[13]),
+                   "+r"(c[14]), "+r"(c[15])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// M64N96K16 F16
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat16, DataType::kFloat16, DataType::kFloat16,
+                   64, 96, 16, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %26, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n96k16.f16.f16.f16 "
+                 "{%0,  %1,  %2,  %3,  %4,  %5,  %6,  %7,  "
+                 "%8,  %9, %10, %11, %12, %13, %14, %15, "
+                 "%16, %17, %18, %19, %20, %21, %22, %23}, "
+                 "%24, %25, p, %27, %28, %29, %30;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3]), "+r"(c[4]),
+                   "+r"(c[5]), "+r"(c[6]), "+r"(c[7]), "+r"(c[8]), "+r"(c[9]),
+                   "+r"(c[10]), "+r"(c[11]), "+r"(c[12]), "+r"(c[13]),
+                   "+r"(c[14]), "+r"(c[15]), "+r"(c[16]), "+r"(c[17]),
+                   "+r"(c[18]), "+r"(c[19]), "+r"(c[20]), "+r"(c[21]),
+                   "+r"(c[22]), "+r"(c[23])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// M64N128K16 F16
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat16, DataType::kFloat16, DataType::kFloat16,
+                   64, 128, 16, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %34, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n128k16.f16.f16.f16 "
+                 "{%0,  %1,  %2,  %3,  %4,  %5,  %6,  %7,  "
+                 "%8,  %9, %10, %11, %12, %13, %14, %15, "
+                 "%16, %17, %18, %19, %20, %21, %22, %23, "
+                 "%24, %25, %26, %27, %28, %29, %30, %31}, "
+                 "%32, %33, p, %35, %36, %37, %38;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3]), "+r"(c[4]),
+                   "+r"(c[5]), "+r"(c[6]), "+r"(c[7]), "+r"(c[8]), "+r"(c[9]),
+                   "+r"(c[10]), "+r"(c[11]), "+r"(c[12]), "+r"(c[13]),
+                   "+r"(c[14]), "+r"(c[15]), "+r"(c[16]), "+r"(c[17]),
+                   "+r"(c[18]), "+r"(c[19]), "+r"(c[20]), "+r"(c[21]),
+                   "+r"(c[22]), "+r"(c[23]), "+r"(c[24]), "+r"(c[25]),
+                   "+r"(c[26]), "+r"(c[27]), "+r"(c[28]), "+r"(c[29]),
+                   "+r"(c[30]), "+r"(c[31])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// M64N192K16 F16
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat16, DataType::kFloat16, DataType::kFloat16,
+                   64, 192, 16, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile(
+        "{\n"
+        ".reg .pred p;\n"
+        "setp.ne.b32 p, %50, 0;\n"
+        "wgmma.mma_async.sync.aligned.m64n192k16.f16.f16.f16 "
+        "{%0,  %1,  %2,  %3,  %4,  %5,  %6,  %7,  "
+        "%8,  %9, %10, %11, %12, %13, %14, %15, "
+        "%16, %17, %18, %19, %20, %21, %22, %23, "
+        "%24, %25, %26, %27, %28, %29, %30, %31, "
+        "%32, %33, %34, %35, %36, %37, %38, %39, "
+        "%40, %41, %42, %43, %44, %45, %46, %47}, "
+        "%48, %49, p, %51, %52, %53, %54;\n"
+        "}\n"
+        : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3]), "+r"(c[4]),
+          "+r"(c[5]), "+r"(c[6]), "+r"(c[7]), "+r"(c[8]), "+r"(c[9]),
+          "+r"(c[10]), "+r"(c[11]), "+r"(c[12]), "+r"(c[13]), "+r"(c[14]),
+          "+r"(c[15]), "+r"(c[16]), "+r"(c[17]), "+r"(c[18]), "+r"(c[19]),
+          "+r"(c[20]), "+r"(c[21]), "+r"(c[22]), "+r"(c[23]), "+r"(c[24]),
+          "+r"(c[25]), "+r"(c[26]), "+r"(c[27]), "+r"(c[28]), "+r"(c[29]),
+          "+r"(c[30]), "+r"(c[31]), "+r"(c[32]), "+r"(c[33]), "+r"(c[34]),
+          "+r"(c[35]), "+r"(c[36]), "+r"(c[37]), "+r"(c[38]), "+r"(c[39]),
+          "+r"(c[40]), "+r"(c[41]), "+r"(c[42]), "+r"(c[43]), "+r"(c[44]),
+          "+r"(c[45]), "+r"(c[46]), "+r"(c[47])
+        : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+          "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)),
+          "n"(int32_t(tnspB)));
+  }
+};
+
+// M64N256K16 F16
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat16, DataType::kFloat16, DataType::kFloat16,
+                   64, 256, 16, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile(
+        "{\n"
+        ".reg .pred p;\n"
+        "setp.ne.b32 p, %66, 0;\n"
+        "wgmma.mma_async.sync.aligned.m64n256k16.f16.f16.f16 "
+        "{%0,  %1,  %2,  %3,  %4,  %5,  %6,  %7,  "
+        "%8,  %9, %10, %11, %12, %13, %14, %15, "
+        "%16, %17, %18, %19, %20, %21, %22, %23, "
+        "%24, %25, %26, %27, %28, %29, %30, %31, "
+        "%32, %33, %34, %35, %36, %37, %38, %39, "
+        "%40, %41, %42, %43, %44, %45, %46, %47, "
+        "%48, %49, %50, %51, %52, %53, %54, %55, "
+        "%56, %57, %58, %59, %60, %61, %62, %63}, "
+        "%64, %65, p, %67, %68, %69, %70;\n"
+        "}\n"
+        : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3]), "+r"(c[4]),
+          "+r"(c[5]), "+r"(c[6]), "+r"(c[7]), "+r"(c[8]), "+r"(c[9]),
+          "+r"(c[10]), "+r"(c[11]), "+r"(c[12]), "+r"(c[13]), "+r"(c[14]),
+          "+r"(c[15]), "+r"(c[16]), "+r"(c[17]), "+r"(c[18]), "+r"(c[19]),
+          "+r"(c[20]), "+r"(c[21]), "+r"(c[22]), "+r"(c[23]), "+r"(c[24]),
+          "+r"(c[25]), "+r"(c[26]), "+r"(c[27]), "+r"(c[28]), "+r"(c[29]),
+          "+r"(c[30]), "+r"(c[31]), "+r"(c[32]), "+r"(c[33]), "+r"(c[34]),
+          "+r"(c[35]), "+r"(c[36]), "+r"(c[37]), "+r"(c[38]), "+r"(c[39]),
+          "+r"(c[40]), "+r"(c[41]), "+r"(c[42]), "+r"(c[43]), "+r"(c[44]),
+          "+r"(c[45]), "+r"(c[46]), "+r"(c[47]), "+r"(c[48]), "+r"(c[49]),
+          "+r"(c[50]), "+r"(c[51]), "+r"(c[52]), "+r"(c[53]), "+r"(c[54]),
+          "+r"(c[55]), "+r"(c[56]), "+r"(c[57]), "+r"(c[58]), "+r"(c[59]),
+          "+r"(c[60]), "+r"(c[61]), "+r"(c[62]), "+r"(c[63])
+        : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+          "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)),
+          "n"(int32_t(tnspB)));
+  }
+};
+
+// ================================= F16 x F16 -> F32
+// =================================
+
+// M64N8K16 F16->F32
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat16, DataType::kFloat16, DataType::kFloat32,
+                   64, 8, 16, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %6, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16 "
+                 "{%0, %1, %2, %3}, %4, %5, p, %7, %8, %9, %10;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// M64N16K16 F16->F32
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat16, DataType::kFloat16, DataType::kFloat32,
+                   64, 16, 16, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile(
+        "{\n"
+        ".reg .pred p;\n"
+        "setp.ne.b32 p, %10, 0;\n"
+        "wgmma.mma_async.sync.aligned.m64n16k16.f32.f16.f16 "
+        "{%0, %1, %2, %3, %4, %5, %6, %7}, %8, %9, p, %11, %12, %13, %14;\n"
+        "}\n"
+        : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3]), "+r"(c[4]),
+          "+r"(c[5]), "+r"(c[6]), "+r"(c[7])
+        : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+          "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)),
+          "n"(int32_t(tnspB)));
+  }
+};
+
+// M64N32K16 F16->F32
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat16, DataType::kFloat16, DataType::kFloat32,
+                   64, 32, 16, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %18, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n32k16.f32.f16.f16 "
+                 "{%0,  %1,  %2,  %3,  %4,  %5,  %6,  %7,  "
+                 "%8,  %9, %10, %11, %12, %13, %14, %15}, "
+                 "%16, %17, p, %19, %20, %21, %22;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3]), "+r"(c[4]),
+                   "+r"(c[5]), "+r"(c[6]), "+r"(c[7]), "+r"(c[8]), "+r"(c[9]),
+                   "+r"(c[10]), "+r"(c[11]), "+r"(c[12]), "+r"(c[13]),
+                   "+r"(c[14]), "+r"(c[15])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// M64N64K16 F16->F32
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat16, DataType::kFloat16, DataType::kFloat32,
+                   64, 64, 16, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %34, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 "
+                 "{%0,  %1,  %2,  %3,  %4,  %5,  %6,  %7,  "
+                 "%8,  %9, %10, %11, %12, %13, %14, %15, "
+                 "%16, %17, %18, %19, %20, %21, %22, %23, "
+                 "%24, %25, %26, %27, %28, %29, %30, %31}, "
+                 "%32, %33, p, %35, %36, %37, %38;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3]), "+r"(c[4]),
+                   "+r"(c[5]), "+r"(c[6]), "+r"(c[7]), "+r"(c[8]), "+r"(c[9]),
+                   "+r"(c[10]), "+r"(c[11]), "+r"(c[12]), "+r"(c[13]),
+                   "+r"(c[14]), "+r"(c[15]), "+r"(c[16]), "+r"(c[17]),
+                   "+r"(c[18]), "+r"(c[19]), "+r"(c[20]), "+r"(c[21]),
+                   "+r"(c[22]), "+r"(c[23]), "+r"(c[24]), "+r"(c[25]),
+                   "+r"(c[26]), "+r"(c[27]), "+r"(c[28]), "+r"(c[29]),
+                   "+r"(c[30]), "+r"(c[31])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// ================================= BF16 x BF16 -> F32
+// =================================
+
+// M64N8K16 BF16->F32
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kBFloat16, DataType::kBFloat16, DataType::kFloat32,
+                   64, 8, 16, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %6, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n8k16.f32.bf16.bf16 "
+                 "{%0, %1, %2, %3}, %4, %5, p, %7, %8, %9, %10;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// M64N16K16 BF16->F32
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kBFloat16, DataType::kBFloat16, DataType::kFloat32,
+                   64, 16, 16, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile(
+        "{\n"
+        ".reg .pred p;\n"
+        "setp.ne.b32 p, %10, 0;\n"
+        "wgmma.mma_async.sync.aligned.m64n16k16.f32.bf16.bf16 "
+        "{%0, %1, %2, %3, %4, %5, %6, %7}, %8, %9, p, %11, %12, %13, %14;\n"
+        "}\n"
+        : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3]), "+r"(c[4]),
+          "+r"(c[5]), "+r"(c[6]), "+r"(c[7])
+        : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+          "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)),
+          "n"(int32_t(tnspB)));
+  }
+};
+
+// ================================= TF32 x TF32 -> F32
+// =================================
+
+// M64N8K8 TF32->F32
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kTensorFloat32, DataType::kTensorFloat32,
+                   DataType::kFloat32, 64, 8, 8, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %6, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n8k8.f32.tf32.tf32 "
+                 "{%0, %1, %2, %3}, %4, %5, p, %7, %8, %9, %10;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// M64N16K8 TF32->F32
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kTensorFloat32, DataType::kTensorFloat32,
+                   DataType::kFloat32, 64, 16, 8, tnspA, tnspB, scaleA,
+                   scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile(
+        "{\n"
+        ".reg .pred p;\n"
+        "setp.ne.b32 p, %10, 0;\n"
+        "wgmma.mma_async.sync.aligned.m64n16k8.f32.tf32.tf32 "
+        "{%0, %1, %2, %3, %4, %5, %6, %7}, %8, %9, p, %11, %12, %13, %14;\n"
+        "}\n"
+        : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3]), "+r"(c[4]),
+          "+r"(c[5]), "+r"(c[6]), "+r"(c[7])
+        : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+          "n"(int32_t(scaleA)), "n"(int32_t(scaleB)), "n"(int32_t(tnspA)),
+          "n"(int32_t(tnspB)));
+  }
+};
+
+// ================================= INT8 x INT8 -> INT32
+// =================================
+
+// M64N8K32 S8->S32
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kInt8, DataType::kInt8, DataType::kInt32, 64, 8,
+                   32, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %4, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n8k32.s32.s8.s8 "
+                 "{%0, %1}, %2, %3, p, %5, %6, %7, %8;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// M64N16K32 S8->S32
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kInt8, DataType::kInt8, DataType::kInt32, 64, 16,
+                   32, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %6, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n16k32.s32.s8.s8 "
+                 "{%0, %1, %2, %3}, %4, %5, p, %7, %8, %9, %10;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// ================================= FP8 x FP8 -> F16/F32
+// =================================
+
+// M64N8K32 E4M3->F16
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat8_e4m3, DataType::kFloat8_e4m3,
+                   DataType::kFloat16, 64, 8, 32, tnspA, tnspB, scaleA,
+                   scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %4, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n8k32.f16.e4m3.e4m3 "
+                 "{%0, %1}, %2, %3, p, %5, %6, %7, %8;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// M64N8K32 E4M3->F32
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat8_e4m3, DataType::kFloat8_e4m3,
+                   DataType::kFloat32, 64, 8, 32, tnspA, tnspB, scaleA,
+                   scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %6, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n8k32.f32.e4m3.e4m3 "
+                 "{%0, %1, %2, %3}, %4, %5, p, %7, %8, %9, %10;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1]), "+r"(c[2]), "+r"(c[3])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// å‡½æ•°æ¨¡æ¿å§”æ‰˜ç»™ç±»æ¨¡æ¿
+template <DataType A_type, DataType B_type, DataType C_type, int M, int N,
+          int K, bool tnspA, bool tnspB, int scaleA = 1, int scaleB = 1>
+TL_DEVICE void wgmma_ss(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                        bool scale_out) {
+  WgmmaSSImpl<A_type, B_type, C_type, M, N, K, tnspA, tnspB, scaleA,
+              scaleB>::execute(desc_a, desc_b, c, scale_out);
+}
+
+// ================================= Mixed Precision Support
+// =================================
+
+// Mixed precision: S8 x U8 -> S32
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kInt8, DataType::kUInt8, DataType::kInt32, 64, 8,
+                   32, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %4, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n8k32.s32.s8.u8 "
+                 "{%0, %1}, %2, %3, p, %5, %6, %7, %8;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// Mixed precision: U8 x S8 -> S32
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kUInt8, DataType::kInt8, DataType::kInt32, 64, 8,
+                   32, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %4, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.s8 "
+                 "{%0, %1}, %2, %3, p, %5, %6, %7, %8;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// Mixed precision: U8 x U8 -> S32
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kUInt8, DataType::kUInt8, DataType::kInt32, 64, 8,
+                   32, tnspA, tnspB, scaleA, scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %4, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8 "
+                 "{%0, %1}, %2, %3, p, %5, %6, %7, %8;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// Mixed precision FP8: E4M3 x E5M2 -> F16
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat8_e4m3, DataType::kFloat8_e5m2,
+                   DataType::kFloat16, 64, 8, 32, tnspA, tnspB, scaleA,
+                   scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %4, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n8k32.f16.e4m3.e5m2 "
+                 "{%0, %1}, %2, %3, p, %5, %6, %7, %8;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// Mixed precision FP8: E5M2 x E4M3 -> F16
+template <bool tnspA, bool tnspB, int scaleA, int scaleB>
+struct WgmmaSSImpl<DataType::kFloat8_e5m2, DataType::kFloat8_e4m3,
+                   DataType::kFloat16, 64, 8, 32, tnspA, tnspB, scaleA,
+                   scaleB> {
+  TL_DEVICE static void execute(uint64_t desc_a, uint64_t desc_b, uint32_t *c,
+                                bool scale_out) {
+    asm volatile("{\n"
+                 ".reg .pred p;\n"
+                 "setp.ne.b32 p, %4, 0;\n"
+                 "wgmma.mma_async.sync.aligned.m64n8k32.f16.e5m2.e4m3 "
+                 "{%0, %1}, %2, %3, p, %5, %6, %7, %8;\n"
+                 "}\n"
+                 : "+r"(c[0]), "+r"(c[1])
+                 : "l"(desc_a), "l"(desc_b), "r"(int32_t(scale_out)),
+                   "n"(int32_t(scaleA)), "n"(int32_t(scaleB)),
+                   "n"(int32_t(tnspA)), "n"(int32_t(tnspB)));
+  }
+};
+
+// ================================= Convenience Templates
+// =================================
+
+// Type trait to determine the number of output registers needed
+template <DataType C_type, int M, int N> struct WgmmaOutputRegs {
+  static constexpr int value =
+      (M * N * (C_type == DataType::kFloat32 ? 32 : 16)) / (32 * 8);
+};
+
+// Type trait to get element size in bits
+template <DataType dtype> struct ElementBits {
+  static constexpr int value =
+      (dtype == DataType::kFloat32 || dtype == DataType::kTensorFloat32 ||
+       dtype == DataType::kInt32)
+          ? 32
+      : (dtype == DataType::kFloat16 || dtype == DataType::kBFloat16 ||
+         dtype == DataType::kInt16 || dtype == DataType::kUInt16)
+          ? 16
+      : (dtype == DataType::kInt8 || dtype == DataType::kUInt8 ||
+         dtype == DataType::kFloat8_e4m3 || dtype == DataType::kFloat8_e5m2)
+          ? 8
+      : (dtype == DataType::kInt4 || dtype == DataType::kUInt4) ? 4
+                                                                : 8;
+};
+
+} // namespace tl
\ No newline at end of file
diff --git a/src/tl_templates/cuda/intrin.h b/src/tl_templates/cuda/intrin.h
new file mode 100644
index 00000000..ef1afa7f
--- /dev/null
+++ b/src/tl_templates/cuda/intrin.h
@@ -0,0 +1,119 @@
+#pragma once
+
+#include "common.h"
+#include "cutlass/cutlass.h"
+
+#if __CUDA_ARCH_LIST__ >= 900
+#include "cute/arch/cluster_sm90.hpp"
+#include "cute/arch/mma_sm90_gmma.hpp"
+#endif
+
+namespace tl {
+
+namespace detail {
+
+// Provide architecture-specific defaults so callers may omit arguments.
+TL_DEVICE constexpr int default_warp_size() {
+#if defined(__HIP_PLATFORM_AMD__) || defined(__HIP_DEVICE_COMPILE__)
+  return 64;
+#else
+  return 32;
+#endif
+}
+
+TL_DEVICE constexpr int default_warps_per_group() { return 4; }
+
+TL_DEVICE int linear_thread_idx_in_block() {
+#if defined(__CUDA_ARCH__) || defined(__HIP_DEVICE_COMPILE__)
+  return threadIdx.x + blockDim.x * (threadIdx.y + blockDim.y * threadIdx.z);
+#else
+  return 0;
+#endif
+}
+
+} // namespace detail
+
+TL_DEVICE int get_lane_idx(int warp_size = detail::default_warp_size()) {
+  warp_size = warp_size > 0 ? warp_size : detail::default_warp_size();
+  return detail::linear_thread_idx_in_block() % warp_size;
+}
+
+TL_DEVICE int get_warp_idx_sync(int warp_size = detail::default_warp_size()) {
+  warp_size = warp_size > 0 ? warp_size : detail::default_warp_size();
+  return detail::linear_thread_idx_in_block() / warp_size;
+}
+
+TL_DEVICE int get_warp_idx(int warp_size = detail::default_warp_size()) {
+  warp_size = warp_size > 0 ? warp_size : detail::default_warp_size();
+  return detail::linear_thread_idx_in_block() / warp_size;
+}
+
+TL_DEVICE int
+get_warp_group_idx(int warp_size = detail::default_warp_size(),
+                   int warps_per_group = detail::default_warps_per_group()) {
+  warp_size = warp_size > 0 ? warp_size : detail::default_warp_size();
+  warps_per_group =
+      warps_per_group > 0 ? warps_per_group : detail::default_warps_per_group();
+  int threads_per_group = warp_size * warps_per_group;
+  threads_per_group = threads_per_group > 0 ? threads_per_group : warp_size;
+  return detail::linear_thread_idx_in_block() / threads_per_group;
+}
+
+#if __CUDA_ARCH_LIST__ >= 900
+TL_DEVICE void warpgroup_arrive() { cute::warpgroup_arrive(); }
+TL_DEVICE void warpgroup_commit_batch() { cute::warpgroup_commit_batch(); }
+
+template <int NumMma> TL_DEVICE void warpgroup_wait() {
+  cute::warpgroup_wait<NumMma>();
+}
+
+// Template parameter:
+//   thread_extent: the logical size (in number of threads) of each "group"
+//                  within which we want to elect exactly ONE representative
+//                  thread.
+template <int thread_extent> TL_DEVICE bool tl_shuffle_elect() {
+
+  // Special case: thread_extent == 0 means "elect exactly one thread
+  // in the entire thread block", i.e., the leader of the first warp of the
+  // block.
+  if constexpr (thread_extent == 0) {
+    // cutlass::canonical_warp_idx_sync():
+    //   Returns the warp ID within the thread block in a "canonical" way
+    //   (0 for the first warp, 1 for the second, ...).
+    // cute::elect_one_sync():
+    //   Elect exactly one lane in the warp to return true (typically lane 0),
+    //   other lanes return false.
+    // The condition ensures that:
+    //   (1) We are in warp 0 of the block.
+    //   (2) We are the elected lane in this warp.
+    return cutlass::canonical_warp_idx_sync() == 0 && cute::elect_one_sync();
+  }
+
+  // General case: thread_extent != 0
+  // (threadIdx.x / 32) is the warp index in the block.
+  // (thread_extent / 32) is the number of warps in one group of size
+  // thread_extent. We take warp_id % num_warps_in_group to get the warp's index
+  // within the group.
+  // __shfl_sync(mask, value, srcLane): broadcast 'value' from srcLane to all
+  // lanes in the warp. Here it broadcasts the group-local warp index from lane
+  // 0. Comparing to 0 selects only the group's warp 0.
+  return __shfl_sync(0xffffffff, // full warp mask
+                     (threadIdx.x / 32) %
+                         (thread_extent / 32), // warp index within group
+                     0                         // take the value from lane 0
+                     ) == 0 &&
+         // Within that group leader warp, elect exactly one lane (typically
+         // lane 0) to be the single representative for the group.
+         cute::elect_one_sync();
+}
+
+template <uint32_t RegCount> TL_DEVICE void warpgroup_reg_alloc() {
+  asm volatile("setmaxnreg.inc.sync.aligned.u32 %0;\n" : : "n"(RegCount));
+}
+
+template <uint32_t RegCount> TL_DEVICE void warpgroup_reg_dealloc() {
+  asm volatile("setmaxnreg.dec.sync.aligned.u32 %0;\n" : : "n"(RegCount));
+}
+#endif
+
+} // namespace tl
diff --git a/src/tl_templates/cuda/ldsm.h b/src/tl_templates/cuda/ldsm.h
index 9cc3f1ba..4d6af8a0 100644
--- a/src/tl_templates/cuda/ldsm.h
+++ b/src/tl_templates/cuda/ldsm.h
@@ -4,8 +4,8 @@
 
 namespace tl {
 
-TL_DEVICE_NOINLINE void ptx_ldmatrix_x1(void const *const smem_ptr,
-                                        void *const local_ptr) {
+TL_DEVICE void ptx_ldmatrix_x1(void const *const smem_ptr,
+                               void *const local_ptr) {
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
   int32_t *value = reinterpret_cast<int32_t *>(local_ptr);
   asm volatile("ldmatrix.sync.aligned.x1.m8n8.shared.b16 {%0}, [%1];\n"
@@ -13,8 +13,8 @@ TL_DEVICE_NOINLINE void ptx_ldmatrix_x1(void const *const smem_ptr,
                : "r"(smem_int_ptr));
 }
 
-TL_DEVICE_NOINLINE void ptx_ldmatrix_x2(void const *const smem_ptr,
-                                        void *const local_ptr) {
+TL_DEVICE void ptx_ldmatrix_x2(void const *const smem_ptr,
+                               void *const local_ptr) {
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
   int32_t *value = reinterpret_cast<int32_t *>(local_ptr);
   asm volatile("ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%0, %1}, [%2];\n"
@@ -22,8 +22,8 @@ TL_DEVICE_NOINLINE void ptx_ldmatrix_x2(void const *const smem_ptr,
                : "r"(smem_int_ptr));
 }
 
-TL_DEVICE_NOINLINE void ptx_ldmatrix_x4(void const *const smem_ptr,
-                                        void *const local_ptr) {
+TL_DEVICE void ptx_ldmatrix_x4(void const *const smem_ptr,
+                               void *const local_ptr) {
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
   int32_t *value = reinterpret_cast<int32_t *>(local_ptr);
   asm volatile(
@@ -32,8 +32,8 @@ TL_DEVICE_NOINLINE void ptx_ldmatrix_x4(void const *const smem_ptr,
       : "r"(smem_int_ptr));
 }
 
-TL_DEVICE_NOINLINE void ptx_ldmatrix_x1_trans(void const *const smem_ptr,
-                                              void *const local_ptr) {
+TL_DEVICE void ptx_ldmatrix_x1_trans(void const *const smem_ptr,
+                                     void *const local_ptr) {
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
   int32_t *value = reinterpret_cast<int32_t *>(local_ptr);
   asm volatile("ldmatrix.sync.aligned.x1.trans.m8n8.shared.b16 {%0}, [%1];\n"
@@ -41,8 +41,8 @@ TL_DEVICE_NOINLINE void ptx_ldmatrix_x1_trans(void const *const smem_ptr,
                : "r"(smem_int_ptr));
 }
 
-TL_DEVICE_NOINLINE void ptx_ldmatrix_x2_trans(void const *const smem_ptr,
-                                              void *const local_ptr) {
+TL_DEVICE void ptx_ldmatrix_x2_trans(void const *const smem_ptr,
+                                     void *const local_ptr) {
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
   int32_t *value = reinterpret_cast<int32_t *>(local_ptr);
   asm volatile(
@@ -51,8 +51,8 @@ TL_DEVICE_NOINLINE void ptx_ldmatrix_x2_trans(void const *const smem_ptr,
       : "r"(smem_int_ptr));
 }
 
-TL_DEVICE_NOINLINE void ptx_ldmatrix_x4_trans(void const *const smem_ptr,
-                                              void *const local_ptr) {
+TL_DEVICE void ptx_ldmatrix_x4_trans(void const *const smem_ptr,
+                                     void *const local_ptr) {
   uint32_t smem_int_ptr = smem_ptr_to_uint(smem_ptr);
   int32_t *value = reinterpret_cast<int32_t *>(local_ptr);
   asm volatile(
diff --git a/src/tl_templates/cuda/ldst.h b/src/tl_templates/cuda/ldst.h
new file mode 100644
index 00000000..c875832e
--- /dev/null
+++ b/src/tl_templates/cuda/ldst.h
@@ -0,0 +1,255 @@
+#pragma once
+
+#include "common.h"
+
+// Memory semantic and scope enums
+enum class Semantic { WEAK, VOLATILE, ACQUIRE, RELEASE, RELAXED };
+enum class Scope { CTA, GPU, SYS };
+
+#ifndef TL_ALWAYS_FALSE_V_DEFINED
+#define TL_ALWAYS_FALSE_V_DEFINED
+template <class> inline constexpr bool always_false_v = false;
+#endif
+
+// Type trait to detect bfloat16 types
+template <typename T> struct is_bfloat16 : std::false_type {};
+
+#ifdef __CUDA_BF16_TYPES_EXIST__
+template <> struct is_bfloat16<__nv_bfloat16> : std::true_type {};
+#endif
+
+// Detect cutlass bfloat16_t
+namespace cutlass {
+struct bfloat16_t;
+}
+template <> struct is_bfloat16<cutlass::bfloat16_t> : std::true_type {};
+
+template <typename T>
+inline constexpr bool is_bfloat16_v = is_bfloat16<T>::value;
+
+// Fallback template for unsupported configurations
+template <Semantic semantic, Scope scope, bool na> struct StImpl {
+  template <typename T> TL_DEVICE static void execute(T *ptr, T value) {
+    static_assert(always_false_v<T>, "tl::st: unsupported configuration. ");
+  }
+};
+
+template <Semantic semantic, Scope scope, bool nc, bool na> struct LdImpl {
+  template <typename T> TL_DEVICE static void execute(const T *ptr, T &value) {
+    static_assert(always_false_v<T>, "tl::ld: unsupported configuration. ");
+  }
+};
+
+// Macro to define implementation with generic type T
+#define TL_ST_IMPL(SEM, SCOPE, NA, SEM_LIT, SCOPE_LIT, NA_LIT)                 \
+  template <> struct StImpl<Semantic::SEM, Scope::SCOPE, NA> {                 \
+    template <typename T> TL_DEVICE static void execute(T *ptr, T value) {     \
+      if constexpr (sizeof(T) == 2) {                                          \
+        if constexpr (is_bfloat16_v<T>) {                                      \
+          uint16_t value_bits = *reinterpret_cast<uint16_t *>(&value);         \
+          asm volatile("st" SEM_LIT SCOPE_LIT NA_LIT                           \
+                       ".b16 [%0], %1;" ::"l"(ptr),                            \
+                       "h"(value_bits)                                         \
+                       : "memory");                                            \
+        } else {                                                               \
+          asm volatile("st" SEM_LIT SCOPE_LIT NA_LIT                           \
+                       ".b16 [%0], %1;" ::"l"(ptr),                            \
+                       "h"(value)                                              \
+                       : "memory");                                            \
+        }                                                                      \
+      } else if constexpr (sizeof(T) == 4) {                                   \
+        if constexpr (std::is_floating_point_v<T>) {                           \
+          asm volatile("st" SEM_LIT SCOPE_LIT NA_LIT                           \
+                       ".b32 [%0], %1;" ::"l"(ptr),                            \
+                       "f"(value)                                              \
+                       : "memory");                                            \
+        } else {                                                               \
+          asm volatile("st" SEM_LIT SCOPE_LIT NA_LIT                           \
+                       ".b32 [%0], %1;" ::"l"(ptr),                            \
+                       "r"(value)                                              \
+                       : "memory");                                            \
+        }                                                                      \
+      } else if constexpr (sizeof(T) == 8) {                                   \
+        if constexpr (std::is_floating_point_v<T>) {                           \
+          asm volatile("st" SEM_LIT SCOPE_LIT NA_LIT                           \
+                       ".b64 [%0], %1;" ::"l"(ptr),                            \
+                       "d"(value)                                              \
+                       : "memory");                                            \
+        } else {                                                               \
+          asm volatile("st" SEM_LIT SCOPE_LIT NA_LIT                           \
+                       ".b64 [%0], %1;" ::"l"(ptr),                            \
+                       "l"(value)                                              \
+                       : "memory");                                            \
+        }                                                                      \
+      } else if constexpr (sizeof(T) == 16) {                                  \
+        asm volatile("st" SEM_LIT SCOPE_LIT NA_LIT                             \
+                     ".v4.s32 [%0], {%1, %2, %3, %4};" ::"l"(ptr),             \
+                     "r"(value.x), "r"(value.y), "r"(value.z), "r"(value.w)    \
+                     : "memory");                                              \
+      }                                                                        \
+    }                                                                          \
+  };
+
+// Macro to define implementation of tl::ld with generic type T
+#define TL_LD_IMPL(SEM, SCOPE, NC, NA, SEM_LIT, SCOPE_LIT, NC_LIT, NA_LIT)     \
+  template <> struct LdImpl<Semantic::SEM, Scope::SCOPE, NC, NA> {             \
+    template <typename T>                                                      \
+    TL_DEVICE static void execute(const T *ptr, T &value) {                    \
+      if constexpr (sizeof(T) == 2) {                                          \
+        if constexpr (is_bfloat16_v<T>) {                                      \
+          uint16_t value_bits;                                                 \
+          asm volatile("ld" SEM_LIT SCOPE_LIT NC_LIT NA_LIT ".b16 %0, [%1];"   \
+                       : "=h"(value_bits)                                      \
+                       : "l"(ptr)                                              \
+                       : "memory");                                            \
+          value = *reinterpret_cast<T *>(&value_bits);                         \
+        } else {                                                               \
+          asm volatile("ld" SEM_LIT SCOPE_LIT NC_LIT NA_LIT ".b16 %0, [%1];"   \
+                       : "=h"(value)                                           \
+                       : "l"(ptr)                                              \
+                       : "memory");                                            \
+        }                                                                      \
+      } else if constexpr (sizeof(T) == 4) {                                   \
+        if constexpr (std::is_floating_point_v<T>) {                           \
+          asm volatile("ld" SEM_LIT SCOPE_LIT NC_LIT NA_LIT ".b32 %0, [%1];"   \
+                       : "=f"(value)                                           \
+                       : "l"(ptr)                                              \
+                       : "memory");                                            \
+        } else {                                                               \
+          asm volatile("ld" SEM_LIT SCOPE_LIT NC_LIT NA_LIT ".b32 %0, [%1];"   \
+                       : "=r"(value)                                           \
+                       : "l"(ptr)                                              \
+                       : "memory");                                            \
+        }                                                                      \
+      } else if constexpr (sizeof(T) == 8) {                                   \
+        if constexpr (std::is_floating_point_v<T>) {                           \
+          asm volatile("ld" SEM_LIT SCOPE_LIT NC_LIT NA_LIT ".b64 %0, [%1];"   \
+                       : "=d"(value)                                           \
+                       : "l"(ptr)                                              \
+                       : "memory");                                            \
+        } else {                                                               \
+          asm volatile("ld" SEM_LIT SCOPE_LIT NC_LIT NA_LIT ".b64 %0, [%1];"   \
+                       : "=l"(value)                                           \
+                       : "l"(ptr)                                              \
+                       : "memory");                                            \
+        }                                                                      \
+      } else if constexpr (sizeof(T) == 16) {                                  \
+        asm volatile("ld" SEM_LIT SCOPE_LIT NC_LIT NA_LIT                      \
+                     ".v4.s32 {%0, %1, %2, %3}, [%4];"                         \
+                     : "=r"(value.x), "=r"(value.y), "=r"(value.z),            \
+                       "=r"(value.w)                                           \
+                     : "l"(ptr)                                                \
+                     : "memory");                                              \
+      }                                                                        \
+    }                                                                          \
+  };
+
+// Register all combinations of arguments for tl::st in need here
+// WEAK (always .global)
+TL_ST_IMPL(WEAK, CTA, false, ".weak", ".global", "")
+TL_ST_IMPL(WEAK, GPU, false, ".weak", ".global", "")
+TL_ST_IMPL(WEAK, GPU, true, ".weak", ".global", ".L1::no_allocate")
+TL_ST_IMPL(WEAK, SYS, false, ".weak", ".global", "")
+TL_ST_IMPL(WEAK, SYS, true, ".weak", ".global", ".L1::no_allocate")
+
+// VOLATILE (always .global, no na)
+TL_ST_IMPL(VOLATILE, CTA, false, ".volatile", ".global", "")
+TL_ST_IMPL(VOLATILE, GPU, false, ".volatile", ".global", "")
+TL_ST_IMPL(VOLATILE, SYS, false, ".volatile", ".global", "")
+
+// RELAXED (scope-aware)
+TL_ST_IMPL(RELAXED, CTA, false, ".relaxed", ".cta", "")
+TL_ST_IMPL(RELAXED, GPU, false, ".relaxed", ".gpu.global", "")
+TL_ST_IMPL(RELAXED, GPU, true, ".relaxed", ".gpu.global", ".L1::no_allocate")
+TL_ST_IMPL(RELAXED, SYS, false, ".relaxed", ".sys.global", "")
+TL_ST_IMPL(RELAXED, SYS, true, ".relaxed", ".sys.global", ".L1::no_allocate")
+
+// RELEASE (scope-aware)
+TL_ST_IMPL(RELEASE, CTA, false, ".release", ".cta", "")
+TL_ST_IMPL(RELEASE, GPU, false, ".release", ".gpu.global", "")
+TL_ST_IMPL(RELEASE, GPU, true, ".release", ".gpu.global", ".L1::no_allocate")
+TL_ST_IMPL(RELEASE, SYS, false, ".release", ".sys.global", "")
+TL_ST_IMPL(RELEASE, SYS, true, ".release", ".sys.global", ".L1::no_allocate")
+
+// Register all combinations of arguments for tl::ld in need here
+// nc (must with no scope and semantic)
+TL_LD_IMPL(WEAK, CTA, true, false, "", ".global", ".nc", "")
+TL_LD_IMPL(WEAK, GPU, true, false, "", ".global", ".nc", "")
+TL_LD_IMPL(WEAK, SYS, true, false, "", ".global", ".nc", "")
+TL_LD_IMPL(WEAK, GPU, true, true, "", ".global", ".nc", ".L1::no_allocate")
+TL_LD_IMPL(WEAK, SYS, true, true, "", ".global", ".nc", ".L1::no_allocate")
+
+// WEAK
+TL_LD_IMPL(WEAK, CTA, false, false, ".weak", ".cta", "", "")
+TL_LD_IMPL(WEAK, GPU, false, false, ".weak", ".gpu.global", "", "")
+TL_LD_IMPL(WEAK, SYS, false, false, ".weak", ".sys.global", "", "")
+TL_LD_IMPL(WEAK, GPU, false, true, ".weak", ".gpu.global", "",
+           ".L1::no_allocate")
+TL_LD_IMPL(WEAK, SYS, false, true, ".weak", ".sys.global", "",
+           ".L1::no_allocate")
+
+// VOLATILE (always .global, no na)
+TL_LD_IMPL(VOLATILE, CTA, false, false, ".volatile", ".global", "", "")
+TL_LD_IMPL(VOLATILE, GPU, false, false, ".volatile", ".global", "", "")
+TL_LD_IMPL(VOLATILE, SYS, false, false, ".volatile", ".global", "", "")
+
+// RELAXED (scope-aware)
+TL_LD_IMPL(RELAXED, CTA, false, false, ".relaxed", ".cta", "", "")
+TL_LD_IMPL(RELAXED, GPU, false, false, ".relaxed", ".gpu.global", "", "")
+TL_LD_IMPL(RELAXED, SYS, false, false, ".relaxed", ".sys.global", "", "")
+TL_LD_IMPL(RELAXED, GPU, false, true, ".relaxed", ".gpu.global", "",
+           ".L1::no_allocate")
+TL_LD_IMPL(RELAXED, SYS, false, true, ".relaxed", ".sys.global", "",
+           ".L1::no_allocate")
+
+// ACQUIRE (scope-aware)
+TL_LD_IMPL(ACQUIRE, CTA, false, false, ".acquire", ".cta", "", "")
+TL_LD_IMPL(ACQUIRE, GPU, false, false, ".acquire", ".gpu.global", "", "")
+TL_LD_IMPL(ACQUIRE, SYS, false, false, ".acquire", ".sys.global", "", "")
+TL_LD_IMPL(ACQUIRE, GPU, false, true, ".acquire", ".gpu.global", "",
+           ".L1::no_allocate")
+TL_LD_IMPL(ACQUIRE, SYS, false, true, ".acquire", ".sys.global", "",
+           ".L1::no_allocate")
+
+#undef TL_ST_IMPL
+#undef TL_LD_IMPL
+
+namespace tl {
+
+// Public interface
+template <Semantic semantic, Scope scope, bool na, typename P, typename T>
+TL_DEVICE void st(P ptr, T value) {
+  static_assert(sizeof(T) == 2 || sizeof(T) == 4 || sizeof(T) == 8 ||
+                    sizeof(T) == 16,
+                "tl::st: T must be 2, 4, 8, or 16 bytes");
+  static_assert(std::is_pointer_v<P> || std::is_same_v<P, uint64_t>,
+                "tl::st: P must be a pointer or uint64_t");
+  static_assert(semantic == Semantic::WEAK || semantic == Semantic::RELAXED ||
+                    semantic == Semantic::RELEASE ||
+                    semantic == Semantic::VOLATILE,
+                "tl::st: semantic must be WEAK, VOLATILE, RELAXED, or RELEASE");
+
+  T *ptr_ = reinterpret_cast<T *>(ptr);
+  StImpl<semantic, scope, na>::execute(ptr_, value);
+}
+
+template <Semantic semantic, Scope scope, bool nc, bool na, typename P,
+          typename T>
+TL_DEVICE void ld(const P ptr, T &value) {
+  static_assert(sizeof(T) == 2 || sizeof(T) == 4 || sizeof(T) == 8 ||
+                    sizeof(T) == 16,
+                "tl::ld: T must be 2, 4, 8, or 16 bytes");
+  static_assert(std::is_pointer_v<P> || std::is_same_v<P, uint64_t>,
+                "tl::ld: P must be a pointer or uint64_t");
+  static_assert(semantic == Semantic::WEAK || semantic == Semantic::RELAXED ||
+                    semantic == Semantic::ACQUIRE ||
+                    semantic == Semantic::VOLATILE,
+                "tl::ld: semantic must be WEAK, RELAXED, ACQUIRE, or VOLATILE");
+
+  const T *ptr_ = reinterpret_cast<const T *>(ptr);
+  LdImpl<semantic, scope, nc, na>::execute(ptr_, value);
+}
+
+// todo: support "ld.global.nc.L1::no_allocate.L2::256B"
+
+} // namespace tl
diff --git a/src/tl_templates/cuda/reduce.h b/src/tl_templates/cuda/reduce.h
index 2783fc53..5981fa07 100644
--- a/src/tl_templates/cuda/reduce.h
+++ b/src/tl_templates/cuda/reduce.h
@@ -22,6 +22,71 @@ struct MinOp {
   }
 };
 
+struct BitAndOp {
+  template <typename T> TL_DEVICE T operator()(T const &x, T const &y) {
+    return x & y;
+  }
+};
+
+struct BitOrOp {
+  template <typename T> TL_DEVICE T operator()(T const &x, T const &y) {
+    return x | y;
+  }
+};
+
+struct BitXorOp {
+  template <typename T> TL_DEVICE T operator()(T const &x, T const &y) {
+    return x ^ y;
+  }
+};
+
+template <class Reducer, int Threads, bool UseAbs, bool NeedAccumulate>
+struct SharedReduceWarp {
+  template <typename T>
+  static TL_DEVICE void run(const T *__restrict__ src, T *__restrict__ dst,
+                            int total_dest, int reduce_extent, int tail,
+                            T init_value) {
+    if (total_dest <= 0 || reduce_extent <= 0)
+      return;
+    constexpr int kWarpSize = 32;
+    static_assert(Threads % kWarpSize == 0,
+                  "SharedReduceWarp expects blockDim.x to be a multiple of "
+                  "warp size on CUDA.");
+    const int tid = threadIdx.x;
+    const int warp_id = tid / kWarpSize;
+    const int lane = tid % kWarpSize;
+    const int num_warps = Threads / kWarpSize;
+    for (int dest_idx = warp_id; dest_idx < total_dest; dest_idx += num_warps) {
+      const int prefix = tail == 1 ? dest_idx : dest_idx / tail;
+      const int suffix = tail == 1 ? 0 : dest_idx % tail;
+      const int src_base = (prefix * reduce_extent) * tail + suffix;
+      const int dst_index = prefix * tail + suffix;
+
+      T partial = init_value;
+      for (int rv = lane; rv < reduce_extent; rv += kWarpSize) {
+        T val = src[src_base + rv * tail];
+        if constexpr (UseAbs) {
+          val = val < T(0) ? -val : val;
+        }
+        partial = Reducer()(partial, val);
+      }
+
+      unsigned mask = __activemask();
+      for (int offset = kWarpSize / 2; offset > 0; offset >>= 1) {
+        T other = __shfl_down_sync(mask, partial, offset);
+        partial = Reducer()(partial, other);
+      }
+
+      if (lane == 0) {
+        if constexpr (NeedAccumulate) {
+          partial = Reducer()(dst[dst_index], partial);
+        }
+        dst[dst_index] = partial;
+      }
+    }
+  }
+};
+
 template <class Reducer, int threads, int scale, int thread_offset = 0,
           int all_threads = threads>
 struct AllReduce {
@@ -68,6 +133,74 @@ struct AllReduce {
   }
 };
 
+template <int threads, bool reverse = false> struct CumSum1D {
+  static_assert(threads == 1024 or threads == 512 or threads == 256 or
+                threads == 128 or threads == 64 or threads == 32);
+  template <typename T, int SEG = 32>
+  static TL_DEVICE void run(const T *__restrict__ src, T *__restrict__ dst,
+                            int N) {
+    if (N <= 0)
+      return;
+
+    constexpr unsigned MASK = 0xffffffff;
+    const int tid = threadIdx.x;
+    const int lane = tid % SEG;
+
+    if (tid >= SEG)
+      return;
+
+    T carry = (T)0;
+
+    if (reverse) {
+      const int num_segments = (N + SEG - 1) / SEG;
+      for (int seg = num_segments - 1; seg >= 0; --seg) {
+        const int idx = seg * SEG + lane;
+        T val = (idx < N) ? src[idx] : (T)0;
+
+#pragma unroll
+        for (int off = 1; off < SEG; off <<= 1) {
+          T n = (T)__shfl_down_sync(MASK, val, off);
+          if (lane < SEG - off)
+            val += n;
+        }
+
+        val += carry;
+
+        if (idx < N)
+          dst[idx] = val;
+
+        T segSum = (T)__shfl_sync(MASK, val, 0);
+        if (lane == 0)
+          carry = segSum;
+        carry = (T)__shfl_sync(MASK, carry, 0);
+      }
+    } else {
+      const int num_segments = (N + SEG - 1) / SEG;
+      for (int seg = 0; seg < num_segments; ++seg) {
+        const int idx = seg * SEG + lane;
+        T val = (idx < N) ? src[idx] : (T)0;
+
+#pragma unroll
+        for (int off = 1; off < SEG; off <<= 1) {
+          T n = (T)__shfl_up_sync(MASK, val, off);
+          if (lane >= off)
+            val += n;
+        }
+
+        val += carry;
+
+        if (idx < N)
+          dst[idx] = val;
+
+        T segSum = (T)__shfl_sync(MASK, val, SEG - 1);
+        if (lane == SEG - 1)
+          carry = segSum;
+        carry = (T)__shfl_sync(MASK, carry, SEG - 1);
+      }
+    }
+  }
+};
+
 template <int threads, int Axis = 0, bool reverse = false> struct CumSum2D {
   static_assert(threads == 1024 or threads == 512 or threads == 256 or
                 threads == 128 or threads == 64 or threads == 32);
@@ -147,4 +280,37 @@ template <int threads, int Axis = 0, bool reverse = false> struct CumSum2D {
   }
 };
 
+// TileScale extra
+
+template <typename T, typename ReduceOp>
+TL_DEVICE T warp_reduce(T value, ReduceOp op) {
+  constexpr uint32_t mask = 0xffffffff;
+  value = op(value, __shfl_xor_sync(mask, value, 16));
+  value = op(value, __shfl_xor_sync(mask, value, 8));
+  value = op(value, __shfl_xor_sync(mask, value, 4));
+  value = op(value, __shfl_xor_sync(mask, value, 2));
+  value = op(value, __shfl_xor_sync(mask, value, 1));
+  return value;
+}
+
+template <typename T> TL_DEVICE T warp_reduce_sum(T value) {
+  return warp_reduce<T>(value, SumOp());
+}
+
+template <typename T> TL_DEVICE T warp_reduce_max(T value) {
+  return warp_reduce<T>(value, MaxOp());
+}
+
+template <typename T> TL_DEVICE T warp_reduce_min(T value) {
+  return warp_reduce<T>(value, MinOp());
+}
+
+template <typename T> TL_DEVICE T warp_reduce_bitand(T value) {
+  return warp_reduce<T>(value, BitAndOp());
+}
+
+template <typename T> TL_DEVICE T warp_reduce_bitor(T value) {
+  return warp_reduce<T>(value, BitOrOp());
+}
+
 } // namespace tl
diff --git a/src/tl_templates/cuda/sync.h b/src/tl_templates/cuda/sync.h
new file mode 100644
index 00000000..cad94ee7
--- /dev/null
+++ b/src/tl_templates/cuda/sync.h
@@ -0,0 +1,245 @@
+#pragma once
+
+#include "common.h"
+#include "ldst.h"
+
+#define IS_MASTER_THREAD()                                                     \
+  (threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.z == 0)
+#define IS_MASTER_BLOCK()                                                      \
+  (blockIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0)
+
+#define BARRIER_MAGIC 0x80000000
+
+namespace tl {
+
+// Triggers a GPU trap for debugging
+TL_DEVICE void trap() { asm("trap;\n"); }
+
+// CTA-level memory fence
+TL_DEVICE void memory_fence_cta() {
+  asm volatile("fence.acq_rel.cta;\n" ::: "memory");
+}
+
+// GPU-level memory fence
+TL_DEVICE void memory_fence_gpu() {
+  asm volatile("fence.acq_rel.gpu;\n" ::: "memory");
+}
+
+// System-level memory fence
+TL_DEVICE void memory_fence_sys() {
+  asm volatile("fence.acq_rel.sys;\n" ::: "memory");
+}
+
+// GPU-level load with acquire semantics
+TL_DEVICE uint32_t ld_acquire_gpu_u32(const uint32_t *ptr) {
+  uint32_t ret;
+  asm volatile("ld.acquire.gpu.global.u32 %0, [%1];\n" : "=r"(ret) : "l"(ptr));
+  return ret;
+}
+
+// GPU-level atomic add with release semantics
+TL_DEVICE uint32_t atomic_add_release_gpu_u32(const uint32_t *ptr,
+                                              uint32_t value) {
+  uint32_t ret;
+  asm volatile("atom.add.release.gpu.global.s32 %0, [%1], %2;\n"
+               : "=r"(ret)
+               : "l"(ptr), "r"(value));
+  return ret;
+}
+
+// System-level atomic load with acquire semantics
+TL_DEVICE int atomic_load_acquire_sys_s32(const int *ptr) {
+  int ret;
+  asm volatile("atom.load.acquire.sys.global.s32 %0, [%1];\n"
+               : "=r"(ret)
+               : "l"(ptr));
+  return ret;
+}
+
+TL_DEVICE int ld_volatile_global(const int *ptr) {
+  int ret;
+  asm volatile("ld.volatile.global.s32 %0, [%1];\n" : "=r"(ret) : "l"(ptr));
+  return ret;
+}
+
+TL_DEVICE int ld_acquire(const int *ptr) {
+  int ret = 0;
+  asm volatile("ld.global.acquire.gpu.b32 %0, [%1];\n" : "=r"(ret) : "l"(ptr));
+  return ret;
+}
+
+// Initialize a GPU barrier
+template <const uint32_t kExpected>
+TL_DEVICE void init_barrier_gpu(uint32_t *barrier) {
+  if (IS_MASTER_BLOCK() && IS_MASTER_THREAD()) {
+    *barrier = BARRIER_MAGIC - kExpected;
+  }
+  memory_fence_gpu(); // TODO: Is fence or sync needed here?
+}
+
+// Arrive at a GPU barrier (atomic increment)
+TL_DEVICE void arrive_barrier_gpu(uint32_t *barrier) {
+  memory_fence_gpu();
+  if (IS_MASTER_THREAD()) {
+    atomic_add_release_gpu_u32(barrier, 1);
+  }
+}
+
+// Wait at a GPU barrier until all expected blocks have arrived
+TL_DEVICE void wait_barrier_gpu(uint32_t *barrier) {
+  if (IS_MASTER_THREAD()) {
+    uint32_t arrive = ld_acquire_gpu_u32(barrier);
+    while (!(arrive & BARRIER_MAGIC)) {
+      arrive = ld_acquire_gpu_u32(barrier);
+    }
+  }
+  __syncthreads();
+}
+
+// Synchronize at a GPU barrier (arrive + wait)
+TL_DEVICE void sync_barrier_gpu(uint32_t *barrier) {
+  // memory_fence_gpu();
+  __syncthreads();
+  if (IS_MASTER_THREAD()) {
+    atomic_add_release_gpu_u32(barrier, 1);
+    uint32_t arrive = ld_acquire_gpu_u32(barrier);
+    while (arrive < BARRIER_MAGIC) {
+      arrive = ld_acquire_gpu_u32(barrier);
+    }
+  }
+  __syncthreads();
+}
+
+// cooperative groups version of GPU barrier arrive
+TL_DEVICE unsigned int sync_grids_arrive(uint32_t *barrier) {
+  unsigned int oldArrive = 0;
+
+  __syncthreads();
+
+  if (IS_MASTER_THREAD()) {
+    unsigned int expected = gridDim.x * gridDim.y * gridDim.z;
+    unsigned int nb = 1;
+    if (IS_MASTER_BLOCK()) {
+      nb = 0x80000000 - (expected - 1);
+    }
+    asm volatile("atom.add.release.gpu.u32 %0,[%1],%2;"
+                 : "=r"(oldArrive)
+                 : "l"((unsigned int *)barrier), "r"(nb)
+                 : "memory");
+  }
+
+  return oldArrive;
+}
+
+// cooperative groups version of GPU barrier arrive
+TL_DEVICE void sync_grids_wait(unsigned int oldArrive, uint32_t *barrier) {
+  if (IS_MASTER_THREAD()) {
+    unsigned int current_arrive;
+    do {
+      asm volatile("ld.acquire.gpu.u32 %0,[%1];"
+                   : "=r"(current_arrive)
+                   : "l"((unsigned int *)barrier)
+                   : "memory");
+    } while (!(((oldArrive ^ current_arrive) & 0x80000000) != 0));
+  }
+  __syncthreads();
+}
+
+TL_DEVICE void sync_grid(uint32_t *barrier) {
+  unsigned int token = sync_grids_arrive(barrier);
+  sync_grids_wait(token, barrier);
+}
+
+// Sync blocks at a system-level barrier with an optinal fence
+// TODO(wt): Add timeout handling
+
+template <bool need_fence = true>
+TL_DEVICE void barrier_blocks(int offset, int rank, int num_ranks) {
+// Macro to compute the barrier pointer for a given target rank
+#define BARRIER_PTR(tgt_rank)                                                  \
+  (reinterpret_cast<int32_t *>(get_remote_base_ptr(tgt_rank) + offset))
+#define FINISHED_SUM_TAG (1024)
+
+  if constexpr (need_fence) {
+    memory_fence_sys();
+    __syncthreads();
+  }
+
+  int tid = threadIdx.x;
+  if (tid < num_ranks) {
+    atomicAdd_system(BARRIER_PTR(rank) + tid, FINISHED_SUM_TAG);
+    atomicSub_system(BARRIER_PTR(tid) + rank, FINISHED_SUM_TAG);
+  }
+
+  while (true) {
+    int value =
+        tid < num_ranks ? ld_volatile_global(BARRIER_PTR(rank) + tid) : 0;
+    if (__all_sync(0xffffffff, value <= 0)) {
+      break;
+    }
+  }
+  __syncthreads();
+
+#undef BARRIER_PTR
+#undef FINISHED_SUM_TAG
+}
+
+template <typename T> TL_DEVICE void wait_eq(void *ptr, T val) {
+  T *flag_ptr = reinterpret_cast<T *>(ptr);
+// Spin-loop
+#pragma unroll 1
+  while (ld_acquire(flag_ptr) != val)
+    ;
+}
+
+template <typename P, typename T> TL_DEVICE void wait_ne(P ptr, T val) {
+  static_assert(std::is_same_v<P, uint64_t> || std::is_pointer_v<P>,
+                "P must be a pointer or uint64_t");
+  T *flag_ptr = reinterpret_cast<T *>(ptr);
+// Spin-loop
+#pragma unroll 1
+  while (ld_volatile_global(flag_ptr) == val)
+    ;
+}
+
+template <typename P, typename T> TL_DEVICE void wait_ge(P ptr, T val) {
+  static_assert(std::is_same_v<P, uint64_t> || std::is_pointer_v<P>,
+                "P must be a pointer or uint64_t");
+  T *flag_ptr = reinterpret_cast<T *>(ptr);
+// Spin-loop
+#pragma unroll 1
+  while (ld_volatile_global(flag_ptr) < val)
+    ;
+}
+
+template <typename P, typename T> TL_DEVICE void wait_le(P ptr, T val) {
+  static_assert(std::is_same_v<P, uint64_t> || std::is_pointer_v<P>,
+                "P must be a pointer or uint64_t");
+  T *flag_ptr = reinterpret_cast<T *>(ptr);
+// Spin-loop
+#pragma unroll 1
+  while (ld_volatile_global(flag_ptr) > val)
+    ;
+}
+
+template <typename P, typename T> TL_DEVICE void wait_gt(P ptr, T val) {
+  static_assert(std::is_same_v<P, uint64_t> || std::is_pointer_v<P>,
+                "P must be a pointer or uint64_t");
+  T *flag_ptr = reinterpret_cast<T *>(ptr);
+// Spin-loop
+#pragma unroll 1
+  while (ld_volatile_global(flag_ptr) <= val)
+    ;
+}
+
+template <typename P, typename T> TL_DEVICE void wait_lt(P ptr, T val) {
+  static_assert(std::is_same_v<P, uint64_t> || std::is_pointer_v<P>,
+                "P must be a pointer or uint64_t");
+  T *flag_ptr = reinterpret_cast<T *>(ptr);
+// Spin-loop
+#pragma unroll 1
+  while (ld_volatile_global(flag_ptr) >= val)
+    ;
+}
+
+} // namespace tl
diff --git a/src/tl_templates/cuda/tcgen_05.h b/src/tl_templates/cuda/tcgen_05.h
new file mode 100644
index 00000000..1211bc24
--- /dev/null
+++ b/src/tl_templates/cuda/tcgen_05.h
@@ -0,0 +1,70 @@
+#pragma once
+
+#include <cstdint>
+#ifndef __CUDACC_RTC__
+#include <cuda.h>
+#endif
+
+#include "common.h"
+
+namespace tl {
+
+TL_DEVICE void tmem_allocate(void *dst_ptr, int num_columns) {
+  uint32_t dst_intptr = smem_ptr_to_uint(dst_ptr);
+  asm volatile(
+      "tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [%0], %1;"
+      :
+      : "r"(dst_intptr), "r"(num_columns));
+}
+
+TL_DEVICE void tmem_deallocate(uint32_t *tmem_ptr, int num_columns) {
+  asm volatile("{\n\t"
+               "tcgen05.dealloc.cta_group::1.sync.aligned.b32  %0, %1; \n\t"
+               "}"
+               :
+               : "r"(*tmem_ptr), "r"(num_columns));
+}
+
+inline void __device__ fence_view_async_tmem_load() {
+  asm volatile("tcgen05.wait::ld.sync.aligned; " ::);
+}
+
+inline void __device__ fence_view_async_tmem_store() {
+  asm volatile("tcgen05.wait::st.sync.aligned; " ::);
+}
+
+template <int M, int N>
+inline void __device__ amma_fp16bf16_ss(uint64_t const desc_a,
+                                        uint64_t const desc_b,
+                                        uint32_t const tmem_c,
+                                        uint32_t const idesc,
+                                        uint32_t const addC = 1) {
+  static_assert(M == 64 || M == 128, "SM100_MMA_F16BF16 M-mode size should be "
+                                     "64 or 128 for 1 CTA cluster MMA.");
+  static_assert(
+      (M == 64 && (N % 8 == 0) && (8 <= N) && (N <= 256)) ||
+          (M == 128 && (N % 16 == 0) && (16 <= N) && (N <= 256)),
+      "SM100_MMA_F16BF16 N-mode size should be a multiple of 8 between 8 and 256 for M=64,\
+                 or a multiple of 16 between 16 and 256 for M=128.");
+
+  uint32_t mask[4] = {0, 0, 0, 0};
+  asm volatile("{\n\t"
+               ".reg .pred p;\n\t"
+               "setp.ne.b32 p, %4, 0;\n\t"
+               "tcgen05.mma.cta_group::1.kind::f16 [%0], %1, %2, %3, {%5, %6, "
+               "%7, %8}, p; \n\t"
+               "}\n"
+               :
+               : "r"(tmem_c), "l"(desc_a), "l"(desc_b), "r"(idesc), "r"(addC),
+                 "r"(mask[0]), "r"(mask[1]), "r"(mask[2]), "r"(mask[3]));
+}
+
+inline __device__ void amma_commit(uint64_t const *smem_ptr) {
+  uint32_t bar_intptr = smem_ptr_to_uint(smem_ptr);
+  asm volatile("tcgen05.commit.cta_group::1.mbarrier::arrive::one.shared::"
+               "cluster.b64 [%0];"
+               :
+               : "r"(bar_intptr));
+}
+
+} // namespace tl
\ No newline at end of file
diff --git a/src/tl_templates/cuda/tcgen_05_ld.h b/src/tl_templates/cuda/tcgen_05_ld.h
new file mode 100644
index 00000000..b2eb2f81
--- /dev/null
+++ b/src/tl_templates/cuda/tcgen_05_ld.h
@@ -0,0 +1,713 @@
+#pragma once
+
+#include <cstdint>
+#ifndef __CUDACC_RTC__
+#include <cuda.h>
+#endif
+
+#include "common.h"
+
+namespace tl {
+
+// 32 data path lanes, 32-bit pattern, repeated N times
+class tmem_ld_32dp32bNx {
+public:
+  template <int N>
+  static TL_DEVICE void copy(uint32_t const &src_addr, uint32_t *dst_ptr) {
+    static_assert(N > 0 && (N & (N - 1)) == 0 && N <= 128,
+                  "N must be a power of 2 and lies between 1 ~ 128");
+
+    if constexpr (N == 1) {
+      asm volatile("tcgen05.ld.sync.aligned.32x32b.x1.b32"
+                   "{%0},"
+                   "[%1];\n"
+                   : "=r"(dst_ptr[0])
+                   : "r"(src_addr));
+    } else if constexpr (N == 2) {
+      asm volatile("tcgen05.ld.sync.aligned.32x32b.x2.b32"
+                   "{%0, %1},"
+                   "[%2];\n"
+                   : "=r"(dst_ptr[0]), "=r"(dst_ptr[1])
+                   : "r"(src_addr));
+    } else if constexpr (N == 4) {
+      asm volatile("tcgen05.ld.sync.aligned.32x32b.x4.b32"
+                   "{%0, %1, %2, %3},"
+                   "[%4];\n"
+                   : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+                     "=r"(dst_ptr[3])
+                   : "r"(src_addr));
+    } else if constexpr (N == 8) {
+      asm volatile("tcgen05.ld.sync.aligned.32x32b.x8.b32"
+                   "{%0, %1, %2, %3, %4, %5, %6, %7},"
+                   "[%8];\n"
+                   : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+                     "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+                     "=r"(dst_ptr[6]), "=r"(dst_ptr[7])
+                   : "r"(src_addr));
+    } else if constexpr (N == 16) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.32x32b.x16.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, "
+          "%14, %15},"
+          "[%16];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15])
+          : "r"(src_addr));
+    } else if constexpr (N == 32) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.32x32b.x32.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, "
+          "%14, %15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, "
+          "%26, %27, %28, %29, %30, %31},"
+          "[%32];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15]), "=r"(dst_ptr[16]), "=r"(dst_ptr[17]),
+            "=r"(dst_ptr[18]), "=r"(dst_ptr[19]), "=r"(dst_ptr[20]),
+            "=r"(dst_ptr[21]), "=r"(dst_ptr[22]), "=r"(dst_ptr[23]),
+            "=r"(dst_ptr[24]), "=r"(dst_ptr[25]), "=r"(dst_ptr[26]),
+            "=r"(dst_ptr[27]), "=r"(dst_ptr[28]), "=r"(dst_ptr[29]),
+            "=r"(dst_ptr[30]), "=r"(dst_ptr[31])
+          : "r"(src_addr));
+    } else if constexpr (N == 64) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.32x32b.x64.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, "
+          "%15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, %26, %27, "
+          "%28, "
+          "%29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, "
+          "%42, "
+          "%43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, "
+          "%56, "
+          "%57, %58, %59, %60, %61, %62, %63},"
+          "[%64];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15]), "=r"(dst_ptr[16]), "=r"(dst_ptr[17]),
+            "=r"(dst_ptr[18]), "=r"(dst_ptr[19]), "=r"(dst_ptr[20]),
+            "=r"(dst_ptr[21]), "=r"(dst_ptr[22]), "=r"(dst_ptr[23]),
+            "=r"(dst_ptr[24]), "=r"(dst_ptr[25]), "=r"(dst_ptr[26]),
+            "=r"(dst_ptr[27]), "=r"(dst_ptr[28]), "=r"(dst_ptr[29]),
+            "=r"(dst_ptr[30]), "=r"(dst_ptr[31]), "=r"(dst_ptr[32]),
+            "=r"(dst_ptr[33]), "=r"(dst_ptr[34]), "=r"(dst_ptr[35]),
+            "=r"(dst_ptr[36]), "=r"(dst_ptr[37]), "=r"(dst_ptr[38]),
+            "=r"(dst_ptr[39]), "=r"(dst_ptr[40]), "=r"(dst_ptr[41]),
+            "=r"(dst_ptr[42]), "=r"(dst_ptr[43]), "=r"(dst_ptr[44]),
+            "=r"(dst_ptr[45]), "=r"(dst_ptr[46]), "=r"(dst_ptr[47]),
+            "=r"(dst_ptr[48]), "=r"(dst_ptr[49]), "=r"(dst_ptr[50]),
+            "=r"(dst_ptr[51]), "=r"(dst_ptr[52]), "=r"(dst_ptr[53]),
+            "=r"(dst_ptr[54]), "=r"(dst_ptr[55]), "=r"(dst_ptr[56]),
+            "=r"(dst_ptr[57]), "=r"(dst_ptr[58]), "=r"(dst_ptr[59]),
+            "=r"(dst_ptr[60]), "=r"(dst_ptr[61]), "=r"(dst_ptr[62]),
+            "=r"(dst_ptr[63])
+          : "r"(src_addr));
+    } else if constexpr (N == 128) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.32x32b.x128.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, "
+          "%15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, %26, %27, "
+          "%28, "
+          "%29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, "
+          "%42, "
+          "%43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, "
+          "%56, "
+          "%57, %58, %59, %60, %61, %62, %63, %64, %65, %66, %67, %68, %69, "
+          "%70, "
+          "%71, %72, %73, %74, %75, %76, %77, %78, %79, %80, %81, %82, %83, "
+          "%84, "
+          "%85, %86, %87, %88, %89, %90, %91, %92, %93, %94, %95, %96, %97, "
+          "%98, "
+          "%99, %100, %101, %102, %103, %104, %105, %106, %107, %108, %109, "
+          "%110, %111, %112, %113, %114, %115, %116, %117, %118, %119, %120, "
+          "%121, %122, %123, %124, %125, %126, %127},"
+          "[%128];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15]), "=r"(dst_ptr[16]), "=r"(dst_ptr[17]),
+            "=r"(dst_ptr[18]), "=r"(dst_ptr[19]), "=r"(dst_ptr[20]),
+            "=r"(dst_ptr[21]), "=r"(dst_ptr[22]), "=r"(dst_ptr[23]),
+            "=r"(dst_ptr[24]), "=r"(dst_ptr[25]), "=r"(dst_ptr[26]),
+            "=r"(dst_ptr[27]), "=r"(dst_ptr[28]), "=r"(dst_ptr[29]),
+            "=r"(dst_ptr[30]), "=r"(dst_ptr[31]), "=r"(dst_ptr[32]),
+            "=r"(dst_ptr[33]), "=r"(dst_ptr[34]), "=r"(dst_ptr[35]),
+            "=r"(dst_ptr[36]), "=r"(dst_ptr[37]), "=r"(dst_ptr[38]),
+            "=r"(dst_ptr[39]), "=r"(dst_ptr[40]), "=r"(dst_ptr[41]),
+            "=r"(dst_ptr[42]), "=r"(dst_ptr[43]), "=r"(dst_ptr[44]),
+            "=r"(dst_ptr[45]), "=r"(dst_ptr[46]), "=r"(dst_ptr[47]),
+            "=r"(dst_ptr[48]), "=r"(dst_ptr[49]), "=r"(dst_ptr[50]),
+            "=r"(dst_ptr[51]), "=r"(dst_ptr[52]), "=r"(dst_ptr[53]),
+            "=r"(dst_ptr[54]), "=r"(dst_ptr[55]), "=r"(dst_ptr[56]),
+            "=r"(dst_ptr[57]), "=r"(dst_ptr[58]), "=r"(dst_ptr[59]),
+            "=r"(dst_ptr[60]), "=r"(dst_ptr[61]), "=r"(dst_ptr[62]),
+            "=r"(dst_ptr[63]), "=r"(dst_ptr[64]), "=r"(dst_ptr[65]),
+            "=r"(dst_ptr[66]), "=r"(dst_ptr[67]), "=r"(dst_ptr[68]),
+            "=r"(dst_ptr[69]), "=r"(dst_ptr[70]), "=r"(dst_ptr[71]),
+            "=r"(dst_ptr[72]), "=r"(dst_ptr[73]), "=r"(dst_ptr[74]),
+            "=r"(dst_ptr[75]), "=r"(dst_ptr[76]), "=r"(dst_ptr[77]),
+            "=r"(dst_ptr[78]), "=r"(dst_ptr[79]), "=r"(dst_ptr[80]),
+            "=r"(dst_ptr[81]), "=r"(dst_ptr[82]), "=r"(dst_ptr[83]),
+            "=r"(dst_ptr[84]), "=r"(dst_ptr[85]), "=r"(dst_ptr[86]),
+            "=r"(dst_ptr[87]), "=r"(dst_ptr[88]), "=r"(dst_ptr[89]),
+            "=r"(dst_ptr[90]), "=r"(dst_ptr[91]), "=r"(dst_ptr[92]),
+            "=r"(dst_ptr[93]), "=r"(dst_ptr[94]), "=r"(dst_ptr[95]),
+            "=r"(dst_ptr[96]), "=r"(dst_ptr[97]), "=r"(dst_ptr[98]),
+            "=r"(dst_ptr[99]), "=r"(dst_ptr[100]), "=r"(dst_ptr[101]),
+            "=r"(dst_ptr[102]), "=r"(dst_ptr[103]), "=r"(dst_ptr[104]),
+            "=r"(dst_ptr[105]), "=r"(dst_ptr[106]), "=r"(dst_ptr[107]),
+            "=r"(dst_ptr[108]), "=r"(dst_ptr[109]), "=r"(dst_ptr[110]),
+            "=r"(dst_ptr[111]), "=r"(dst_ptr[112]), "=r"(dst_ptr[113]),
+            "=r"(dst_ptr[114]), "=r"(dst_ptr[115]), "=r"(dst_ptr[116]),
+            "=r"(dst_ptr[117]), "=r"(dst_ptr[118]), "=r"(dst_ptr[119]),
+            "=r"(dst_ptr[120]), "=r"(dst_ptr[121]), "=r"(dst_ptr[122]),
+            "=r"(dst_ptr[123]), "=r"(dst_ptr[124]), "=r"(dst_ptr[125]),
+            "=r"(dst_ptr[126]), "=r"(dst_ptr[127])
+          : "r"(src_addr));
+    } else {
+      asm volatile("trap");
+    }
+  }
+};
+
+// 16 data path lanes, 64-bit pattern, repeated N times
+class tmem_ld_16dp64bNx {
+public:
+  template <int N>
+  static TL_DEVICE void copy(uint32_t const &src_addr, uint32_t *dst_ptr) {
+    static_assert(N > 0 && (N & (N - 1)) == 0 && N <= 128,
+                  "N must be a power of 2 and lies between 1 ~ 128");
+
+    if constexpr (N == 1) {
+      asm volatile("tcgen05.ld.sync.aligned.16x64b.x1.b32"
+                   "{%0},"
+                   "[%1];\n"
+                   : "=r"(dst_ptr[0])
+                   : "r"(src_addr));
+    } else if constexpr (N == 2) {
+      asm volatile("tcgen05.ld.sync.aligned.16x64b.x2.b32"
+                   "{%0, %1},"
+                   "[%2];\n"
+                   : "=r"(dst_ptr[0]), "=r"(dst_ptr[1])
+                   : "r"(src_addr));
+    } else if constexpr (N == 4) {
+      asm volatile("tcgen05.ld.sync.aligned.16x64b.x4.b32"
+                   "{%0, %1, %2, %3},"
+                   "[%4];\n"
+                   : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+                     "=r"(dst_ptr[3])
+                   : "r"(src_addr));
+    } else if constexpr (N == 8) {
+      asm volatile("tcgen05.ld.sync.aligned.16x64b.x8.b32"
+                   "{%0, %1, %2, %3, %4, %5, %6, %7},"
+                   "[%8];\n"
+                   : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+                     "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+                     "=r"(dst_ptr[6]), "=r"(dst_ptr[7])
+                   : "r"(src_addr));
+    } else if constexpr (N == 16) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.16x64b.x16.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, "
+          "%14, %15},"
+          "[%16];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15])
+          : "r"(src_addr));
+    } else if constexpr (N == 32) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.16x64b.x32.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, "
+          "%14, %15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, "
+          "%26, %27, %28, %29, %30, %31},"
+          "[%32];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15]), "=r"(dst_ptr[16]), "=r"(dst_ptr[17]),
+            "=r"(dst_ptr[18]), "=r"(dst_ptr[19]), "=r"(dst_ptr[20]),
+            "=r"(dst_ptr[21]), "=r"(dst_ptr[22]), "=r"(dst_ptr[23]),
+            "=r"(dst_ptr[24]), "=r"(dst_ptr[25]), "=r"(dst_ptr[26]),
+            "=r"(dst_ptr[27]), "=r"(dst_ptr[28]), "=r"(dst_ptr[29]),
+            "=r"(dst_ptr[30]), "=r"(dst_ptr[31])
+          : "r"(src_addr));
+    } else if constexpr (N == 64) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.16x64b.x64.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, "
+          "%15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, %26, %27, "
+          "%28, "
+          "%29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, "
+          "%42, "
+          "%43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, "
+          "%56, "
+          "%57, %58, %59, %60, %61, %62, %63},"
+          "[%64];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15]), "=r"(dst_ptr[16]), "=r"(dst_ptr[17]),
+            "=r"(dst_ptr[18]), "=r"(dst_ptr[19]), "=r"(dst_ptr[20]),
+            "=r"(dst_ptr[21]), "=r"(dst_ptr[22]), "=r"(dst_ptr[23]),
+            "=r"(dst_ptr[24]), "=r"(dst_ptr[25]), "=r"(dst_ptr[26]),
+            "=r"(dst_ptr[27]), "=r"(dst_ptr[28]), "=r"(dst_ptr[29]),
+            "=r"(dst_ptr[30]), "=r"(dst_ptr[31]), "=r"(dst_ptr[32]),
+            "=r"(dst_ptr[33]), "=r"(dst_ptr[34]), "=r"(dst_ptr[35]),
+            "=r"(dst_ptr[36]), "=r"(dst_ptr[37]), "=r"(dst_ptr[38]),
+            "=r"(dst_ptr[39]), "=r"(dst_ptr[40]), "=r"(dst_ptr[41]),
+            "=r"(dst_ptr[42]), "=r"(dst_ptr[43]), "=r"(dst_ptr[44]),
+            "=r"(dst_ptr[45]), "=r"(dst_ptr[46]), "=r"(dst_ptr[47]),
+            "=r"(dst_ptr[48]), "=r"(dst_ptr[49]), "=r"(dst_ptr[50]),
+            "=r"(dst_ptr[51]), "=r"(dst_ptr[52]), "=r"(dst_ptr[53]),
+            "=r"(dst_ptr[54]), "=r"(dst_ptr[55]), "=r"(dst_ptr[56]),
+            "=r"(dst_ptr[57]), "=r"(dst_ptr[58]), "=r"(dst_ptr[59]),
+            "=r"(dst_ptr[60]), "=r"(dst_ptr[61]), "=r"(dst_ptr[62]),
+            "=r"(dst_ptr[63])
+          : "r"(src_addr));
+    } else if constexpr (N == 128) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.16x64b.x128.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, "
+          "%15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, %26, %27, "
+          "%28, "
+          "%29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, "
+          "%42, "
+          "%43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, "
+          "%56, "
+          "%57, %58, %59, %60, %61, %62, %63, %64, %65, %66, %67, %68, %69, "
+          "%70, "
+          "%71, %72, %73, %74, %75, %76, %77, %78, %79, %80, %81, %82, %83, "
+          "%84, "
+          "%85, %86, %87, %88, %89, %90, %91, %92, %93, %94, %95, %96, %97, "
+          "%98, "
+          "%99, %100, %101, %102, %103, %104, %105, %106, %107, %108, %109, "
+          "%110, %111, %112, %113, %114, %115, %116, %117, %118, %119, %120, "
+          "%121, %122, %123, %124, %125, %126, %127},"
+          "[%128];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15]), "=r"(dst_ptr[16]), "=r"(dst_ptr[17]),
+            "=r"(dst_ptr[18]), "=r"(dst_ptr[19]), "=r"(dst_ptr[20]),
+            "=r"(dst_ptr[21]), "=r"(dst_ptr[22]), "=r"(dst_ptr[23]),
+            "=r"(dst_ptr[24]), "=r"(dst_ptr[25]), "=r"(dst_ptr[26]),
+            "=r"(dst_ptr[27]), "=r"(dst_ptr[28]), "=r"(dst_ptr[29]),
+            "=r"(dst_ptr[30]), "=r"(dst_ptr[31]), "=r"(dst_ptr[32]),
+            "=r"(dst_ptr[33]), "=r"(dst_ptr[34]), "=r"(dst_ptr[35]),
+            "=r"(dst_ptr[36]), "=r"(dst_ptr[37]), "=r"(dst_ptr[38]),
+            "=r"(dst_ptr[39]), "=r"(dst_ptr[40]), "=r"(dst_ptr[41]),
+            "=r"(dst_ptr[42]), "=r"(dst_ptr[43]), "=r"(dst_ptr[44]),
+            "=r"(dst_ptr[45]), "=r"(dst_ptr[46]), "=r"(dst_ptr[47]),
+            "=r"(dst_ptr[48]), "=r"(dst_ptr[49]), "=r"(dst_ptr[50]),
+            "=r"(dst_ptr[51]), "=r"(dst_ptr[52]), "=r"(dst_ptr[53]),
+            "=r"(dst_ptr[54]), "=r"(dst_ptr[55]), "=r"(dst_ptr[56]),
+            "=r"(dst_ptr[57]), "=r"(dst_ptr[58]), "=r"(dst_ptr[59]),
+            "=r"(dst_ptr[60]), "=r"(dst_ptr[61]), "=r"(dst_ptr[62]),
+            "=r"(dst_ptr[63]), "=r"(dst_ptr[64]), "=r"(dst_ptr[65]),
+            "=r"(dst_ptr[66]), "=r"(dst_ptr[67]), "=r"(dst_ptr[68]),
+            "=r"(dst_ptr[69]), "=r"(dst_ptr[70]), "=r"(dst_ptr[71]),
+            "=r"(dst_ptr[72]), "=r"(dst_ptr[73]), "=r"(dst_ptr[74]),
+            "=r"(dst_ptr[75]), "=r"(dst_ptr[76]), "=r"(dst_ptr[77]),
+            "=r"(dst_ptr[78]), "=r"(dst_ptr[79]), "=r"(dst_ptr[80]),
+            "=r"(dst_ptr[81]), "=r"(dst_ptr[82]), "=r"(dst_ptr[83]),
+            "=r"(dst_ptr[84]), "=r"(dst_ptr[85]), "=r"(dst_ptr[86]),
+            "=r"(dst_ptr[87]), "=r"(dst_ptr[88]), "=r"(dst_ptr[89]),
+            "=r"(dst_ptr[90]), "=r"(dst_ptr[91]), "=r"(dst_ptr[92]),
+            "=r"(dst_ptr[93]), "=r"(dst_ptr[94]), "=r"(dst_ptr[95]),
+            "=r"(dst_ptr[96]), "=r"(dst_ptr[97]), "=r"(dst_ptr[98]),
+            "=r"(dst_ptr[99]), "=r"(dst_ptr[100]), "=r"(dst_ptr[101]),
+            "=r"(dst_ptr[102]), "=r"(dst_ptr[103]), "=r"(dst_ptr[104]),
+            "=r"(dst_ptr[105]), "=r"(dst_ptr[106]), "=r"(dst_ptr[107]),
+            "=r"(dst_ptr[108]), "=r"(dst_ptr[109]), "=r"(dst_ptr[110]),
+            "=r"(dst_ptr[111]), "=r"(dst_ptr[112]), "=r"(dst_ptr[113]),
+            "=r"(dst_ptr[114]), "=r"(dst_ptr[115]), "=r"(dst_ptr[116]),
+            "=r"(dst_ptr[117]), "=r"(dst_ptr[118]), "=r"(dst_ptr[119]),
+            "=r"(dst_ptr[120]), "=r"(dst_ptr[121]), "=r"(dst_ptr[122]),
+            "=r"(dst_ptr[123]), "=r"(dst_ptr[124]), "=r"(dst_ptr[125]),
+            "=r"(dst_ptr[126]), "=r"(dst_ptr[127])
+          : "r"(src_addr));
+    } else {
+      asm volatile("trap");
+    }
+  }
+};
+
+// 16 data path lanes, 128-bit pattern, repeated N times
+class tmem_ld_16dp128bNx {
+public:
+  template <int N>
+  static TL_DEVICE void copy(uint32_t const &src_addr, uint32_t *dst_ptr) {
+    static_assert(N > 0 && (N & (N - 1)) == 0 && N <= 64,
+                  "N must be a power of 2 and lies between 1 ~ 64");
+
+    if constexpr (N == 1) {
+      asm volatile("tcgen05.ld.sync.aligned.16x128b.x1.b32"
+                   "{%0, %1},"
+                   "[%2];\n"
+                   : "=r"(dst_ptr[0]), "=r"(dst_ptr[1])
+                   : "r"(src_addr));
+    } else if constexpr (N == 2) {
+      asm volatile("tcgen05.ld.sync.aligned.16x128b.x2.b32"
+                   "{%0, %1, %2, %3},"
+                   "[%4];\n"
+                   : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+                     "=r"(dst_ptr[3])
+                   : "r"(src_addr));
+    } else if constexpr (N == 4) {
+      asm volatile("tcgen05.ld.sync.aligned.16x128b.x4.b32"
+                   "{%0, %1, %2, %3, %4, %5, %6, %7},"
+                   "[%8];\n"
+                   : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+                     "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+                     "=r"(dst_ptr[6]), "=r"(dst_ptr[7])
+                   : "r"(src_addr));
+    } else if constexpr (N == 8) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.16x128b.x8.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, "
+          "%14, %15},"
+          "[%16];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15])
+          : "r"(src_addr));
+    } else if constexpr (N == 16) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.16x128b.x16.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, "
+          "%14, %15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, "
+          "%26, %27, %28, %29, %30, %31},"
+          "[%32];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15]), "=r"(dst_ptr[16]), "=r"(dst_ptr[17]),
+            "=r"(dst_ptr[18]), "=r"(dst_ptr[19]), "=r"(dst_ptr[20]),
+            "=r"(dst_ptr[21]), "=r"(dst_ptr[22]), "=r"(dst_ptr[23]),
+            "=r"(dst_ptr[24]), "=r"(dst_ptr[25]), "=r"(dst_ptr[26]),
+            "=r"(dst_ptr[27]), "=r"(dst_ptr[28]), "=r"(dst_ptr[29]),
+            "=r"(dst_ptr[30]), "=r"(dst_ptr[31])
+          : "r"(src_addr));
+    } else if constexpr (N == 32) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.16x128b.x32.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, "
+          "%15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, %26, %27, "
+          "%28, "
+          "%29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, "
+          "%42, "
+          "%43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, "
+          "%56, "
+          "%57, %58, %59, %60, %61, %62, %63},"
+          "[%64];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15]), "=r"(dst_ptr[16]), "=r"(dst_ptr[17]),
+            "=r"(dst_ptr[18]), "=r"(dst_ptr[19]), "=r"(dst_ptr[20]),
+            "=r"(dst_ptr[21]), "=r"(dst_ptr[22]), "=r"(dst_ptr[23]),
+            "=r"(dst_ptr[24]), "=r"(dst_ptr[25]), "=r"(dst_ptr[26]),
+            "=r"(dst_ptr[27]), "=r"(dst_ptr[28]), "=r"(dst_ptr[29]),
+            "=r"(dst_ptr[30]), "=r"(dst_ptr[31]), "=r"(dst_ptr[32]),
+            "=r"(dst_ptr[33]), "=r"(dst_ptr[34]), "=r"(dst_ptr[35]),
+            "=r"(dst_ptr[36]), "=r"(dst_ptr[37]), "=r"(dst_ptr[38]),
+            "=r"(dst_ptr[39]), "=r"(dst_ptr[40]), "=r"(dst_ptr[41]),
+            "=r"(dst_ptr[42]), "=r"(dst_ptr[43]), "=r"(dst_ptr[44]),
+            "=r"(dst_ptr[45]), "=r"(dst_ptr[46]), "=r"(dst_ptr[47]),
+            "=r"(dst_ptr[48]), "=r"(dst_ptr[49]), "=r"(dst_ptr[50]),
+            "=r"(dst_ptr[51]), "=r"(dst_ptr[52]), "=r"(dst_ptr[53]),
+            "=r"(dst_ptr[54]), "=r"(dst_ptr[55]), "=r"(dst_ptr[56]),
+            "=r"(dst_ptr[57]), "=r"(dst_ptr[58]), "=r"(dst_ptr[59]),
+            "=r"(dst_ptr[60]), "=r"(dst_ptr[61]), "=r"(dst_ptr[62]),
+            "=r"(dst_ptr[63])
+          : "r"(src_addr));
+    } else if constexpr (N == 64) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.16x128b.x64.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, "
+          "%15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, %26, %27, "
+          "%28, "
+          "%29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, "
+          "%42, "
+          "%43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, "
+          "%56, "
+          "%57, %58, %59, %60, %61, %62, %63, %64, %65, %66, %67, %68, %69, "
+          "%70, "
+          "%71, %72, %73, %74, %75, %76, %77, %78, %79, %80, %81, %82, %83, "
+          "%84, "
+          "%85, %86, %87, %88, %89, %90, %91, %92, %93, %94, %95, %96, %97, "
+          "%98, "
+          "%99, %100, %101, %102, %103, %104, %105, %106, %107, %108, %109, "
+          "%110, %111, %112, %113, %114, %115, %116, %117, %118, %119, %120, "
+          "%121, %122, %123, %124, %125, %126, %127},"
+          "[%128];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15]), "=r"(dst_ptr[16]), "=r"(dst_ptr[17]),
+            "=r"(dst_ptr[18]), "=r"(dst_ptr[19]), "=r"(dst_ptr[20]),
+            "=r"(dst_ptr[21]), "=r"(dst_ptr[22]), "=r"(dst_ptr[23]),
+            "=r"(dst_ptr[24]), "=r"(dst_ptr[25]), "=r"(dst_ptr[26]),
+            "=r"(dst_ptr[27]), "=r"(dst_ptr[28]), "=r"(dst_ptr[29]),
+            "=r"(dst_ptr[30]), "=r"(dst_ptr[31]), "=r"(dst_ptr[32]),
+            "=r"(dst_ptr[33]), "=r"(dst_ptr[34]), "=r"(dst_ptr[35]),
+            "=r"(dst_ptr[36]), "=r"(dst_ptr[37]), "=r"(dst_ptr[38]),
+            "=r"(dst_ptr[39]), "=r"(dst_ptr[40]), "=r"(dst_ptr[41]),
+            "=r"(dst_ptr[42]), "=r"(dst_ptr[43]), "=r"(dst_ptr[44]),
+            "=r"(dst_ptr[45]), "=r"(dst_ptr[46]), "=r"(dst_ptr[47]),
+            "=r"(dst_ptr[48]), "=r"(dst_ptr[49]), "=r"(dst_ptr[50]),
+            "=r"(dst_ptr[51]), "=r"(dst_ptr[52]), "=r"(dst_ptr[53]),
+            "=r"(dst_ptr[54]), "=r"(dst_ptr[55]), "=r"(dst_ptr[56]),
+            "=r"(dst_ptr[57]), "=r"(dst_ptr[58]), "=r"(dst_ptr[59]),
+            "=r"(dst_ptr[60]), "=r"(dst_ptr[61]), "=r"(dst_ptr[62]),
+            "=r"(dst_ptr[63]), "=r"(dst_ptr[64]), "=r"(dst_ptr[65]),
+            "=r"(dst_ptr[66]), "=r"(dst_ptr[67]), "=r"(dst_ptr[68]),
+            "=r"(dst_ptr[69]), "=r"(dst_ptr[70]), "=r"(dst_ptr[71]),
+            "=r"(dst_ptr[72]), "=r"(dst_ptr[73]), "=r"(dst_ptr[74]),
+            "=r"(dst_ptr[75]), "=r"(dst_ptr[76]), "=r"(dst_ptr[77]),
+            "=r"(dst_ptr[78]), "=r"(dst_ptr[79]), "=r"(dst_ptr[80]),
+            "=r"(dst_ptr[81]), "=r"(dst_ptr[82]), "=r"(dst_ptr[83]),
+            "=r"(dst_ptr[84]), "=r"(dst_ptr[85]), "=r"(dst_ptr[86]),
+            "=r"(dst_ptr[87]), "=r"(dst_ptr[88]), "=r"(dst_ptr[89]),
+            "=r"(dst_ptr[90]), "=r"(dst_ptr[91]), "=r"(dst_ptr[92]),
+            "=r"(dst_ptr[93]), "=r"(dst_ptr[94]), "=r"(dst_ptr[95]),
+            "=r"(dst_ptr[96]), "=r"(dst_ptr[97]), "=r"(dst_ptr[98]),
+            "=r"(dst_ptr[99]), "=r"(dst_ptr[100]), "=r"(dst_ptr[101]),
+            "=r"(dst_ptr[102]), "=r"(dst_ptr[103]), "=r"(dst_ptr[104]),
+            "=r"(dst_ptr[105]), "=r"(dst_ptr[106]), "=r"(dst_ptr[107]),
+            "=r"(dst_ptr[108]), "=r"(dst_ptr[109]), "=r"(dst_ptr[110]),
+            "=r"(dst_ptr[111]), "=r"(dst_ptr[112]), "=r"(dst_ptr[113]),
+            "=r"(dst_ptr[114]), "=r"(dst_ptr[115]), "=r"(dst_ptr[116]),
+            "=r"(dst_ptr[117]), "=r"(dst_ptr[118]), "=r"(dst_ptr[119]),
+            "=r"(dst_ptr[120]), "=r"(dst_ptr[121]), "=r"(dst_ptr[122]),
+            "=r"(dst_ptr[123]), "=r"(dst_ptr[124]), "=r"(dst_ptr[125]),
+            "=r"(dst_ptr[126]), "=r"(dst_ptr[127])
+          : "r"(src_addr));
+    } else {
+      asm volatile("trap");
+    }
+  }
+};
+
+// 16 data path lanes, 256-bit pattern, repeated N times
+class tmem_ld_16dp256bNx {
+public:
+  template <int N>
+  static TL_DEVICE void copy(uint32_t const &src_addr, uint32_t *dst_ptr) {
+    static_assert(N > 0 && (N & (N - 1)) == 0 && N <= 32,
+                  "N must be a power of 2 and lies between 1 ~ 32");
+
+    if constexpr (N == 1) {
+      asm volatile("tcgen05.ld.sync.aligned.16x256b.x1.b32"
+                   "{%0, %1, %2, %3},"
+                   "[%4];\n"
+                   : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+                     "=r"(dst_ptr[3])
+                   : "r"(src_addr));
+    } else if constexpr (N == 2) {
+      asm volatile("tcgen05.ld.sync.aligned.16x256b.x2.b32"
+                   "{%0, %1, %2, %3, %4, %5, %6, %7},"
+                   "[%8];\n"
+                   : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+                     "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+                     "=r"(dst_ptr[6]), "=r"(dst_ptr[7])
+                   : "r"(src_addr));
+    } else if constexpr (N == 4) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.16x256b.x4.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, "
+          "%14, %15},"
+          "[%16];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15])
+          : "r"(src_addr));
+    } else if constexpr (N == 8) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.16x256b.x8.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, "
+          "%14, %15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, "
+          "%26, %27, %28, %29, %30, %31},"
+          "[%32];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15]), "=r"(dst_ptr[16]), "=r"(dst_ptr[17]),
+            "=r"(dst_ptr[18]), "=r"(dst_ptr[19]), "=r"(dst_ptr[20]),
+            "=r"(dst_ptr[21]), "=r"(dst_ptr[22]), "=r"(dst_ptr[23]),
+            "=r"(dst_ptr[24]), "=r"(dst_ptr[25]), "=r"(dst_ptr[26]),
+            "=r"(dst_ptr[27]), "=r"(dst_ptr[28]), "=r"(dst_ptr[29]),
+            "=r"(dst_ptr[30]), "=r"(dst_ptr[31])
+          : "r"(src_addr));
+    } else if constexpr (N == 16) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.16x256b.x16.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, "
+          "%15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, %26, %27, "
+          "%28, "
+          "%29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, "
+          "%42, "
+          "%43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, "
+          "%56, "
+          "%57, %58, %59, %60, %61, %62, %63},"
+          "[%64];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15]), "=r"(dst_ptr[16]), "=r"(dst_ptr[17]),
+            "=r"(dst_ptr[18]), "=r"(dst_ptr[19]), "=r"(dst_ptr[20]),
+            "=r"(dst_ptr[21]), "=r"(dst_ptr[22]), "=r"(dst_ptr[23]),
+            "=r"(dst_ptr[24]), "=r"(dst_ptr[25]), "=r"(dst_ptr[26]),
+            "=r"(dst_ptr[27]), "=r"(dst_ptr[28]), "=r"(dst_ptr[29]),
+            "=r"(dst_ptr[30]), "=r"(dst_ptr[31]), "=r"(dst_ptr[32]),
+            "=r"(dst_ptr[33]), "=r"(dst_ptr[34]), "=r"(dst_ptr[35]),
+            "=r"(dst_ptr[36]), "=r"(dst_ptr[37]), "=r"(dst_ptr[38]),
+            "=r"(dst_ptr[39]), "=r"(dst_ptr[40]), "=r"(dst_ptr[41]),
+            "=r"(dst_ptr[42]), "=r"(dst_ptr[43]), "=r"(dst_ptr[44]),
+            "=r"(dst_ptr[45]), "=r"(dst_ptr[46]), "=r"(dst_ptr[47]),
+            "=r"(dst_ptr[48]), "=r"(dst_ptr[49]), "=r"(dst_ptr[50]),
+            "=r"(dst_ptr[51]), "=r"(dst_ptr[52]), "=r"(dst_ptr[53]),
+            "=r"(dst_ptr[54]), "=r"(dst_ptr[55]), "=r"(dst_ptr[56]),
+            "=r"(dst_ptr[57]), "=r"(dst_ptr[58]), "=r"(dst_ptr[59]),
+            "=r"(dst_ptr[60]), "=r"(dst_ptr[61]), "=r"(dst_ptr[62]),
+            "=r"(dst_ptr[63])
+          : "r"(src_addr));
+    } else if constexpr (N == 32) {
+      asm volatile(
+          "tcgen05.ld.sync.aligned.16x256b.x32.b32"
+          "{%0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, "
+          "%15, %16, %17, %18, %19, %20, %21, %22, %23, %24, %25, %26, %27, "
+          "%28, "
+          "%29, %30, %31, %32, %33, %34, %35, %36, %37, %38, %39, %40, %41, "
+          "%42, "
+          "%43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, "
+          "%56, "
+          "%57, %58, %59, %60, %61, %62, %63, %64, %65, %66, %67, %68, %69, "
+          "%70, "
+          "%71, %72, %73, %74, %75, %76, %77, %78, %79, %80, %81, %82, %83, "
+          "%84, "
+          "%85, %86, %87, %88, %89, %90, %91, %92, %93, %94, %95, %96, %97, "
+          "%98, "
+          "%99, %100, %101, %102, %103, %104, %105, %106, %107, %108, %109, "
+          "%110, %111, %112, %113, %114, %115, %116, %117, %118, %119, %120, "
+          "%121, %122, %123, %124, %125, %126, %127},"
+          "[%128];\n"
+          : "=r"(dst_ptr[0]), "=r"(dst_ptr[1]), "=r"(dst_ptr[2]),
+            "=r"(dst_ptr[3]), "=r"(dst_ptr[4]), "=r"(dst_ptr[5]),
+            "=r"(dst_ptr[6]), "=r"(dst_ptr[7]), "=r"(dst_ptr[8]),
+            "=r"(dst_ptr[9]), "=r"(dst_ptr[10]), "=r"(dst_ptr[11]),
+            "=r"(dst_ptr[12]), "=r"(dst_ptr[13]), "=r"(dst_ptr[14]),
+            "=r"(dst_ptr[15]), "=r"(dst_ptr[16]), "=r"(dst_ptr[17]),
+            "=r"(dst_ptr[18]), "=r"(dst_ptr[19]), "=r"(dst_ptr[20]),
+            "=r"(dst_ptr[21]), "=r"(dst_ptr[22]), "=r"(dst_ptr[23]),
+            "=r"(dst_ptr[24]), "=r"(dst_ptr[25]), "=r"(dst_ptr[26]),
+            "=r"(dst_ptr[27]), "=r"(dst_ptr[28]), "=r"(dst_ptr[29]),
+            "=r"(dst_ptr[30]), "=r"(dst_ptr[31]), "=r"(dst_ptr[32]),
+            "=r"(dst_ptr[33]), "=r"(dst_ptr[34]), "=r"(dst_ptr[35]),
+            "=r"(dst_ptr[36]), "=r"(dst_ptr[37]), "=r"(dst_ptr[38]),
+            "=r"(dst_ptr[39]), "=r"(dst_ptr[40]), "=r"(dst_ptr[41]),
+            "=r"(dst_ptr[42]), "=r"(dst_ptr[43]), "=r"(dst_ptr[44]),
+            "=r"(dst_ptr[45]), "=r"(dst_ptr[46]), "=r"(dst_ptr[47]),
+            "=r"(dst_ptr[48]), "=r"(dst_ptr[49]), "=r"(dst_ptr[50]),
+            "=r"(dst_ptr[51]), "=r"(dst_ptr[52]), "=r"(dst_ptr[53]),
+            "=r"(dst_ptr[54]), "=r"(dst_ptr[55]), "=r"(dst_ptr[56]),
+            "=r"(dst_ptr[57]), "=r"(dst_ptr[58]), "=r"(dst_ptr[59]),
+            "=r"(dst_ptr[60]), "=r"(dst_ptr[61]), "=r"(dst_ptr[62]),
+            "=r"(dst_ptr[63]), "=r"(dst_ptr[64]), "=r"(dst_ptr[65]),
+            "=r"(dst_ptr[66]), "=r"(dst_ptr[67]), "=r"(dst_ptr[68]),
+            "=r"(dst_ptr[69]), "=r"(dst_ptr[70]), "=r"(dst_ptr[71]),
+            "=r"(dst_ptr[72]), "=r"(dst_ptr[73]), "=r"(dst_ptr[74]),
+            "=r"(dst_ptr[75]), "=r"(dst_ptr[76]), "=r"(dst_ptr[77]),
+            "=r"(dst_ptr[78]), "=r"(dst_ptr[79]), "=r"(dst_ptr[80]),
+            "=r"(dst_ptr[81]), "=r"(dst_ptr[82]), "=r"(dst_ptr[83]),
+            "=r"(dst_ptr[84]), "=r"(dst_ptr[85]), "=r"(dst_ptr[86]),
+            "=r"(dst_ptr[87]), "=r"(dst_ptr[88]), "=r"(dst_ptr[89]),
+            "=r"(dst_ptr[90]), "=r"(dst_ptr[91]), "=r"(dst_ptr[92]),
+            "=r"(dst_ptr[93]), "=r"(dst_ptr[94]), "=r"(dst_ptr[95]),
+            "=r"(dst_ptr[96]), "=r"(dst_ptr[97]), "=r"(dst_ptr[98]),
+            "=r"(dst_ptr[99]), "=r"(dst_ptr[100]), "=r"(dst_ptr[101]),
+            "=r"(dst_ptr[102]), "=r"(dst_ptr[103]), "=r"(dst_ptr[104]),
+            "=r"(dst_ptr[105]), "=r"(dst_ptr[106]), "=r"(dst_ptr[107]),
+            "=r"(dst_ptr[108]), "=r"(dst_ptr[109]), "=r"(dst_ptr[110]),
+            "=r"(dst_ptr[111]), "=r"(dst_ptr[112]), "=r"(dst_ptr[113]),
+            "=r"(dst_ptr[114]), "=r"(dst_ptr[115]), "=r"(dst_ptr[116]),
+            "=r"(dst_ptr[117]), "=r"(dst_ptr[118]), "=r"(dst_ptr[119]),
+            "=r"(dst_ptr[120]), "=r"(dst_ptr[121]), "=r"(dst_ptr[122]),
+            "=r"(dst_ptr[123]), "=r"(dst_ptr[124]), "=r"(dst_ptr[125]),
+            "=r"(dst_ptr[126]), "=r"(dst_ptr[127])
+          : "r"(src_addr));
+    } else {
+      asm volatile("trap");
+    }
+  }
+};
+
+// 32 data path lanes, 64-bit pattern, repeated N times
+// (conducted with 2x16dp64bNx)
+class tmem_ld_32dp64bNx {
+public:
+  template <int N>
+  static TL_DEVICE void copy(uint32_t const &src_addr, uint32_t *dst_ptr) {
+    tmem_ld_16dp64bNx::copy<N>(src_addr, dst_ptr);
+    tmem_ld_16dp64bNx::copy<N>(src_addr + (16 << 16), dst_ptr + N);
+  }
+};
+
+// 32 data path lanes, 128-bit pattern, repeated N times
+class tmem_ld_32dp128bNx {
+public:
+  template <int N>
+  static TL_DEVICE void copy(uint32_t const &src_addr, uint32_t *dst_ptr) {
+    tmem_ld_16dp128bNx::copy<N>(src_addr, dst_ptr);
+    tmem_ld_16dp128bNx::copy<N>(src_addr + (16 << 16), dst_ptr + N * 2);
+  }
+};
+
+// 32 data path lanes, 256-bit pattern, repeated N times
+class tmem_ld_32dp256bNx {
+public:
+  template <int N>
+  static TL_DEVICE void copy(uint32_t const &src_addr, uint32_t *dst_ptr) {
+    tmem_ld_16dp256bNx::copy<N>(src_addr, dst_ptr);
+    tmem_ld_16dp256bNx::copy<N>(src_addr + (16 << 16), dst_ptr + N * 4);
+  }
+};
+
+} // namespace tl
diff --git a/src/tl_templates/cuda/threadblock_swizzle.h b/src/tl_templates/cuda/threadblock_swizzle.h
index 60fa0ad1..00a230c1 100644
--- a/src/tl_templates/cuda/threadblock_swizzle.h
+++ b/src/tl_templates/cuda/threadblock_swizzle.h
@@ -4,7 +4,7 @@
 
 namespace tl {
 
-template <int panel_width> TL_DEVICE dim3 rasterization2DRow() {
+template <int panel_width, int offset = 0> TL_DEVICE dim3 rasterization2DRow() {
   const unsigned int block_idx = blockIdx.x + blockIdx.y * gridDim.x;
   const unsigned int grid_size = gridDim.x * gridDim.y;
   const unsigned int panel_size = panel_width * gridDim.x;
@@ -18,11 +18,13 @@ template <int panel_width> TL_DEVICE dim3 rasterization2DRow() {
   const unsigned int col_idx = (panel_idx & 1)
                                    ? gridDim.x - 1 - panel_offset / stride
                                    : panel_offset / stride;
-  const unsigned int row_idx = panel_offset % stride + panel_idx * panel_width;
+  const unsigned int row_idx =
+      (panel_offset % stride + panel_idx * panel_width + offset) % gridDim.y;
   return {col_idx, row_idx, blockIdx.z};
 }
 
-template <int panel_width> TL_DEVICE dim3 rasterization2DColumn() {
+template <int panel_width, int offset = 0>
+TL_DEVICE dim3 rasterization2DColumn() {
   const unsigned int block_idx = blockIdx.x + blockIdx.y * gridDim.x;
   const unsigned int grid_size = gridDim.x * gridDim.y;
   const unsigned int panel_size = panel_width * gridDim.y;
@@ -36,7 +38,8 @@ template <int panel_width> TL_DEVICE dim3 rasterization2DColumn() {
   const unsigned int row_idx = (panel_idx & 1)
                                    ? gridDim.y - 1 - panel_offset / stride
                                    : panel_offset / stride;
-  const unsigned int col_idx = panel_offset % stride + panel_idx * panel_width;
+  const unsigned int col_idx =
+      (panel_offset % stride + panel_idx * panel_width + offset) % gridDim.x;
   return {col_idx, row_idx, blockIdx.z};
 }
 
