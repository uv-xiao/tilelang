diff --git a/src/op/distributed.cc b/src/op/distributed.cc
new file mode 100644
index 00000000..84a23afa
--- /dev/null
+++ b/src/op/distributed.cc
@@ -0,0 +1,216 @@
+/*!
+ * \file tl/op/distributed.cc
+ * \brief Distributed intrinsics.
+ *
+ */
+
+#include "distributed.h"
+
+#include <tvm/tir/builtin.h>
+#include <tvm/tir/op.h>
+#include <tvm/tir/op_attr_types.h>
+
+#include "../target/cuda.h"
+#include "../target/utils.h"
+
+namespace tvm {
+namespace tl {
+
+#define TIR_DEFINE_TL_BUILTIN(OpName)                                          \
+  const Op &OpName() {                                                         \
+    static const Op &op = Op::Get("tl." #OpName);                              \
+    return op;                                                                 \
+  }                                                                            \
+  TVM_REGISTER_OP("tl." #OpName)                                               \
+      .set_attr<TScriptPrinterName>("TScriptPrinterName", #OpName)
+
+// TODO: check the effect kind and num_inputs
+TIR_DEFINE_TL_BUILTIN(GetPE).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(GetPENum).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(IntPE).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(BarrierAll)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(BarrierAllBlock)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(BarrierAllWarp)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(SyncAll).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(SyncAllBlock)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(SyncAllWarp)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(Quiet).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(Fence).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(GetmemNbiBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(GetmemBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(GetmemNbiWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(GetmemWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(GetmemNbi).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(Getmem).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemNbiBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemNbiWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(Putmem).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemNbi).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemSignal)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemSignalNbi)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemSignalBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemSignalNbiBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemSignalWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemSignalNbiWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(SignalOp).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(SignalWaitUntil)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(Broadcast).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(BroadcastWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(BroadcastBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(BroadcastmemBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(Fcollect).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(FcollectWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(FcollectBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(CpengineCpAsync)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(get_rank).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(get_num_ranks)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(get_remote_base_ptr)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(get_uintptr_t)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+} // namespace tl
+} // namespace tvm
\ No newline at end of file
diff --git a/src/op/distributed.h b/src/op/distributed.h
new file mode 100644
index 00000000..5170cc7e
--- /dev/null
+++ b/src/op/distributed.h
@@ -0,0 +1,234 @@
+/*!
+ * \file tl/op/distributed.h
+ * \brief Distributed intrinsics.
+ *
+ */
+
+#include "operator.h"
+#include <tvm/ir/transform.h>
+
+namespace tvm {
+namespace tl {
+
+/*!
+ * \brief tvm intrinsics for getting the PE id
+ *
+ * int GetPE()
+ *
+ */
+const Op &GetPE();
+
+/*!
+ * \brief tvm intrinsics for getting the total number of PEs
+ */
+const Op &GetPENum();
+
+/*!
+ * \brief tvm intrinsics for getting the PE id
+ *
+ * int IntPE()
+ *
+ */
+const Op &IntPE();
+
+/*!
+ * \brief tvm intrinsics for global barrier synchronization
+ */
+const Op &BarrierAll();
+
+/*!
+ * \brief tvm intrinsics for block-level barrier synchronization
+ */
+const Op &BarrierAllBlock();
+
+/*!
+ * \brief tvm intrinsics for warp-level barrier synchronization
+ */
+const Op &BarrierAllWarp();
+
+/*!
+ * \brief tvm intrinsics for global synchronization
+ */
+const Op &SyncAll();
+
+/*!
+ * \brief tvm intrinsics for block-level synchronization
+ */
+const Op &SyncAllBlock();
+
+/*!
+ * \brief tvm intrinsics for warp-level synchronization
+ */
+const Op &SyncAllWarp();
+
+/*!
+ * \brief tvm intrinsics for quiet operation
+ */
+const Op &Quiet();
+
+/*!
+ * \brief tvm intrinsics for memory fence operation
+ */
+const Op &Fence();
+
+/*!
+ * \brief tvm intrinsics for non-blocking block-level memory get
+ */
+const Op &GetmemNbiBlock();
+
+/*!
+ * \brief tvm intrinsics for blocking block-level memory get
+ */
+const Op &GetmemBlock();
+
+/*!
+ * \brief tvm intrinsics for non-blocking warp-level memory get
+ */
+const Op &GetmemNbiWarp();
+
+/*!
+ * \brief tvm intrinsics for blocking warp-level memory get
+ */
+const Op &GetmemWarp();
+
+/*!
+ * \brief tvm intrinsics for non-blocking memory get
+ */
+const Op &GetmemNbi();
+
+/*!
+ * \brief tvm intrinsics for blocking memory get
+ */
+const Op &Getmem();
+
+/*!
+ * \brief tvm intrinsics for block-level memory put
+ */
+const Op &PutmemBlock();
+
+/*!
+ * \brief tvm intrinsics for non-blocking block-level memory put
+ */
+const Op &PutmemNbiBlock();
+
+/*!
+ * \brief tvm intrinsics for warp-level memory put
+ */
+const Op &PutmemWarp();
+
+/*!
+ * \brief tvm intrinsics for non-blocking warp-level memory put
+ */
+const Op &PutmemNbiWarp();
+
+/*!
+ * \brief tvm intrinsics for memory put
+ */
+const Op &Putmem();
+
+/*!
+ * \brief tvm intrinsics for non-blocking memory put
+ */
+const Op &PutmemNbi();
+
+/*!
+ * \brief tvm intrinsics for signaled memory put
+ */
+const Op &PutmemSignal();
+
+/*!
+ * \brief tvm intrinsics for non-blocking signaled memory put
+ */
+const Op &PutmemSignalNbi();
+
+/*!
+ * \brief tvm intrinsics for block-level signaled memory put
+ */
+const Op &PutmemSignalBlock();
+
+/*!
+ * \brief tvm intrinsics for non-blocking block-level signaled memory put
+ */
+const Op &PutmemSignalNbiBlock();
+
+/*!
+ * \brief tvm intrinsics for warp-level signaled memory put
+ */
+const Op &PutmemSignalWarp();
+
+/*!
+ * \brief tvm intrinsics for non-blocking warp-level signaled memory put
+ */
+const Op &PutmemSignalNbiWarp();
+
+/*!
+ * \brief tvm intrinsics for signal operation
+ */
+const Op &SignalOp();
+
+/*!
+ * \brief tvm intrinsics for waiting on signal
+ */
+const Op &SignalWaitUntil();
+
+/*!
+ * \brief tvm intrinsics for broadcast operation
+ */
+const Op &Broadcast();
+
+/*!
+ * \brief tvm intrinsics for warp-level broadcast
+ */
+const Op &BroadcastWarp();
+
+/*!
+ * \brief tvm intrinsics for block-level broadcast
+ */
+const Op &BroadcastBlock();
+
+/*!
+ * \brief tvm intrinsics for block-level memory broadcast
+ */
+const Op &BroadcastmemBlock();
+
+/*!
+ * \brief tvm intrinsics for collective gather operation
+ */
+const Op &Fcollect();
+
+/*!
+ * \brief tvm intrinsics for warp-level collective gather
+ */
+const Op &FcollectWarp();
+
+/*!
+ * \brief tvm intrinsics for block-level collective gather
+ */
+const Op &FcollectBlock();
+
+/*!
+ * \brief tvm intrinsics for collective gather operation
+ */
+const Op &CpengineCpAsync();
+
+/*!
+ * \brief tvm intrinsics for getting the rank of the current process
+ */
+const Op &get_rank();
+
+/*!
+ * \brief tvm intrinsics for getting the number of processes
+ */
+const Op &get_num_ranks();
+
+/*!
+ * \brief tvm intrinsics for getting the remote base pointer
+ */
+const Op &get_remote_base_ptr();
+
+/*!
+ * \brief tvm intrinsics for getting the uintptr_t of a pointer
+ */
+const Op &get_uintptr_t();
+} // namespace tl
+} // namespace tvm
diff --git a/src/op/remote_copy.cc b/src/op/remote_copy.cc
new file mode 100644
index 00000000..fba501e4
--- /dev/null
+++ b/src/op/remote_copy.cc
@@ -0,0 +1,407 @@
+/*!
+ * \file tl/op/remote_copy.cc
+ * \brief Remote copy operators.
+ *
+ */
+
+#include "remote_copy.h"
+
+#include <tvm/tir/builtin.h>
+#include <tvm/tir/op.h>
+#include <tvm/tir/op_attr_types.h>
+
+#include <sstream>
+
+#include "../target/cuda.h"
+#include "../target/utils.h"
+#include "builtin.h"
+#include "distributed.h"
+#include "parallel.h"
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+PrimExpr PutOpNode::get_offset(const BufferLoadNode *load) const {
+  PrimExpr offset = 0;
+  PrimExpr stride = 1;
+  auto buffer_shape = load->buffer->shape;
+  for (int i = load->indices.size() - 1; i >= 0; i--) {
+    offset += load->indices[i] * stride;
+    stride *= buffer_shape[i];
+  }
+  return div(offset * load->dtype.bits(), 8);
+}
+
+PrimExpr PutOpNode::MakeAddress(const Buffer &buffer,
+                                const Array<PrimExpr> &indices) const {
+  return Call(DataType::Handle(), builtin::address_of(),
+              {BufferLoad(buffer, indices)});
+}
+
+PrimExpr PutOpNode::MakeRemappedAddress(const LowerArgs &T,
+                                        const Buffer &buffer,
+                                        const Array<PrimExpr> &indices) const {
+  Buffer remapped = buffer;
+  if (T.buffer_remap.count(buffer)) {
+    remapped = T.buffer_remap[buffer];
+  }
+  return MakeAddress(remapped, indices);
+}
+
+PutOp::PutOp(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<PutOpNode> node = make_object<PutOpNode>();
+  node->src_addr = args[0];
+  node->dst_addr = args[1];
+  ICHECK(node->src_addr.as<CallNode>()) << "src_addr must be a call node";
+  ICHECK(node->src_addr.as<CallNode>()->op.same_as(builtin::address_of()))
+      << "src_addr must be address_of op";
+  ICHECK(node->dst_addr.as<CallNode>()) << "dst_addr must be a call node";
+  ICHECK(node->dst_addr.as<CallNode>()->op.same_as(builtin::address_of()))
+      << "dst_addr must be address_of op";
+
+  const auto *src_load =
+      node->src_addr.as<CallNode>()->args[0].as<BufferLoadNode>();
+  const auto *dst_load =
+      node->dst_addr.as<CallNode>()->args[0].as<BufferLoadNode>();
+  ICHECK(src_load && dst_load) << "address_of must wrap BufferLoad nodes";
+
+  node->src_offset = node->get_offset(src_load);
+  node->dst_offset = node->get_offset(dst_load);
+  node->src_buffer = src_load->buffer;
+  node->dst_buffer = dst_load->buffer;
+  node->src_indices = src_load->indices;
+  node->dst_indices = dst_load->indices;
+
+  node->copy_size = args[2];
+  node->dst_pe = args[3];
+  node->unroll_factor = args[4].as<IntImm>().value()->value;
+  node->scope = args[5].as<StringImm>().value()->value;
+  node->enable_aggressive_vectorize = bool(args[6].as<IntImm>().value()->value);
+  data_ = std::move(node);
+  (void)vmap;
+}
+
+bool PutOpNode::is_distributed() const {
+  return !(dst_pe->IsInstance<IntImmNode>() &&
+           dst_pe.as<IntImmNode>()->value == -1);
+}
+
+Stmt PutOpNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  (void)analyzer;
+  Array<PrimExpr> new_args;
+  std::stringstream ss;
+  if (scope == "warp") {
+    ss << "tl::cp_warp<" << copy_size << ", " << unroll_factor << ", "
+       << (enable_aggressive_vectorize ? "true" : "false") << ">";
+  } else if (scope == "block") {
+    ss << "tl::cp_block<" << copy_size << ">";
+  } else {
+    LOG(FATAL) << "Invalid scope: " << scope;
+  }
+
+  new_args.push_back(StringImm(ss.str()));
+  if (is_distributed()) {
+    PrimExpr dst_addr_expr = MakeRemappedAddress(T, dst_buffer, dst_indices);
+    PrimExpr local_rank = Call(DataType::Int(64), tl::get_rank(), {});
+    PrimExpr local_base_ptr =
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {local_rank});
+    PrimExpr offset_to_base =
+        Sub(Call(DataType::Handle(), tl::get_uintptr_t(), {dst_addr_expr}),
+            local_base_ptr);
+    new_args.push_back(
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {dst_pe}) +
+        offset_to_base);
+  } else {
+    new_args.push_back(MakeRemappedAddress(T, dst_buffer, dst_indices));
+  }
+  new_args.push_back(MakeRemappedAddress(T, src_buffer, src_indices));
+  auto put = Call(DataType::Handle(), builtin::call_extern(), new_args);
+  return Evaluate(put);
+}
+
+LayoutMap PutOpNode::InferLayout(const LayoutInferArgs &T,
+                                 InferLevel level) const {
+  (void)T;
+  (void)level;
+  return {};
+}
+
+TileOperator PutOpNode::Clone() const {
+  auto node = make_object<PutOpNode>(*this);
+  return PutOp(node);
+}
+
+PrimExpr GetOpNode::get_offset(const BufferLoadNode *load) const {
+  PrimExpr offset = 0;
+  PrimExpr stride = 1;
+  auto buffer_shape = load->buffer->shape;
+  for (int i = load->indices.size() - 1; i >= 0; i--) {
+    offset += load->indices[i] * stride;
+    stride *= buffer_shape[i];
+  }
+  return div(offset * load->dtype.bits(), 8);
+}
+
+PrimExpr GetOpNode::MakeAddress(const Buffer &buffer,
+                                const Array<PrimExpr> &indices) const {
+  return Call(DataType::Handle(), builtin::address_of(),
+              {BufferLoad(buffer, indices)});
+}
+
+PrimExpr GetOpNode::MakeRemappedAddress(const LowerArgs &T,
+                                        const Buffer &buffer,
+                                        const Array<PrimExpr> &indices) const {
+  Buffer remapped = buffer;
+  if (T.buffer_remap.count(buffer)) {
+    remapped = T.buffer_remap[buffer];
+  }
+  return MakeAddress(remapped, indices);
+}
+
+GetOp::GetOp(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<GetOpNode> node = make_object<GetOpNode>();
+  node->src_addr = args[0];
+  node->dst_addr = args[1];
+  ICHECK(node->src_addr.as<CallNode>()) << "src_addr must be a call node";
+  ICHECK(node->src_addr.as<CallNode>()->op.same_as(builtin::address_of()))
+      << "src_addr must be address_of op";
+  ICHECK(node->dst_addr.as<CallNode>()) << "dst_addr must be a call node";
+  ICHECK(node->dst_addr.as<CallNode>()->op.same_as(builtin::address_of()))
+      << "dst_addr must be address_of op";
+
+  const auto *src_load =
+      node->src_addr.as<CallNode>()->args[0].as<BufferLoadNode>();
+  const auto *dst_load =
+      node->dst_addr.as<CallNode>()->args[0].as<BufferLoadNode>();
+  ICHECK(src_load && dst_load) << "address_of must wrap BufferLoad nodes";
+
+  node->src_offset = node->get_offset(src_load);
+  node->dst_offset = node->get_offset(dst_load);
+  node->src_buffer = src_load->buffer;
+  node->dst_buffer = dst_load->buffer;
+  node->src_indices = src_load->indices;
+  node->dst_indices = dst_load->indices;
+
+  node->copy_size = args[2];
+  node->src_pe = args[3];
+  node->unroll_factor = args[4].as<IntImm>().value()->value;
+  node->scope = args[5].as<StringImm>().value()->value;
+  node->enable_aggressive_vectorize = bool(args[6].as<IntImm>().value()->value);
+  data_ = std::move(node);
+  (void)vmap;
+}
+
+bool GetOpNode::is_distributed() const {
+  return !(src_pe->IsInstance<IntImmNode>() &&
+           src_pe.as<IntImmNode>()->value == -1);
+}
+
+Stmt GetOpNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  (void)analyzer;
+  Array<PrimExpr> new_args;
+  std::stringstream ss;
+  if (scope == "warp") {
+    ss << "tl::cp_warp<" << copy_size << ", " << unroll_factor << ", "
+       << (enable_aggressive_vectorize ? "true" : "false") << ">";
+  } else if (scope == "block") {
+    ss << "tl::cp_block<" << copy_size << ">";
+  } else {
+    LOG(FATAL) << "Invalid scope: " << scope;
+  }
+
+  new_args.push_back(StringImm(ss.str()));
+  PrimExpr dst_addr_expr = MakeRemappedAddress(T, dst_buffer, dst_indices);
+  new_args.push_back(dst_addr_expr); // Always dst first in tl_templates
+  if (is_distributed()) {
+    PrimExpr src_addr_expr = MakeRemappedAddress(T, src_buffer, src_indices);
+    PrimExpr local_rank = Call(DataType::Int(64), tl::get_rank(), {});
+    PrimExpr local_base_ptr =
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {local_rank});
+    PrimExpr offset_to_base =
+        Sub(Call(DataType::Handle(), tl::get_uintptr_t(), {src_addr_expr}),
+            local_base_ptr);
+    new_args.push_back(
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {src_pe}) +
+        offset_to_base);
+  } else {
+    new_args.push_back(MakeRemappedAddress(T, src_buffer, src_indices));
+  }
+
+  auto get = Call(DataType::Handle(), builtin::call_extern(), new_args);
+  return Evaluate(get);
+}
+
+LayoutMap GetOpNode::InferLayout(const LayoutInferArgs &T,
+                                 InferLevel level) const {
+  (void)T;
+  (void)level;
+  return {};
+}
+
+TileOperator GetOpNode::Clone() const {
+  auto node = make_object<GetOpNode>(*this);
+  return GetOp(node);
+}
+
+StOp::StOp(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<StOpNode> node = make_object<StOpNode>();
+  node->dst = args[0];
+  ICHECK(node->dst.as<CallNode>()) << "dst must be a call node";
+  ICHECK(node->dst.as<CallNode>()->op.same_as(builtin::address_of()))
+      << "dst must be address_of op";
+
+  node->value = args[1];
+  node->sem = args[2].as<IntImm>().value()->value;
+  node->scope = args[3].as<IntImm>().value()->value;
+  node->na = args[4].as<IntImm>().value()->value;
+  node->dst_pe = args[5];
+  data_ = std::move(node);
+  (void)vmap;
+}
+
+bool StOpNode::is_distributed() const {
+  return !(dst_pe->IsInstance<IntImmNode>() &&
+           dst_pe.as<IntImmNode>()->value == -1);
+}
+
+Stmt StOpNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  (void)analyzer;
+  (void)T;
+  Array<PrimExpr> new_args;
+  std::stringstream ss;
+
+  // Map integers to enum literal strings
+  const char *sem_str[] = {"Semantic::WEAK", "Semantic::VOLATILE",
+                           "Semantic::ACQUIRE", "Semantic::RELEASE",
+                           "Semantic::RELAXED"};
+  const char *scope_str[] = {"Scope::CTA", "Scope::GPU", "Scope::SYS"};
+
+  // Build function name: tl::st<Semantic::X, Scope::Y, bool>
+  ss << "tl::st<" << sem_str[sem] << ", " << scope_str[scope] << ", "
+     << (na ? "true" : "false") << ">";
+
+  new_args.push_back(StringImm(ss.str()));
+  if (is_distributed()) {
+    PrimExpr local_rank = Call(DataType::Int(64), tl::get_rank(), {});
+    PrimExpr local_base_ptr =
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {local_rank});
+    PrimExpr offset_to_base = Sub(
+        Call(DataType::Handle(), tl::get_uintptr_t(), {dst}), local_base_ptr);
+    new_args.push_back(
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {dst_pe}) +
+        offset_to_base);
+  } else {
+    new_args.push_back(dst);
+  }
+  new_args.push_back(value);
+
+  auto st = Call(DataType::Handle(), builtin::call_extern(), new_args);
+  return Evaluate(st);
+}
+
+LayoutMap StOpNode::InferLayout(const LayoutInferArgs &T,
+                                InferLevel level) const {
+  (void)T;
+  (void)level;
+  return {};
+}
+
+TileOperator StOpNode::Clone() const {
+  auto node = make_object<StOpNode>(*this);
+  return StOp(node);
+}
+
+LdOp::LdOp(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<LdOpNode> node = make_object<LdOpNode>();
+  node->src = args[0];
+  ICHECK(node->src.as<CallNode>()) << "src must be a call node";
+  ICHECK(node->src.as<CallNode>()->op.same_as(builtin::address_of()))
+      << "src must be address_of op";
+
+  node->value = args[1];
+  node->sem = args[2].as<IntImm>().value()->value;
+  node->scope = args[3].as<IntImm>().value()->value;
+  node->na = args[4].as<IntImm>().value()->value;
+  node->nc = args[5].as<IntImm>().value()->value;
+  node->src_pe = args[6];
+  data_ = std::move(node);
+  (void)vmap;
+}
+
+bool LdOpNode::is_distributed() const {
+  return !(src_pe->IsInstance<IntImmNode>() &&
+           src_pe.as<IntImmNode>()->value == -1);
+}
+
+Stmt LdOpNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  (void)analyzer;
+  (void)T;
+  Array<PrimExpr> new_args;
+  std::stringstream ss;
+
+  // Map integers to enum literal strings
+  const char *sem_str[] = {"Semantic::WEAK", "Semantic::VOLATILE",
+                           "Semantic::ACQUIRE", "Semantic::RELEASE",
+                           "Semantic::RELAXED"};
+  const char *scope_str[] = {"Scope::CTA", "Scope::GPU", "Scope::SYS"};
+
+  // Build function name: tl::ld<Semantic::X, Scope::Y, bool, bool>
+  ss << "tl::ld<" << sem_str[sem] << ", " << scope_str[scope] << ", "
+     << (nc ? "true" : "false") << ", " << (na ? "true" : "false") << ">";
+
+  new_args.push_back(StringImm(ss.str()));
+  if (is_distributed()) {
+    PrimExpr local_rank = Call(DataType::Int(64), tl::get_rank(), {});
+    PrimExpr local_base_ptr =
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {local_rank});
+    PrimExpr offset_to_base = Sub(
+        Call(DataType::Handle(), tl::get_uintptr_t(), {src}), local_base_ptr);
+    new_args.push_back(
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {src_pe}) +
+        offset_to_base);
+  } else {
+    new_args.push_back(src);
+  }
+  new_args.push_back(value);
+
+  auto ld = Call(DataType::Handle(), builtin::call_extern(), new_args);
+  return Evaluate(ld);
+}
+
+LayoutMap LdOpNode::InferLayout(const LayoutInferArgs &T,
+                                InferLevel level) const {
+  (void)T;
+  (void)level;
+  return {};
+}
+
+TileOperator LdOpNode::Clone() const {
+  auto node = make_object<LdOpNode>(*this);
+  return LdOp(node);
+}
+
+TIR_REGISTER_TL_OP(PutOp, put)
+    .set_num_inputs(7)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_REGISTER_TL_OP(GetOp, get)
+    .set_num_inputs(7)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_REGISTER_TL_OP(StOp, st).set_num_inputs(6).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_REGISTER_TL_OP(LdOp, ld).set_num_inputs(7).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TVM_FFI_STATIC_INIT_BLOCK({ PutOpNode::RegisterReflection(); });
+TVM_FFI_STATIC_INIT_BLOCK({ GetOpNode::RegisterReflection(); });
+TVM_FFI_STATIC_INIT_BLOCK({ StOpNode::RegisterReflection(); });
+TVM_FFI_STATIC_INIT_BLOCK({ LdOpNode::RegisterReflection(); });
+
+} // namespace tl
+} // namespace tvm
diff --git a/src/op/remote_copy.h b/src/op/remote_copy.h
new file mode 100644
index 00000000..3c118f33
--- /dev/null
+++ b/src/op/remote_copy.h
@@ -0,0 +1,325 @@
+/*!
+ * \file tl/op/remote_copy.h
+ * \brief Remote copy operators.
+ *
+ */
+
+#ifndef TVM_TL_OP_BULK_COPY_H_
+#define TVM_TL_OP_BULK_COPY_H_
+
+#include <tvm/target/target.h>
+#include <tvm/tir/stmt_functor.h>
+
+#include "../layout/layout.h"
+#include "operator.h"
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+class PutOpNode : public TileOperatorNode {
+public:
+  PrimExpr src_addr;           ///< Address of the source buffer (address_of)
+  PrimExpr dst_addr;           ///< Address of the destination buffer
+  PrimExpr src_offset;         ///< Byte offset within the source buffer
+  PrimExpr dst_offset;         ///< Byte offset within the destination buffer
+  PrimExpr copy_size;          ///< Number of bytes/elements to copy
+  PrimExpr dst_pe;             ///< Destination processing element (optional)
+  int unroll_factor;           ///< Unroll factor for warp copies
+  Buffer src_buffer;           ///< Source buffer reference
+  Buffer dst_buffer;           ///< Destination buffer reference
+  Array<PrimExpr> src_indices; ///< Source indices used for address computation
+  Array<PrimExpr>
+      dst_indices;   ///< Destination indices used for address computation
+  std::string scope; ///< Scope: {warp, block}
+  bool enable_aggressive_vectorize; ///< Whether to enable aggressive
+                                    ///< vectorization, only effctive for
+                                    ///< warp-scope
+
+  bool is_distributed() const;
+
+  static constexpr const char *_type_key = "tl.PutOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(PutOpNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+  static const Op &Get();
+  TileOperator Clone() const override;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<PutOpNode>()
+        .def_ro("src_addr", &PutOpNode::src_addr)
+        .def_ro("dst_addr", &PutOpNode::dst_addr)
+        .def_ro("copy_size", &PutOpNode::copy_size)
+        .def_ro("dst_pe", &PutOpNode::dst_pe)
+        .def_ro("unroll_factor", &PutOpNode::unroll_factor)
+        .def_ro("src_buffer", &PutOpNode::src_buffer)
+        .def_ro("dst_buffer", &PutOpNode::dst_buffer)
+        .def_ro("src_indices", &PutOpNode::src_indices)
+        .def_ro("dst_indices", &PutOpNode::dst_indices)
+        .def_ro("scope", &PutOpNode::scope);
+  }
+
+  bool SEqualReduce(const PutOpNode *other, SEqualReducer equal) const {
+    return equal(src_addr, other->src_addr) &&
+           equal(dst_addr, other->dst_addr) &&
+           equal(src_offset, other->src_offset) &&
+           equal(dst_offset, other->dst_offset) &&
+           equal(copy_size, other->copy_size) && equal(dst_pe, other->dst_pe) &&
+           equal(unroll_factor, other->unroll_factor) &&
+           equal(src_buffer, other->src_buffer) &&
+           equal(dst_buffer, other->dst_buffer) &&
+           equal(src_indices, other->src_indices) &&
+           equal(dst_indices, other->dst_indices) && scope == other->scope;
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(src_addr);
+    hash_reduce(dst_addr);
+    hash_reduce(src_offset);
+    hash_reduce(dst_offset);
+    hash_reduce(copy_size);
+    hash_reduce(dst_pe);
+    hash_reduce(unroll_factor);
+    hash_reduce(src_buffer);
+    hash_reduce(dst_buffer);
+    hash_reduce(src_indices);
+    hash_reduce(dst_indices);
+    hash_reduce(scope);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  PrimExpr get_offset(const BufferLoadNode *load) const;
+
+private:
+  PrimExpr MakeAddress(const Buffer &buffer,
+                       const Array<PrimExpr> &indices) const;
+  PrimExpr MakeRemappedAddress(const LowerArgs &T, const Buffer &buffer,
+                               const Array<PrimExpr> &indices) const;
+};
+
+class PutOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(PutOp, TileOperator, PutOpNode);
+  TVM_DLL PutOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+class GetOpNode : public TileOperatorNode {
+public:
+  PrimExpr src_addr;           ///< Remote source buffer address
+  PrimExpr dst_addr;           ///< Local destination buffer address
+  PrimExpr src_offset;         ///< Byte offset within the source buffer
+  PrimExpr dst_offset;         ///< Byte offset within the destination buffer
+  PrimExpr copy_size;          ///< Number of bytes/elements to copy
+  PrimExpr src_pe;             ///< Source processing element (optional)
+  int unroll_factor;           ///< Unroll factor for warp copies
+  Buffer src_buffer;           ///< Source buffer reference
+  Buffer dst_buffer;           ///< Destination buffer reference
+  Array<PrimExpr> src_indices; ///< Source indices used for address computation
+  Array<PrimExpr>
+      dst_indices;   ///< Destination indices used for address computation
+  std::string scope; ///< Scope: {warp, block}
+  bool enable_aggressive_vectorize; ///< Whether to enable aggressive
+                                    ///< vectorization, only effctive for
+                                    ///< warp-scope
+
+  bool is_distributed() const;
+
+  static constexpr const char *_type_key = "tl.GetOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(GetOpNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+  static const Op &Get();
+  TileOperator Clone() const override;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<GetOpNode>()
+        .def_ro("src_addr", &GetOpNode::src_addr)
+        .def_ro("dst_addr", &GetOpNode::dst_addr)
+        .def_ro("copy_size", &GetOpNode::copy_size)
+        .def_ro("src_pe", &GetOpNode::src_pe)
+        .def_ro("unroll_factor", &GetOpNode::unroll_factor)
+        .def_ro("src_buffer", &GetOpNode::src_buffer)
+        .def_ro("dst_buffer", &GetOpNode::dst_buffer)
+        .def_ro("src_indices", &GetOpNode::src_indices)
+        .def_ro("dst_indices", &GetOpNode::dst_indices)
+        .def_ro("scope", &GetOpNode::scope);
+  }
+
+  bool SEqualReduce(const GetOpNode *other, SEqualReducer equal) const {
+    return equal(src_addr, other->src_addr) &&
+           equal(dst_addr, other->dst_addr) &&
+           equal(src_offset, other->src_offset) &&
+           equal(dst_offset, other->dst_offset) &&
+           equal(copy_size, other->copy_size) && equal(src_pe, other->src_pe) &&
+           equal(unroll_factor, other->unroll_factor) &&
+           equal(src_buffer, other->src_buffer) &&
+           equal(dst_buffer, other->dst_buffer) &&
+           equal(src_indices, other->src_indices) &&
+           equal(dst_indices, other->dst_indices) && scope == other->scope;
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(src_addr);
+    hash_reduce(dst_addr);
+    hash_reduce(src_offset);
+    hash_reduce(dst_offset);
+    hash_reduce(copy_size);
+    hash_reduce(src_pe);
+    hash_reduce(unroll_factor);
+    hash_reduce(src_buffer);
+    hash_reduce(dst_buffer);
+    hash_reduce(src_indices);
+    hash_reduce(dst_indices);
+    hash_reduce(scope);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  PrimExpr get_offset(const BufferLoadNode *load) const;
+
+private:
+  PrimExpr MakeAddress(const Buffer &buffer,
+                       const Array<PrimExpr> &indices) const;
+  PrimExpr MakeRemappedAddress(const LowerArgs &T, const Buffer &buffer,
+                               const Array<PrimExpr> &indices) const;
+};
+
+class GetOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(GetOp, TileOperator, GetOpNode);
+  TVM_DLL GetOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+class StOpNode : public TileOperatorNode {
+public:
+  PrimExpr dst;    ///< Destination address
+  PrimExpr value;  ///< Value to store
+  PrimExpr dst_pe; ///< Destination processing element (optional)
+  int scope;
+  int sem;
+  int na;
+
+  bool is_distributed() const;
+
+  static constexpr const char *_type_key = "tl.StOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(StOpNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+  static const Op &Get();
+  TileOperator Clone() const override;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<StOpNode>()
+        .def_ro("dst", &StOpNode::dst)
+        .def_ro("value", &StOpNode::value)
+        .def_ro("dst_pe", &StOpNode::dst_pe)
+        .def_ro("scope", &StOpNode::scope)
+        .def_ro("sem", &StOpNode::sem)
+        .def_ro("na", &StOpNode::na);
+  }
+
+  bool SEqualReduce(const StOpNode *other, SEqualReducer equal) const {
+    return equal(dst, other->dst) && equal(value, other->value) &&
+           equal(dst_pe, other->dst_pe) && scope == other->scope &&
+           sem == other->sem && na == other->na;
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(dst);
+    hash_reduce(value);
+    hash_reduce(dst_pe);
+    hash_reduce(scope);
+    hash_reduce(sem);
+    hash_reduce(na);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+};
+
+class StOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(StOp, TileOperator, StOpNode);
+  TVM_DLL StOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+class LdOpNode : public TileOperatorNode {
+public:
+  PrimExpr src;    ///< Source address
+  PrimExpr value;  ///< Value to store
+  PrimExpr src_pe; ///< Source PE (optional)
+  int scope;
+  int sem;
+  int na;
+  int nc;
+
+  bool is_distributed() const;
+
+  static constexpr const char *_type_key = "tl.LdOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(LdOpNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+  static const Op &Get();
+  TileOperator Clone() const override;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<LdOpNode>()
+        .def_ro("src", &LdOpNode::src)
+        .def_ro("value", &LdOpNode::value)
+        .def_ro("src_pe", &LdOpNode::src_pe)
+        .def_ro("scope", &LdOpNode::scope)
+        .def_ro("sem", &LdOpNode::sem)
+        .def_ro("na", &LdOpNode::na)
+        .def_ro("nc", &LdOpNode::nc);
+  }
+
+  bool SEqualReduce(const LdOpNode *other, SEqualReducer equal) const {
+    return equal(src, other->src) && equal(value, other->value) &&
+           equal(src_pe, other->src_pe) && scope == other->scope &&
+           sem == other->sem && na == other->na && nc == other->nc;
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(src);
+    hash_reduce(value);
+    hash_reduce(src_pe);
+    hash_reduce(scope);
+    hash_reduce(sem);
+    hash_reduce(na);
+    hash_reduce(nc);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+};
+
+class LdOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(LdOp, TileOperator, LdOpNode);
+  TVM_DLL LdOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+} // namespace tl
+} // namespace tvm
+
+#endif //  TVM_TL_OP_BULK_COPY_H_
diff --git a/src/op/sync.cc b/src/op/sync.cc
new file mode 100644
index 00000000..892fc222
--- /dev/null
+++ b/src/op/sync.cc
@@ -0,0 +1,217 @@
+/*!
+ * \file tl/op/sync.cc
+ * \brief Synchronization intrinsics.
+ *
+ */
+
+#include "sync.h"
+
+#include <tvm/tir/builtin.h>
+#include <tvm/tir/op.h>
+#include <tvm/tir/op_attr_types.h>
+
+#include "distributed.h"
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+PrimExpr BarrierBlocksOpNode::get_offset(const BufferLoadNode *load) const {
+  PrimExpr offset = 0;
+  PrimExpr stride = 1;
+  auto buffer_shape = load->buffer->shape;
+  for (int i = load->indices.size() - 1; i >= 0; i--) {
+    offset += load->indices[i] * stride;
+    stride *= buffer_shape[i];
+  }
+  return div(offset * load->dtype.bits(), 8);
+}
+
+#define TIR_DEFINE_TL_BUILTIN(OpName)                                          \
+  const Op &OpName() {                                                         \
+    static const Op &op = Op::Get("tl." #OpName);                              \
+    return op;                                                                 \
+  }                                                                            \
+  TVM_REGISTER_OP("tl." #OpName)                                               \
+      .set_attr<TScriptPrinterName>("TScriptPrinterName", #OpName)
+
+TIR_DEFINE_TL_BUILTIN(init_barrier_gpu)
+    .set_num_inputs(2)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(arrive_barrier_gpu)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(wait_barrier_gpu)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(wait_eq).set_num_inputs(2).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(sync_barrier_gpu)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(sync_grid).set_num_inputs(1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+BarrierBlocksOp::BarrierBlocksOp(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<BarrierBlocksOpNode> node = make_object<BarrierBlocksOpNode>();
+  node->local_bar_addr = args[0];
+  node->need_fence = bool(args[1].as<IntImmNode>()->value);
+  const auto *call = node->local_bar_addr.as<CallNode>();
+  ICHECK(call) << "local_bar_addr must be a call node";
+  ICHECK(call->op.same_as(builtin::address_of()))
+      << "local_bar_addr must be address_of op";
+
+  const auto *load = call->args[0].as<BufferLoadNode>();
+  ICHECK(load) << "address_of argument must be a BufferLoad";
+  node->offset = node->get_offset(load);
+  node->local_bar = load->buffer;
+  node->local_indices = load->indices;
+  data_ = std::move(node);
+  (void)vmap;
+}
+
+Stmt BarrierBlocksOpNode::Lower(const LowerArgs &T,
+                                arith::Analyzer *analyzer) const {
+  (void)analyzer;
+  Array<PrimExpr> new_args;
+  std::stringstream ss;
+  ss << "tl::barrier_blocks";
+  if (!need_fence) {
+    ss << "<false>";
+  }
+  new_args.push_back(StringImm(ss.str()));
+
+  PrimExpr bar_addr = MakeLocalBarAddr(T);
+  PrimExpr rank = Call(DataType::Int(64), tl::get_rank(), {});
+  PrimExpr num_ranks = Call(DataType::Int(64), tl::get_num_ranks(), {});
+  PrimExpr local_base_ptr =
+      Call(DataType::Handle(), tl::get_remote_base_ptr(), {rank});
+  PrimExpr offset_to_base =
+      Sub(Call(DataType::Handle(), tl::get_uintptr_t(), {bar_addr}),
+          local_base_ptr);
+
+  new_args.push_back(offset_to_base);
+  new_args.push_back(rank);
+  new_args.push_back(num_ranks);
+
+  auto barrier_blocks =
+      Call(DataType::Handle(), builtin::call_extern(), new_args);
+  return Evaluate(barrier_blocks);
+}
+
+LayoutMap BarrierBlocksOpNode::InferLayout(const LayoutInferArgs &T,
+                                           InferLevel level) const {
+  (void)T;
+  (void)level;
+  return {};
+}
+
+TileOperator BarrierBlocksOpNode::Clone() const {
+  auto node = make_object<BarrierBlocksOpNode>(*this);
+  return BarrierBlocksOp(node);
+}
+
+PrimExpr BarrierBlocksOpNode::MakeLocalBarAddr(const LowerArgs &T) const {
+  const auto *call = local_bar_addr.as<CallNode>();
+  ICHECK(call && call->op.same_as(builtin::address_of()))
+      << "local_bar_addr must remain an address_of call";
+  const auto *load = call->args[0].as<BufferLoadNode>();
+  ICHECK(load) << "address_of must wrap a BufferLoad";
+  Buffer buffer = load->buffer;
+  if (T.buffer_remap.count(buffer)) {
+    buffer = T.buffer_remap[buffer];
+  }
+  return Call(DataType::Handle(), builtin::address_of(),
+              {BufferLoad(buffer, local_indices)});
+}
+
+WaitOp::WaitOp(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<WaitOpNode> node = make_object<WaitOpNode>();
+  node->relation = args[0].as<IntImmNode>()->value;
+  node->addr = args[1];
+  node->expected = args[2];
+  node->peer = args[3];
+  data_ = std::move(node);
+  (void)vmap;
+}
+
+bool WaitOpNode::is_distributed() const {
+  return !(peer->IsInstance<IntImmNode>() &&
+           peer.as<IntImmNode>()->value == -1);
+}
+
+Stmt WaitOpNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  (void)analyzer;
+  (void)T;
+  Array<PrimExpr> new_args;
+  std::stringstream ss;
+
+  // Map relation as int to literal_strings
+  const char *relation_str[] = {"eq", "ne", "ge", "le", "gt", "lt"};
+  ss << "tl::wait_" << relation_str[relation];
+
+  new_args.push_back(StringImm(ss.str()));
+  if (is_distributed()) {
+    PrimExpr local_rank = Call(DataType::Int(64), tl::get_rank(), {});
+    PrimExpr local_base_ptr =
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {local_rank});
+    PrimExpr offset_to_base = Sub(
+        Call(DataType::Handle(), tl::get_uintptr_t(), {addr}), local_base_ptr);
+    new_args.push_back(
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {peer}) +
+        offset_to_base);
+  } else {
+    new_args.push_back(addr);
+  }
+  new_args.push_back(expected);
+
+  auto wait = Call(DataType::Handle(), builtin::call_extern(), new_args);
+  return Evaluate(wait);
+}
+
+LayoutMap WaitOpNode::InferLayout(const LayoutInferArgs &T,
+                                  InferLevel level) const {
+  (void)T;
+  (void)level;
+  return {};
+}
+
+TileOperator WaitOpNode::Clone() const {
+  auto node = make_object<WaitOpNode>(*this);
+  return WaitOp(node);
+}
+
+TIR_REGISTER_TL_OP(BarrierBlocksOp, barrier_blocks)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_REGISTER_TL_OP(WaitOp, wait)
+    .set_num_inputs(4)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(fence_cta).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(fence_gpu).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(fence_sys).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TVM_FFI_STATIC_INIT_BLOCK({ BarrierBlocksOpNode::RegisterReflection(); });
+TVM_FFI_STATIC_INIT_BLOCK({ WaitOpNode::RegisterReflection(); });
+
+} // namespace tl
+} // namespace tvm
diff --git a/src/op/sync.h b/src/op/sync.h
new file mode 100644
index 00000000..16487877
--- /dev/null
+++ b/src/op/sync.h
@@ -0,0 +1,214 @@
+/*!
+ * \file tl/op/sync.h
+ * \brief Synchronization intrinsics.
+ *
+ */
+
+#ifndef TVM_TL_OP_SYNC_H_
+#define TVM_TL_OP_SYNC_H_
+
+#include <tvm/target/target.h>
+#include <tvm/tir/stmt_functor.h>
+
+#include "operator.h"
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+/*!
+ * \brief Initialize a barrier for GPU-level synchronization
+ *
+ * void init_barrier_gpu(barrier, expected)
+ */
+TVM_DLL const Op &init_barrier_gpu();
+
+/*!
+ * \brief Arrive at a barrier for GPU-level synchronization
+ *
+ * void arrive_barrier_gpu(barrier)
+ */
+TVM_DLL const Op &arrive_barrier_gpu();
+
+/*!
+ * \brief Wait at a barrier for GPU-level synchronization
+ *
+ * void wait_barrier_gpu(barrier)
+ */
+TVM_DLL const Op &wait_barrier_gpu();
+
+/*!
+ * \brief Wait until *addr == expected* for GPU-level synchronization
+ * void wait_eq(addr, expected)
+ */
+
+TVM_DLL const Op &wait_eq();
+
+/*!
+ * \brief TileOperatorNode for wait operation.
+ *
+ * WaitOpNode represents a wait primitive,
+ * which waits until a condition on a memory address is met.
+ */
+class WaitOpNode : public TileOperatorNode {
+public:
+  PrimExpr addr;     ///< The address to watch.
+  PrimExpr expected; ///< The expected value to compare against.
+  PrimExpr peer;     ///< The peer to compare against.
+  int relation;      ///< The relation to compare against.
+
+  bool is_distributed() const;
+
+  static constexpr const char *_type_key = "tl.WaitOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(WaitOpNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+  static const Op &Get();
+  TileOperator Clone() const override;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<WaitOpNode>()
+        .def_ro("addr", &WaitOpNode::addr)
+        .def_ro("expected", &WaitOpNode::expected)
+        .def_ro("peer", &WaitOpNode::peer)
+        .def_ro("relation", &WaitOpNode::relation);
+  }
+
+  bool SEqualReduce(const WaitOpNode *other, SEqualReducer equal) const {
+    return equal(addr, other->addr) && equal(expected, other->expected) &&
+           equal(peer, other->peer) && equal(relation, other->relation);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(addr);
+    hash_reduce(expected);
+    hash_reduce(peer);
+    hash_reduce(relation);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+};
+
+/*!
+ * \brief Wrapper for the WaitOp operator.
+ */
+class WaitOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(WaitOp, TileOperator, WaitOpNode);
+  TVM_DLL WaitOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+/*!
+ * \brief Synchronize at a barrier for GPU-level synchronization
+ *
+ * void sync_barrier_gpu(barrier)
+ */
+TVM_DLL const Op &sync_barrier_gpu();
+
+/*!
+ * \brief Synchronize at a barrier for GPU-level synchronization in cooperative
+ * group style
+ *
+ * void sync_grid(barrier)
+ */
+TVM_DLL const Op &sync_grid();
+
+/*!
+ * \brief Synchronize all blocks at a system-level barrier
+ *
+ * void barrier_blocks(barrier, rank, num_ranks)
+ *
+ */
+class BarrierBlocksOpNode : public TileOperatorNode {
+public:
+  PrimExpr local_bar_addr;       ///< Address expression for the local barrier
+  PrimExpr offset;               ///< Byte offset within the barrier buffer
+  Buffer local_bar;              ///< Local barrier buffer reference
+  Array<PrimExpr> local_indices; ///< Indices used to access the barrier buffer
+  bool need_fence;               ///< Whether need sys-level fence
+
+  static constexpr const char *_type_key = "tl.BarrierBlocksOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(BarrierBlocksOpNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+  static const Op &Get();
+  TileOperator Clone() const override;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<BarrierBlocksOpNode>()
+        .def_ro("local_bar_addr", &BarrierBlocksOpNode::local_bar_addr)
+        .def_ro("offset", &BarrierBlocksOpNode::offset)
+        .def_ro("local_bar", &BarrierBlocksOpNode::local_bar)
+        .def_ro("local_indices", &BarrierBlocksOpNode::local_indices);
+  }
+
+  bool SEqualReduce(const BarrierBlocksOpNode *other,
+                    SEqualReducer equal) const {
+    return equal(local_bar_addr, other->local_bar_addr) &&
+           equal(offset, other->offset) && equal(local_bar, other->local_bar) &&
+           equal(local_indices, other->local_indices);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(local_bar_addr);
+    hash_reduce(offset);
+    hash_reduce(local_bar);
+    hash_reduce(local_indices);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  PrimExpr get_offset(const BufferLoadNode *load) const;
+
+private:
+  PrimExpr MakeLocalBarAddr(const LowerArgs &T) const;
+};
+
+/*!
+ * \brief Wrapper for the BarrierBlocks operator
+ */
+class BarrierBlocksOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(BarrierBlocksOp, TileOperator,
+                                BarrierBlocksOpNode);
+  TVM_DLL BarrierBlocksOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+/*!
+ * \brief Create a memory fence at the block level (visible to all threads in
+ * the current block)
+ *
+ * void fence_cta()
+ */
+TVM_DLL const Op &fence_cta();
+
+/*!
+ * \brief Synchronize all threads at the GPU level (visible to all blocks on the
+ * current device)
+ *
+ * void fence_gpu()
+ */
+TVM_DLL const Op &fence_gpu();
+
+/*!
+ * \brief Synchronize all threads at the system level (visible in a node)
+ *
+ * void fence_sys()
+ */
+TVM_DLL const Op &fence_sys();
+
+} // namespace tl
+} // namespace tvm
+
+#endif // TVM_TL_OP_SYNC_H_
diff --git a/src/tl_templates/cuda/distributed.h b/src/tl_templates/cuda/distributed.h
new file mode 100644
index 00000000..7eca3a70
--- /dev/null
+++ b/src/tl_templates/cuda/distributed.h
@@ -0,0 +1,21 @@
+#pragma once
+
+#include "common.h"
+
+namespace tl {
+
+extern "C" extern __device__ uint64_t meta_data[1024];
+
+TL_DEVICE uint64_t get_rank() { return meta_data[0]; }
+
+TL_DEVICE uint64_t get_num_ranks() { return meta_data[1]; }
+
+TL_DEVICE uint64_t get_remote_base_ptr(uint64_t rank) {
+  return meta_data[2 + rank];
+}
+
+template <typename dtype_t> TL_DEVICE uint64_t get_uintptr_t(dtype_t *ptr) {
+  return reinterpret_cast<uint64_t>(ptr);
+}
+
+} // namespace tl
diff --git a/src/tl_templates/cuda/ldst.h b/src/tl_templates/cuda/ldst.h
new file mode 100644
index 00000000..c875832e
--- /dev/null
+++ b/src/tl_templates/cuda/ldst.h
@@ -0,0 +1,255 @@
+#pragma once
+
+#include "common.h"
+
+// Memory semantic and scope enums
+enum class Semantic { WEAK, VOLATILE, ACQUIRE, RELEASE, RELAXED };
+enum class Scope { CTA, GPU, SYS };
+
+#ifndef TL_ALWAYS_FALSE_V_DEFINED
+#define TL_ALWAYS_FALSE_V_DEFINED
+template <class> inline constexpr bool always_false_v = false;
+#endif
+
+// Type trait to detect bfloat16 types
+template <typename T> struct is_bfloat16 : std::false_type {};
+
+#ifdef __CUDA_BF16_TYPES_EXIST__
+template <> struct is_bfloat16<__nv_bfloat16> : std::true_type {};
+#endif
+
+// Detect cutlass bfloat16_t
+namespace cutlass {
+struct bfloat16_t;
+}
+template <> struct is_bfloat16<cutlass::bfloat16_t> : std::true_type {};
+
+template <typename T>
+inline constexpr bool is_bfloat16_v = is_bfloat16<T>::value;
+
+// Fallback template for unsupported configurations
+template <Semantic semantic, Scope scope, bool na> struct StImpl {
+  template <typename T> TL_DEVICE static void execute(T *ptr, T value) {
+    static_assert(always_false_v<T>, "tl::st: unsupported configuration. ");
+  }
+};
+
+template <Semantic semantic, Scope scope, bool nc, bool na> struct LdImpl {
+  template <typename T> TL_DEVICE static void execute(const T *ptr, T &value) {
+    static_assert(always_false_v<T>, "tl::ld: unsupported configuration. ");
+  }
+};
+
+// Macro to define implementation with generic type T
+#define TL_ST_IMPL(SEM, SCOPE, NA, SEM_LIT, SCOPE_LIT, NA_LIT)                 \
+  template <> struct StImpl<Semantic::SEM, Scope::SCOPE, NA> {                 \
+    template <typename T> TL_DEVICE static void execute(T *ptr, T value) {     \
+      if constexpr (sizeof(T) == 2) {                                          \
+        if constexpr (is_bfloat16_v<T>) {                                      \
+          uint16_t value_bits = *reinterpret_cast<uint16_t *>(&value);         \
+          asm volatile("st" SEM_LIT SCOPE_LIT NA_LIT                           \
+                       ".b16 [%0], %1;" ::"l"(ptr),                            \
+                       "h"(value_bits)                                         \
+                       : "memory");                                            \
+        } else {                                                               \
+          asm volatile("st" SEM_LIT SCOPE_LIT NA_LIT                           \
+                       ".b16 [%0], %1;" ::"l"(ptr),                            \
+                       "h"(value)                                              \
+                       : "memory");                                            \
+        }                                                                      \
+      } else if constexpr (sizeof(T) == 4) {                                   \
+        if constexpr (std::is_floating_point_v<T>) {                           \
+          asm volatile("st" SEM_LIT SCOPE_LIT NA_LIT                           \
+                       ".b32 [%0], %1;" ::"l"(ptr),                            \
+                       "f"(value)                                              \
+                       : "memory");                                            \
+        } else {                                                               \
+          asm volatile("st" SEM_LIT SCOPE_LIT NA_LIT                           \
+                       ".b32 [%0], %1;" ::"l"(ptr),                            \
+                       "r"(value)                                              \
+                       : "memory");                                            \
+        }                                                                      \
+      } else if constexpr (sizeof(T) == 8) {                                   \
+        if constexpr (std::is_floating_point_v<T>) {                           \
+          asm volatile("st" SEM_LIT SCOPE_LIT NA_LIT                           \
+                       ".b64 [%0], %1;" ::"l"(ptr),                            \
+                       "d"(value)                                              \
+                       : "memory");                                            \
+        } else {                                                               \
+          asm volatile("st" SEM_LIT SCOPE_LIT NA_LIT                           \
+                       ".b64 [%0], %1;" ::"l"(ptr),                            \
+                       "l"(value)                                              \
+                       : "memory");                                            \
+        }                                                                      \
+      } else if constexpr (sizeof(T) == 16) {                                  \
+        asm volatile("st" SEM_LIT SCOPE_LIT NA_LIT                             \
+                     ".v4.s32 [%0], {%1, %2, %3, %4};" ::"l"(ptr),             \
+                     "r"(value.x), "r"(value.y), "r"(value.z), "r"(value.w)    \
+                     : "memory");                                              \
+      }                                                                        \
+    }                                                                          \
+  };
+
+// Macro to define implementation of tl::ld with generic type T
+#define TL_LD_IMPL(SEM, SCOPE, NC, NA, SEM_LIT, SCOPE_LIT, NC_LIT, NA_LIT)     \
+  template <> struct LdImpl<Semantic::SEM, Scope::SCOPE, NC, NA> {             \
+    template <typename T>                                                      \
+    TL_DEVICE static void execute(const T *ptr, T &value) {                    \
+      if constexpr (sizeof(T) == 2) {                                          \
+        if constexpr (is_bfloat16_v<T>) {                                      \
+          uint16_t value_bits;                                                 \
+          asm volatile("ld" SEM_LIT SCOPE_LIT NC_LIT NA_LIT ".b16 %0, [%1];"   \
+                       : "=h"(value_bits)                                      \
+                       : "l"(ptr)                                              \
+                       : "memory");                                            \
+          value = *reinterpret_cast<T *>(&value_bits);                         \
+        } else {                                                               \
+          asm volatile("ld" SEM_LIT SCOPE_LIT NC_LIT NA_LIT ".b16 %0, [%1];"   \
+                       : "=h"(value)                                           \
+                       : "l"(ptr)                                              \
+                       : "memory");                                            \
+        }                                                                      \
+      } else if constexpr (sizeof(T) == 4) {                                   \
+        if constexpr (std::is_floating_point_v<T>) {                           \
+          asm volatile("ld" SEM_LIT SCOPE_LIT NC_LIT NA_LIT ".b32 %0, [%1];"   \
+                       : "=f"(value)                                           \
+                       : "l"(ptr)                                              \
+                       : "memory");                                            \
+        } else {                                                               \
+          asm volatile("ld" SEM_LIT SCOPE_LIT NC_LIT NA_LIT ".b32 %0, [%1];"   \
+                       : "=r"(value)                                           \
+                       : "l"(ptr)                                              \
+                       : "memory");                                            \
+        }                                                                      \
+      } else if constexpr (sizeof(T) == 8) {                                   \
+        if constexpr (std::is_floating_point_v<T>) {                           \
+          asm volatile("ld" SEM_LIT SCOPE_LIT NC_LIT NA_LIT ".b64 %0, [%1];"   \
+                       : "=d"(value)                                           \
+                       : "l"(ptr)                                              \
+                       : "memory");                                            \
+        } else {                                                               \
+          asm volatile("ld" SEM_LIT SCOPE_LIT NC_LIT NA_LIT ".b64 %0, [%1];"   \
+                       : "=l"(value)                                           \
+                       : "l"(ptr)                                              \
+                       : "memory");                                            \
+        }                                                                      \
+      } else if constexpr (sizeof(T) == 16) {                                  \
+        asm volatile("ld" SEM_LIT SCOPE_LIT NC_LIT NA_LIT                      \
+                     ".v4.s32 {%0, %1, %2, %3}, [%4];"                         \
+                     : "=r"(value.x), "=r"(value.y), "=r"(value.z),            \
+                       "=r"(value.w)                                           \
+                     : "l"(ptr)                                                \
+                     : "memory");                                              \
+      }                                                                        \
+    }                                                                          \
+  };
+
+// Register all combinations of arguments for tl::st in need here
+// WEAK (always .global)
+TL_ST_IMPL(WEAK, CTA, false, ".weak", ".global", "")
+TL_ST_IMPL(WEAK, GPU, false, ".weak", ".global", "")
+TL_ST_IMPL(WEAK, GPU, true, ".weak", ".global", ".L1::no_allocate")
+TL_ST_IMPL(WEAK, SYS, false, ".weak", ".global", "")
+TL_ST_IMPL(WEAK, SYS, true, ".weak", ".global", ".L1::no_allocate")
+
+// VOLATILE (always .global, no na)
+TL_ST_IMPL(VOLATILE, CTA, false, ".volatile", ".global", "")
+TL_ST_IMPL(VOLATILE, GPU, false, ".volatile", ".global", "")
+TL_ST_IMPL(VOLATILE, SYS, false, ".volatile", ".global", "")
+
+// RELAXED (scope-aware)
+TL_ST_IMPL(RELAXED, CTA, false, ".relaxed", ".cta", "")
+TL_ST_IMPL(RELAXED, GPU, false, ".relaxed", ".gpu.global", "")
+TL_ST_IMPL(RELAXED, GPU, true, ".relaxed", ".gpu.global", ".L1::no_allocate")
+TL_ST_IMPL(RELAXED, SYS, false, ".relaxed", ".sys.global", "")
+TL_ST_IMPL(RELAXED, SYS, true, ".relaxed", ".sys.global", ".L1::no_allocate")
+
+// RELEASE (scope-aware)
+TL_ST_IMPL(RELEASE, CTA, false, ".release", ".cta", "")
+TL_ST_IMPL(RELEASE, GPU, false, ".release", ".gpu.global", "")
+TL_ST_IMPL(RELEASE, GPU, true, ".release", ".gpu.global", ".L1::no_allocate")
+TL_ST_IMPL(RELEASE, SYS, false, ".release", ".sys.global", "")
+TL_ST_IMPL(RELEASE, SYS, true, ".release", ".sys.global", ".L1::no_allocate")
+
+// Register all combinations of arguments for tl::ld in need here
+// nc (must with no scope and semantic)
+TL_LD_IMPL(WEAK, CTA, true, false, "", ".global", ".nc", "")
+TL_LD_IMPL(WEAK, GPU, true, false, "", ".global", ".nc", "")
+TL_LD_IMPL(WEAK, SYS, true, false, "", ".global", ".nc", "")
+TL_LD_IMPL(WEAK, GPU, true, true, "", ".global", ".nc", ".L1::no_allocate")
+TL_LD_IMPL(WEAK, SYS, true, true, "", ".global", ".nc", ".L1::no_allocate")
+
+// WEAK
+TL_LD_IMPL(WEAK, CTA, false, false, ".weak", ".cta", "", "")
+TL_LD_IMPL(WEAK, GPU, false, false, ".weak", ".gpu.global", "", "")
+TL_LD_IMPL(WEAK, SYS, false, false, ".weak", ".sys.global", "", "")
+TL_LD_IMPL(WEAK, GPU, false, true, ".weak", ".gpu.global", "",
+           ".L1::no_allocate")
+TL_LD_IMPL(WEAK, SYS, false, true, ".weak", ".sys.global", "",
+           ".L1::no_allocate")
+
+// VOLATILE (always .global, no na)
+TL_LD_IMPL(VOLATILE, CTA, false, false, ".volatile", ".global", "", "")
+TL_LD_IMPL(VOLATILE, GPU, false, false, ".volatile", ".global", "", "")
+TL_LD_IMPL(VOLATILE, SYS, false, false, ".volatile", ".global", "", "")
+
+// RELAXED (scope-aware)
+TL_LD_IMPL(RELAXED, CTA, false, false, ".relaxed", ".cta", "", "")
+TL_LD_IMPL(RELAXED, GPU, false, false, ".relaxed", ".gpu.global", "", "")
+TL_LD_IMPL(RELAXED, SYS, false, false, ".relaxed", ".sys.global", "", "")
+TL_LD_IMPL(RELAXED, GPU, false, true, ".relaxed", ".gpu.global", "",
+           ".L1::no_allocate")
+TL_LD_IMPL(RELAXED, SYS, false, true, ".relaxed", ".sys.global", "",
+           ".L1::no_allocate")
+
+// ACQUIRE (scope-aware)
+TL_LD_IMPL(ACQUIRE, CTA, false, false, ".acquire", ".cta", "", "")
+TL_LD_IMPL(ACQUIRE, GPU, false, false, ".acquire", ".gpu.global", "", "")
+TL_LD_IMPL(ACQUIRE, SYS, false, false, ".acquire", ".sys.global", "", "")
+TL_LD_IMPL(ACQUIRE, GPU, false, true, ".acquire", ".gpu.global", "",
+           ".L1::no_allocate")
+TL_LD_IMPL(ACQUIRE, SYS, false, true, ".acquire", ".sys.global", "",
+           ".L1::no_allocate")
+
+#undef TL_ST_IMPL
+#undef TL_LD_IMPL
+
+namespace tl {
+
+// Public interface
+template <Semantic semantic, Scope scope, bool na, typename P, typename T>
+TL_DEVICE void st(P ptr, T value) {
+  static_assert(sizeof(T) == 2 || sizeof(T) == 4 || sizeof(T) == 8 ||
+                    sizeof(T) == 16,
+                "tl::st: T must be 2, 4, 8, or 16 bytes");
+  static_assert(std::is_pointer_v<P> || std::is_same_v<P, uint64_t>,
+                "tl::st: P must be a pointer or uint64_t");
+  static_assert(semantic == Semantic::WEAK || semantic == Semantic::RELAXED ||
+                    semantic == Semantic::RELEASE ||
+                    semantic == Semantic::VOLATILE,
+                "tl::st: semantic must be WEAK, VOLATILE, RELAXED, or RELEASE");
+
+  T *ptr_ = reinterpret_cast<T *>(ptr);
+  StImpl<semantic, scope, na>::execute(ptr_, value);
+}
+
+template <Semantic semantic, Scope scope, bool nc, bool na, typename P,
+          typename T>
+TL_DEVICE void ld(const P ptr, T &value) {
+  static_assert(sizeof(T) == 2 || sizeof(T) == 4 || sizeof(T) == 8 ||
+                    sizeof(T) == 16,
+                "tl::ld: T must be 2, 4, 8, or 16 bytes");
+  static_assert(std::is_pointer_v<P> || std::is_same_v<P, uint64_t>,
+                "tl::ld: P must be a pointer or uint64_t");
+  static_assert(semantic == Semantic::WEAK || semantic == Semantic::RELAXED ||
+                    semantic == Semantic::ACQUIRE ||
+                    semantic == Semantic::VOLATILE,
+                "tl::ld: semantic must be WEAK, RELAXED, ACQUIRE, or VOLATILE");
+
+  const T *ptr_ = reinterpret_cast<const T *>(ptr);
+  LdImpl<semantic, scope, nc, na>::execute(ptr_, value);
+}
+
+// todo: support "ld.global.nc.L1::no_allocate.L2::256B"
+
+} // namespace tl
diff --git a/src/tl_templates/cuda/sync.h b/src/tl_templates/cuda/sync.h
new file mode 100644
index 00000000..cad94ee7
--- /dev/null
+++ b/src/tl_templates/cuda/sync.h
@@ -0,0 +1,245 @@
+#pragma once
+
+#include "common.h"
+#include "ldst.h"
+
+#define IS_MASTER_THREAD()                                                     \
+  (threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.z == 0)
+#define IS_MASTER_BLOCK()                                                      \
+  (blockIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0)
+
+#define BARRIER_MAGIC 0x80000000
+
+namespace tl {
+
+// Triggers a GPU trap for debugging
+TL_DEVICE void trap() { asm("trap;\n"); }
+
+// CTA-level memory fence
+TL_DEVICE void memory_fence_cta() {
+  asm volatile("fence.acq_rel.cta;\n" ::: "memory");
+}
+
+// GPU-level memory fence
+TL_DEVICE void memory_fence_gpu() {
+  asm volatile("fence.acq_rel.gpu;\n" ::: "memory");
+}
+
+// System-level memory fence
+TL_DEVICE void memory_fence_sys() {
+  asm volatile("fence.acq_rel.sys;\n" ::: "memory");
+}
+
+// GPU-level load with acquire semantics
+TL_DEVICE uint32_t ld_acquire_gpu_u32(const uint32_t *ptr) {
+  uint32_t ret;
+  asm volatile("ld.acquire.gpu.global.u32 %0, [%1];\n" : "=r"(ret) : "l"(ptr));
+  return ret;
+}
+
+// GPU-level atomic add with release semantics
+TL_DEVICE uint32_t atomic_add_release_gpu_u32(const uint32_t *ptr,
+                                              uint32_t value) {
+  uint32_t ret;
+  asm volatile("atom.add.release.gpu.global.s32 %0, [%1], %2;\n"
+               : "=r"(ret)
+               : "l"(ptr), "r"(value));
+  return ret;
+}
+
+// System-level atomic load with acquire semantics
+TL_DEVICE int atomic_load_acquire_sys_s32(const int *ptr) {
+  int ret;
+  asm volatile("atom.load.acquire.sys.global.s32 %0, [%1];\n"
+               : "=r"(ret)
+               : "l"(ptr));
+  return ret;
+}
+
+TL_DEVICE int ld_volatile_global(const int *ptr) {
+  int ret;
+  asm volatile("ld.volatile.global.s32 %0, [%1];\n" : "=r"(ret) : "l"(ptr));
+  return ret;
+}
+
+TL_DEVICE int ld_acquire(const int *ptr) {
+  int ret = 0;
+  asm volatile("ld.global.acquire.gpu.b32 %0, [%1];\n" : "=r"(ret) : "l"(ptr));
+  return ret;
+}
+
+// Initialize a GPU barrier
+template <const uint32_t kExpected>
+TL_DEVICE void init_barrier_gpu(uint32_t *barrier) {
+  if (IS_MASTER_BLOCK() && IS_MASTER_THREAD()) {
+    *barrier = BARRIER_MAGIC - kExpected;
+  }
+  memory_fence_gpu(); // TODO: Is fence or sync needed here?
+}
+
+// Arrive at a GPU barrier (atomic increment)
+TL_DEVICE void arrive_barrier_gpu(uint32_t *barrier) {
+  memory_fence_gpu();
+  if (IS_MASTER_THREAD()) {
+    atomic_add_release_gpu_u32(barrier, 1);
+  }
+}
+
+// Wait at a GPU barrier until all expected blocks have arrived
+TL_DEVICE void wait_barrier_gpu(uint32_t *barrier) {
+  if (IS_MASTER_THREAD()) {
+    uint32_t arrive = ld_acquire_gpu_u32(barrier);
+    while (!(arrive & BARRIER_MAGIC)) {
+      arrive = ld_acquire_gpu_u32(barrier);
+    }
+  }
+  __syncthreads();
+}
+
+// Synchronize at a GPU barrier (arrive + wait)
+TL_DEVICE void sync_barrier_gpu(uint32_t *barrier) {
+  // memory_fence_gpu();
+  __syncthreads();
+  if (IS_MASTER_THREAD()) {
+    atomic_add_release_gpu_u32(barrier, 1);
+    uint32_t arrive = ld_acquire_gpu_u32(barrier);
+    while (arrive < BARRIER_MAGIC) {
+      arrive = ld_acquire_gpu_u32(barrier);
+    }
+  }
+  __syncthreads();
+}
+
+// cooperative groups version of GPU barrier arrive
+TL_DEVICE unsigned int sync_grids_arrive(uint32_t *barrier) {
+  unsigned int oldArrive = 0;
+
+  __syncthreads();
+
+  if (IS_MASTER_THREAD()) {
+    unsigned int expected = gridDim.x * gridDim.y * gridDim.z;
+    unsigned int nb = 1;
+    if (IS_MASTER_BLOCK()) {
+      nb = 0x80000000 - (expected - 1);
+    }
+    asm volatile("atom.add.release.gpu.u32 %0,[%1],%2;"
+                 : "=r"(oldArrive)
+                 : "l"((unsigned int *)barrier), "r"(nb)
+                 : "memory");
+  }
+
+  return oldArrive;
+}
+
+// cooperative groups version of GPU barrier arrive
+TL_DEVICE void sync_grids_wait(unsigned int oldArrive, uint32_t *barrier) {
+  if (IS_MASTER_THREAD()) {
+    unsigned int current_arrive;
+    do {
+      asm volatile("ld.acquire.gpu.u32 %0,[%1];"
+                   : "=r"(current_arrive)
+                   : "l"((unsigned int *)barrier)
+                   : "memory");
+    } while (!(((oldArrive ^ current_arrive) & 0x80000000) != 0));
+  }
+  __syncthreads();
+}
+
+TL_DEVICE void sync_grid(uint32_t *barrier) {
+  unsigned int token = sync_grids_arrive(barrier);
+  sync_grids_wait(token, barrier);
+}
+
+// Sync blocks at a system-level barrier with an optinal fence
+// TODO(wt): Add timeout handling
+
+template <bool need_fence = true>
+TL_DEVICE void barrier_blocks(int offset, int rank, int num_ranks) {
+// Macro to compute the barrier pointer for a given target rank
+#define BARRIER_PTR(tgt_rank)                                                  \
+  (reinterpret_cast<int32_t *>(get_remote_base_ptr(tgt_rank) + offset))
+#define FINISHED_SUM_TAG (1024)
+
+  if constexpr (need_fence) {
+    memory_fence_sys();
+    __syncthreads();
+  }
+
+  int tid = threadIdx.x;
+  if (tid < num_ranks) {
+    atomicAdd_system(BARRIER_PTR(rank) + tid, FINISHED_SUM_TAG);
+    atomicSub_system(BARRIER_PTR(tid) + rank, FINISHED_SUM_TAG);
+  }
+
+  while (true) {
+    int value =
+        tid < num_ranks ? ld_volatile_global(BARRIER_PTR(rank) + tid) : 0;
+    if (__all_sync(0xffffffff, value <= 0)) {
+      break;
+    }
+  }
+  __syncthreads();
+
+#undef BARRIER_PTR
+#undef FINISHED_SUM_TAG
+}
+
+template <typename T> TL_DEVICE void wait_eq(void *ptr, T val) {
+  T *flag_ptr = reinterpret_cast<T *>(ptr);
+// Spin-loop
+#pragma unroll 1
+  while (ld_acquire(flag_ptr) != val)
+    ;
+}
+
+template <typename P, typename T> TL_DEVICE void wait_ne(P ptr, T val) {
+  static_assert(std::is_same_v<P, uint64_t> || std::is_pointer_v<P>,
+                "P must be a pointer or uint64_t");
+  T *flag_ptr = reinterpret_cast<T *>(ptr);
+// Spin-loop
+#pragma unroll 1
+  while (ld_volatile_global(flag_ptr) == val)
+    ;
+}
+
+template <typename P, typename T> TL_DEVICE void wait_ge(P ptr, T val) {
+  static_assert(std::is_same_v<P, uint64_t> || std::is_pointer_v<P>,
+                "P must be a pointer or uint64_t");
+  T *flag_ptr = reinterpret_cast<T *>(ptr);
+// Spin-loop
+#pragma unroll 1
+  while (ld_volatile_global(flag_ptr) < val)
+    ;
+}
+
+template <typename P, typename T> TL_DEVICE void wait_le(P ptr, T val) {
+  static_assert(std::is_same_v<P, uint64_t> || std::is_pointer_v<P>,
+                "P must be a pointer or uint64_t");
+  T *flag_ptr = reinterpret_cast<T *>(ptr);
+// Spin-loop
+#pragma unroll 1
+  while (ld_volatile_global(flag_ptr) > val)
+    ;
+}
+
+template <typename P, typename T> TL_DEVICE void wait_gt(P ptr, T val) {
+  static_assert(std::is_same_v<P, uint64_t> || std::is_pointer_v<P>,
+                "P must be a pointer or uint64_t");
+  T *flag_ptr = reinterpret_cast<T *>(ptr);
+// Spin-loop
+#pragma unroll 1
+  while (ld_volatile_global(flag_ptr) <= val)
+    ;
+}
+
+template <typename P, typename T> TL_DEVICE void wait_lt(P ptr, T val) {
+  static_assert(std::is_same_v<P, uint64_t> || std::is_pointer_v<P>,
+                "P must be a pointer or uint64_t");
+  T *flag_ptr = reinterpret_cast<T *>(ptr);
+// Spin-loop
+#pragma unroll 1
+  while (ld_volatile_global(flag_ptr) >= val)
+    ;
+}
+
+} // namespace tl
diff --git a/tilelang/language/distributed/__init__.py b/tilelang/language/distributed/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/tilelang/language/distributed/common.py b/tilelang/language/distributed/common.py
new file mode 100644
index 00000000..adb559e9
--- /dev/null
+++ b/tilelang/language/distributed/common.py
@@ -0,0 +1,162 @@
+"""The language interface for tl programs."""
+from __future__ import annotations
+
+from tvm import tir
+from tvm.tir import address_of
+from tvm.tir import PrimExpr, IntImm
+from enum import Enum
+
+
+def get_rank():
+    """Get the rank of the current process.
+    """
+    return tir.call_intrin("uint64", tir.op.Op.get("tl.get_rank"))
+
+
+def get_num_ranks():
+    """Get the number of processes.
+    """
+    return tir.call_intrin("uint64", tir.op.Op.get("tl.get_num_ranks"))
+
+
+def put_warp(src: PrimExpr,
+             dst: PrimExpr,
+             size: PrimExpr,
+             dst_pe: PrimExpr | IntImm | None = -1,
+             unroll_factor: int = 4,
+             enable_aggressive_vectorize: bool = False):
+    """Put to a remote buffer with unrolled loop.
+
+    Args:
+        src: PrimExpr
+            The source address.
+        dst: PrimExpr
+            The destination address.
+        size: PrimExpr
+            The size of the put in elements.
+        dst_pe: PrimExpr | None
+            The PE index of the destination.
+            -1 by default, which means local copy.
+        unroll_factor: int
+            The unroll factor
+        enable_aggressive_vectorize: bool
+            Whether to enable aggressive vectorization.
+            If True, the compiler with try to vectorize the copy via int4.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.put"), src, dst, size, dst_pe, unroll_factor,
+                           "warp", enable_aggressive_vectorize)
+
+
+def get_warp(src: PrimExpr,
+             dst: PrimExpr,
+             size: PrimExpr,
+             src_pe: PrimExpr | IntImm | None = -1,
+             unroll_factor: int = 4,
+             enable_aggressive_vectorize: bool = False):
+    """Get from a remote buffer with unrolled loop.
+
+    Args:
+        src: PrimExpr
+            The source address.
+        dst: PrimExpr
+            The destination address.
+        size: PrimExpr
+            The size of the get in elements.
+        src_pe: PrimExpr | None
+            The PE index of the source.
+            -1 by default, which means local copy.
+        unroll_factor: int
+            The unroll factor
+        enable_aggressive_vectorize: bool
+            Whether to enable aggressive vectorization.
+            If True, the compiler with try to vectorize the copy via int4.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.get"), src, dst, size, src_pe, unroll_factor,
+                           "warp", enable_aggressive_vectorize)
+
+
+def put_block(src: PrimExpr, dst: PrimExpr, size: PrimExpr, dst_pe: PrimExpr | IntImm | None = -1):
+    """Put to a remote buffer.
+
+    Args:
+        src: PrimExpr
+            The source address.
+        dst: PrimExpr
+            The destination address.
+        size: PrimExpr
+            The size of the put in elements.
+        dst_pe: PrimExpr | None
+            The PE index of the destination.
+            -1 by default, which means local copy.
+    """
+    return tir.call_intrin(
+        "handle", tir.op.Op.get("tl.put"), src, dst, size, dst_pe, 0, "block", True
+    )  # NOTE: unroll_factor is not needed because currently we implement block-level comm based on NVSHMEM-style copy
+
+
+def get_block(src: PrimExpr, dst: PrimExpr, size: PrimExpr, src_pe: PrimExpr | IntImm | None = -1):
+    """Get from a remote buffer.
+
+    Args:
+        src: PrimExpr
+            The source address.
+        dst: PrimExpr
+            The destination address.
+        size: PrimExpr
+            The size of the get in elements.
+        src_pe: PrimExpr | None
+            The PE index of the source.
+            -1 by default, which means local copy.
+    """
+    return tir.call_intrin(
+        "handle", tir.op.Op.get("tl.get"), src, dst, size, src_pe, 0, "block", True
+    )  # NOTE: unroll_factor is not needed because currently we implement block-level comm based on NVSHMEM-style copy
+
+
+class BinaryRelation(Enum):
+    EQ = 0
+    NE = 1
+    GE = 2
+    LE = 3
+    GT = 4
+    LT = 5
+
+
+def wait_eq(barrier: PrimExpr, expected: PrimExpr):
+    """Wait until *barrier == expected* for GPU-level synchronization.
+    # todo: have different semantic compared to 3 fns below currently
+    Args:
+        barrier: The barrier to wait at
+        expected: The expected value to wait for
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.wait_eq"), address_of(barrier), expected)
+
+
+def wait_ne(ptr: PrimExpr, expected: PrimExpr, peer: PrimExpr | None = -1):
+    """Wait until *ptr != expected"""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.wait"), BinaryRelation.NE.value,
+                           address_of(ptr), expected, peer)
+
+
+def wait_ge(ptr: PrimExpr, expected: PrimExpr, peer: PrimExpr | None = -1):
+    """Wait until *ptr >= expected"""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.wait"), BinaryRelation.GE.value,
+                           address_of(ptr), expected, peer)
+
+
+def wait_le(ptr: PrimExpr, expected: PrimExpr, peer: PrimExpr | None = -1):
+    """Wait until *ptr <= expected"""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.wait"), BinaryRelation.LE.value,
+                           address_of(ptr), expected, peer)
+
+
+def wait_gt(ptr: PrimExpr, expected: PrimExpr, peer: PrimExpr | None = -1):
+    """Wait until *ptr > expected"""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.wait"), BinaryRelation.GT.value,
+                           address_of(ptr), expected, peer)
+
+
+def wait_lt(ptr: PrimExpr, expected: PrimExpr, peer: PrimExpr | None = -1):
+    """Wait until *ptr < expected"""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.wait"), BinaryRelation.LT.value,
+                           address_of(ptr), expected, peer)
diff --git a/tilelang/language/distributed/multi_device/__init__.py b/tilelang/language/distributed/multi_device/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/tilelang/language/distributed/multi_device/cpengine.py b/tilelang/language/distributed/multi_device/cpengine.py
new file mode 100644
index 00000000..f5617d77
--- /dev/null
+++ b/tilelang/language/distributed/multi_device/cpengine.py
@@ -0,0 +1,7 @@
+"""The language interface for tl programs."""
+
+from tvm import tir
+
+
+def cpengine_cpasync(*args):
+    return tir.call_intrin("int32", tir.op.Op.get("tl.CpengineCpAsync"), *args)
diff --git a/tilelang/language/distributed/multi_device/nvshmem.py b/tilelang/language/distributed/multi_device/nvshmem.py
new file mode 100644
index 00000000..186a5d99
--- /dev/null
+++ b/tilelang/language/distributed/multi_device/nvshmem.py
@@ -0,0 +1,218 @@
+"""The language interface for tl programs."""
+
+from tvm import tir
+
+
+def get_pe():
+    """Get the processing element (PE) ID."""
+    return tir.call_intrin("int32", tir.op.Op.get("tl.GetPE"))
+
+
+def get_pe_num():
+    """Get the total number of processing elements (PEs)."""
+    return tir.call_intrin("int32", tir.op.Op.get("tl.GetPENum"))
+
+
+def int_p(dest, value, pe):
+    """Put a single integer value to a remote PE with a very low latency.
+    Args:
+        dest: Symmetric address of the destination data object.
+        value: The value to be transferred to dest.
+        pe: The PE ID of the destination PE.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.IntPE"), dest, value, pe)
+
+
+def barrier_all():
+    """Synchronizes all processing elements (PEs),
+    ensuring completion of all previously issued memory stores and remote memory updates."""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.BarrierAll"))
+
+
+def barrier_all_block(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.BarrierAllBlock"), *args)
+
+
+def barrier_all_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.BarrierAllWarp"), *args)
+
+
+def sync_all():
+    """Synchronizes all processing elements (PEs).
+    In contrast with `barrier_all`,
+    `sync_all` only ensures completion and visibility of previously issued memory stores,
+    and does not ensure completion of remote memory updates issued via NVSHMEM routines."""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.SyncAll"))
+
+
+def sync_all_block(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.SyncAllBlock"), *args)
+
+
+def sync_all_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.SyncAllWarp"), *args)
+
+
+def quiet():
+    """Ensures completion of all operations on symmetric data objects issued by the calling PE."""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.Quiet"))
+
+
+def fence():
+    """Ensures ordering of delivery of operations on symmetric data objects."""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.Fence"))
+
+
+def getmem_nbi_block(dest, src, nelems, pe):
+    """Get data from remote memory to local memory at block granularity without blocking.
+    Args:
+        dest: Symmetric address of the destination data object.
+        src: Symmetric address of the object containing the data to be copied.
+        nelems: Number of elements to be transferred (in bytes).
+        pe: The PE ID of the source PE.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.GetmemNbiBlock"), dest, src, nelems, pe)
+
+
+def getmem_block(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.GetmemBlock"), *args)
+
+
+def getmem_nbi_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.GetmemNbiWarp"), *args)
+
+
+def getmem_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.GetmemWarp"), *args)
+
+
+def getmem_nbi(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.GetmemNbi"), *args)
+
+
+def getmem(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.Getmem"), *args)
+
+
+def putmem_block(*args):
+    """Put data from local memory to remote memory at block granularity.
+    Args:
+        dest: Symmetric address of the destination data object.
+        src: Symmetric address of the object containing the data to be copied.
+        nelems: Number of elements to be transferred (in bytes).
+        pe: The PE ID of the destination PE.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemBlock"), *args)
+
+
+def putmem_nbi_block(dest, src, nelems, pe):
+    """Put data from local memory to remote memory at block granularity without blocking.
+    Args:
+        dest: Symmetric address of the destination data object.
+        src: Symmetric address of the object containing the data to be copied.
+        nelems: Number of elements to be transferred (in bytes).
+        pe: The PE ID of the destination PE.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemNbiBlock"), dest, src, nelems, pe)
+
+
+def putmem_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemWarp"), *args)
+
+
+def putmem_nbi_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemNbiWarp"), *args)
+
+
+def putmem(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.Putmem"), *args)
+
+
+def putmem_nbi(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemNbi"), *args)
+
+
+def putmem_signal(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemSignal"), *args)
+
+
+def putmem_signal_nbi(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemSignalNbi"), *args)
+
+
+def putmem_signal_block(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemSignalBlock"), *args)
+
+
+def putmem_signal_nbi_block(dest, src, nelems, sig_addr, signal, sig_op, pe):
+    """Put data from local memory to remote memory at block granularity without blocking,
+    and update a remote flag on delivery.
+    Args:
+        dest: Symmetric address of the destination data object.
+        src: Symmetric address of the object containing the data to be copied.
+        nelems: Number of elements to be transferred (in bytes).
+        sig_addr: Symmetric address of the remote flag to be updated.
+        signal: The value used for updating the remote signal data object.
+        sig_op: The type of update to be performed on the remote signal data object.
+        pe: The PE ID of the destination PE.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemSignalNbiBlock"), dest, src, nelems,
+                           sig_addr, signal, sig_op, pe)
+
+
+def putmem_signal_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemSignalWarp"), *args)
+
+
+def putmem_signal_nbi_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemSignalNbiWarp"), *args)
+
+
+def signal_op(sig_addr, signal, sig_op, pe):
+    """Atomically updates `sig_addr` with `signal` using operation `sig_op` on the specified PE.
+    Args:
+        sig_addr: Symmetric address of the signal word to be updated.
+        signal: The value used for updating the remote signal data object.
+        sig_op: The type of update to be performed on the remote signal data object.
+        pe: The PE ID of the destination PE.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.SignalOp"), sig_addr, signal, sig_op, pe)
+
+
+def signal_wait_until(sig_addr, cmp, cmp_val):
+    """Waits until the signal at `sig_addr` reaches the specified `signal` value on the specified PE.
+    Args:
+        sig_addr (uint64_t*): Symmetric address of the signal word to be waited on.
+        cmp: The comparison operation to be performed on the signal value.
+        cmp_val (uint64_t): The value to compare against the signal value.
+    """
+    # Actually nvshmem_signal_wait_until returns a uint64_t* value, but we simply ignore it
+    return tir.call_intrin("handle", tir.op.Op.get("tl.SignalWaitUntil"), sig_addr, cmp, cmp_val)
+
+
+def broadcast(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.Broadcast"), *args)
+
+
+def broadcast_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.BroadcastWarp"), *args)
+
+
+def broadcast_block(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.BroadcastBlock"), *args)
+
+
+def broadcastmem_block(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.BroadcastmemBlock"), *args)
+
+
+def fcollect(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.Fcollect"), *args)
+
+
+def fcollect_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.FcollectWarp"), *args)
+
+
+def fcollect_block(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.FcollectBlock"), *args)
