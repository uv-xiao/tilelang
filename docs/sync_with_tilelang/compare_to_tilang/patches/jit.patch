diff --git a/tilelang/jit/__init__.py b/tilelang/jit/__init__.py
index c5966d45..2080a00c 100644
--- a/tilelang/jit/__init__.py
+++ b/tilelang/jit/__init__.py
@@ -1,25 +1,23 @@
 """
-This module provides an auto-tuning infrastructure for TileLang (tl) programs. 
-It includes functionality to JIT-compile TileLang programs into a runnable 
+This module provides an auto-tuning infrastructure for TileLang (tl) programs.
+It includes functionality to JIT-compile TileLang programs into a runnable
 kernel adapter using TVM.
 """
+from __future__ import annotations
 
 from typing import (
     Any,
-    List,
-    Union,
     Callable,
-    Tuple,
     overload,
     Literal,
-    Dict,  # For type hinting dicts
-    Optional,
 )
 from tilelang import tvm as tvm
+from tilelang.jit.adapter.utils import is_metal_target
 from tvm.tir import PrimFunc
 from tvm.target import Target
 
 from tilelang.jit.kernel import JITKernel
+from tilelang.utils.target import determine_target
 from tilelang.cache import cached
 from os import path, makedirs
 from logging import getLogger
@@ -31,12 +29,13 @@ logger = getLogger(__name__)
 
 def compile(
     func: PrimFunc = None,
-    out_idx: Union[List[int], int, None] = None,
+    out_idx: list[int] | int | None = None,
     execution_backend: Literal["dlpack", "ctypes", "cython", "nvrtc"] = "cython",
-    target: Union[str, Target] = "auto",
-    target_host: Union[str, Target] = None,
+    target: str | Target = "auto",
+    target_host: str | Target | None = None,
     verbose: bool = False,
-    pass_configs: Optional[Dict[str, Any]] = None,
+    pass_configs: dict[str, Any] | None = None,
+    compile_flags: list[str] | str | None = None,
 ) -> JITKernel:
     """
     Compile the given TileLang PrimFunc with TVM and build a JITKernel.
@@ -56,16 +55,17 @@ def compile(
         Whether to enable verbose output (default: False).
     pass_configs : dict, optional
         Additional keyword arguments to pass to the Compiler PassContext.
-        Available options:
-            "tir.disable_vectorize": bool, default: False
-            "tl.disable_tma_lower": bool, default: False
-            "tl.disable_warp_specialized": bool, default: False
-            "tl.config_index_bitwidth": int, default: None
-            "tl.disable_dynamic_tail_split": bool, default: False
-            "tl.dynamic_vectorize_size_bits": int, default: 128
-            "tl.disable_safe_memory_legalize": bool, default: False
+        Refer to `tilelang.transform.PassConfigKey` for supported options.
     """
     assert isinstance(func, PrimFunc), f"target function must be a PrimFunc but got {type(func)}"
+    if isinstance(compile_flags, str):
+        compile_flags = [compile_flags]
+
+    # This path is not a performance critical path, so we can afford to convert the target.
+    target = Target(determine_target(target))
+
+    if is_metal_target(target):
+        assert execution_backend == 'torch', 'Currently metal target only support `tl.jit(execution_backend="torch")`'
 
     return cached(
         func=func,
@@ -75,27 +75,30 @@ def compile(
         target_host=target_host,
         verbose=verbose,
         pass_configs=pass_configs,
+        compile_flags=compile_flags,
     )
 
 
 class _JitImplementation:
 
-    out_idx: Optional[Union[List[int], int]]
-    target: Union[str, Target]
-    target_host: Union[str, Target]
+    out_idx: list[int] | int | None
+    target: str | Target
+    target_host: str | Target
     execution_backend: Literal["dlpack", "ctypes", "cython"]
     verbose: bool
-    pass_configs: Optional[Dict[str, Any]]
-    debug_root_path: Optional[str]
+    pass_configs: dict[str, Any] | None
+    debug_root_path: str | None
+    compile_flags: list[str] | str | None
 
     def __init__(self,
                  out_idx: Any = None,
-                 target: Union[str, Target] = "auto",
-                 target_host: Union[str, Target] = None,
+                 target: str | Target = "auto",
+                 target_host: str | Target = None,
                  execution_backend: Literal["dlpack", "ctypes", "cython"] = "cython",
                  verbose: bool = False,
-                 pass_configs: Optional[Dict[str, Any]] = None,
-                 debug_root_path: Optional[str] = None):
+                 pass_configs: dict[str, Any] | None = None,
+                 debug_root_path: str | None = None,
+                 compile_flags: list[str] | str | None = None):
         """
         Initializes the JIT compiler decorator.
 
@@ -127,6 +130,9 @@ class _JitImplementation:
             If None, no debug information is saved (default: None).
             If a relative path is given, it's made absolute relative to the project root
             or current working directory.
+        compile_flags : Optional[Union[List[str], str]], optional
+            Additional compilation flags to pass to the compiler.
+            If None, no additional compilation flags are passed (default: None).
         """
         self.out_idx = out_idx
         self.execution_backend = execution_backend
@@ -134,6 +140,7 @@ class _JitImplementation:
         self.target_host = target_host
         self.verbose = verbose
         self.pass_configs = pass_configs
+        self.compile_flags = compile_flags
 
         # Corrected debug_root_path handling
         self.debug_root_path = debug_root_path
@@ -144,12 +151,12 @@ class _JitImplementation:
             except NameError:
                 self.debug_root_path = path.abspath(self.debug_root_path)
 
-        self._kernel_cache: Dict[tuple, Kernel] = {}
+        self._kernel_cache: dict[tuple, Kernel] = {}
 
     # This tells the type checker what the *wrapper* function will return.
     # this is for linting, please do not remove it.
     @overload
-    def __call__(self, func: Callable[_P, _RProg]) -> Callable[_P, Tuple[_RProg, Kernel]]:
+    def __call__(self, func: Callable[_P, _RProg]) -> Callable[_P, tuple[_RProg, Kernel]]:
         ...
 
     @overload
@@ -176,6 +183,7 @@ class _JitImplementation:
                     'target_host': self.target_host,
                     'verbose': self.verbose,
                     'pass_configs': self.pass_configs,
+                    'compile_flags': self.compile_flags,
                 }
                 return compile_args
 
@@ -202,6 +210,7 @@ class _JitImplementation:
                     target_host=self.target_host,
                     verbose=self.verbose,
                     pass_configs=self.pass_configs,
+                    compile_flags=self.compile_flags,
                 )
 
                 if self.debug_root_path:
@@ -222,15 +231,16 @@ class _JitImplementation:
 
 
 def jit(  # This is the new public interface
-        func: Union[Callable[_P, _RProg], PrimFunc, None] = None,
+        func: Callable[_P, _RProg] | PrimFunc | None = None,
         *,  # Indicates subsequent arguments are keyword-only
         out_idx: Any = None,
-        target: Union[str, Target] = "auto",
-        target_host: Union[str, Target] = None,
-        execution_backend: Literal["dlpack", "ctypes", "cython"] = "cython",
+        target: str | Target = "auto",
+        target_host: str | Target = None,
+        execution_backend: Literal["dlpack", "ctypes", "cython", "nvrtc"] = "cython",
         verbose: bool = False,
-        pass_configs: Optional[Dict[str, Any]] = None,
-        debug_root_path: Optional[str] = None):
+        pass_configs: dict[str, Any] | None = None,
+        debug_root_path: str | None = None,
+        compile_flags: list[str] | str | None = None):
     """
     Just-In-Time (JIT) compiler decorator for TileLang functions.
 
@@ -247,7 +257,7 @@ def jit(  # This is the new public interface
         Compilation target for TVM (e.g., "cuda", "llvm"). Defaults to "auto".
     target_host : Union[str, Target], optional
         Target host for cross-compilation. Defaults to None.
-    execution_backend : Literal["dlpack", "ctypes", "cython"], optional
+    execution_backend : Literal["dlpack", "ctypes", "cython", "nvrtc"], optional
         Backend for kernel execution and argument passing. Defaults to "cython".
     verbose : bool, optional
         Enables verbose logging during compilation. Defaults to False.
@@ -262,6 +272,9 @@ def jit(  # This is the new public interface
         Either a JIT-compiled wrapper around the input function, or a configured decorator
         instance that can then be applied to a function.
     """
+    if isinstance(compile_flags, str):
+        compile_flags = [compile_flags]
+
     if callable(func):
         # Case 1: Used as @jit (func_or_out_idx is the function, others are defaults)
         # Create a default _JitImplementation instance and apply it to the function.
@@ -272,7 +285,8 @@ def jit(  # This is the new public interface
             execution_backend=execution_backend,
             verbose=verbose,
             pass_configs=pass_configs,
-            debug_root_path=debug_root_path)
+            debug_root_path=debug_root_path,
+            compile_flags=compile_flags)
         return default_decorator(func)
     elif isinstance(func, PrimFunc):
         raise ValueError("Use tilelang.jit to decorate prim_func is not supported yet.")
@@ -287,5 +301,6 @@ def jit(  # This is the new public interface
             execution_backend=execution_backend,
             verbose=verbose,
             pass_configs=pass_configs,
-            debug_root_path=debug_root_path)
+            debug_root_path=debug_root_path,
+            compile_flags=compile_flags)
         return configured_decorator
diff --git a/tilelang/jit/adapter/__init__.py b/tilelang/jit/adapter/__init__.py
index 43c94099..0e8fb98c 100644
--- a/tilelang/jit/adapter/__init__.py
+++ b/tilelang/jit/adapter/__init__.py
@@ -2,4 +2,5 @@ from .base import BaseKernelAdapter  # noqa: F401
 from .dlpack import TorchDLPackKernelAdapter  # noqa: F401
 from .ctypes import CtypesKernelAdapter  # noqa: F401
 from .cython import CythonKernelAdapter  # noqa: F401
-from .nvrtc import NVRTCKernelAdapter  # noqa: F401
\ No newline at end of file
+from .nvrtc import NVRTCKernelAdapter  # noqa: F401
+from .torch import MetalKernelAdapter  # noqa: F401
diff --git a/tilelang/jit/adapter/base.py b/tilelang/jit/adapter/base.py
index eff0986b..9d998bc9 100644
--- a/tilelang/jit/adapter/base.py
+++ b/tilelang/jit/adapter/base.py
@@ -1,21 +1,22 @@
 """The profiler and convert to torch utils"""
+from __future__ import annotations
 
 from abc import ABC, abstractmethod
-from typing import Any, List, Callable, Optional
+from typing import Any, Callable
 from tilelang.engine.param import KernelParam
 
 
 class BaseKernelAdapter(ABC):
 
-    func: Optional[Callable] = None
+    func: Callable | None = None
 
-    def __init__(self, mod, params: List[KernelParam], result_idx: List[int]) -> None:
+    def __init__(self, mod, params: list[KernelParam], result_idx: list[int]) -> None:
         self.mod = mod
         self.params = params
         self.result_idx = self._legalize_result_idx(result_idx)
         self._post_init()
 
-    def _legalize_result_idx(self, result_idx: Optional[List[int]]) -> List[int]:
+    def _legalize_result_idx(self, result_idx: list[int] | None) -> list[int]:
         params = self.params
         # result_idx is a list of indices of the output tensors
         if result_idx is None:
@@ -30,7 +31,7 @@ class BaseKernelAdapter(ABC):
             result_idx = [result_idx]
         elif isinstance(result_idx, list):
             for i, idx in enumerate(result_idx):
-                if idx >= len(params) or idx <= -len(params):
+                if idx >= len(params) or idx < -len(params):
                     raise ValueError(
                         f"result_idx should be an integer between {-len(params) - 1} and {len(params) - 1}"
                     )
diff --git a/tilelang/jit/adapter/ctypes/adapter.py b/tilelang/jit/adapter/ctypes/adapter.py
index 98e85c9f..648c66c1 100644
--- a/tilelang/jit/adapter/ctypes/adapter.py
+++ b/tilelang/jit/adapter/ctypes/adapter.py
@@ -1,12 +1,13 @@
 """The profiler and convert to torch utils"""
+from __future__ import annotations
 
 import torch
 from ..base import BaseKernelAdapter
 import ctypes
-from typing import List, Optional, Union, Callable, Dict, Tuple, Any
+from typing import Callable, Any
 from tilelang import tvm as tvm
 from tvm.target import Target
-from tvm.relay import TensorType
+from tvm.relax import TensorType
 from tvm import tir
 from tilelang.jit.adapter.wrapper import TLWrapper
 from tilelang.jit.adapter.libgen import LibraryGenerator
@@ -16,7 +17,7 @@ from tilelang.utils.language import retrieve_func_from_module
 
 class CtypesKernelAdapter(BaseKernelAdapter):
     """Adapter class that converts TVM/TIR functions to callable CUDA kernels using ctypes.
-    
+
     This adapter handles:
     1. Converting TIR functions to compiled CUDA libraries
     2. Managing dynamic shapes in tensor operations
@@ -25,33 +26,34 @@ class CtypesKernelAdapter(BaseKernelAdapter):
 
     # Class attributes to store compiled kernel information
     target = "cuda"
-    ir_module: Optional[tvm.IRModule] = None
+    ir_module: tvm.IRModule | None = None
     # The global source code of the kernel -> global means the source code of the kernel
     # that is not wrapped by the wrapper code
-    kernel_global_source: Optional[str] = None
-    lib: Optional[ctypes.CDLL] = None  # Compiled library handle
-    wrapped_source: Optional[str] = None  # Generated C++ wrapper code
+    kernel_global_source: str | None = None
+    lib: ctypes.CDLL | None = None  # Compiled library handle
+    wrapped_source: str | None = None  # Generated C++ wrapper code
     # Maps symbolic variables to their corresponding buffer and shape indices
-    dynamic_symbolic_map: Optional[Dict[tir.Var, Tuple[int, int]]] = None
+    dynamic_symbolic_map: dict[tir.Var, tuple[int, int]] | None = None
     # Pass configs for the compiler
-    pass_configs: Optional[Dict[str, Any]] = None
+    pass_configs: dict[str, Any] | None = None
 
     # Add new cache attributes
-    param_dtypes: Optional[List[torch.dtype]] = None  # Cache for parameter dtypes
-    param_shapes: Optional[List[List]] = None  # Cache for parameter shapes
+    param_dtypes: list[torch.dtype] | None = None  # Cache for parameter dtypes
+    param_shapes: list[list] | None = None  # Cache for parameter shapes
 
     def __init__(self,
-                 params: List[TensorType],
-                 result_idx: List[int],
+                 params: list[TensorType],
+                 result_idx: list[int],
                  target: str,
-                 func_or_mod: Union[tir.PrimFunc, tvm.IRModule],
-                 host_mod: Optional[tvm.IRModule] = None,
-                 device_mod: Optional[tvm.IRModule] = None,
-                 kernel_global_source: Optional[str] = None,
+                 func_or_mod: tir.PrimFunc | tvm.IRModule,
+                 host_mod: tvm.IRModule | None = None,
+                 device_mod: tvm.IRModule | None = None,
+                 kernel_global_source: str | None = None,
                  verbose: bool = False,
-                 pass_configs: Optional[Dict[str, Any]] = None):
+                 pass_configs: dict[str, Any] | None = None,
+                 compile_flags: list[str] | None = None):
         """Initialize the adapter with the given TIR function or module.
-        
+
         Args:
             params: List of tensor types for inputs/outputs
             result_idx: Indices of output tensors
@@ -87,8 +89,9 @@ class CtypesKernelAdapter(BaseKernelAdapter):
         self.target = Target.canon_target(determine_target(target))
         self.verbose = verbose
         self.wrapper = TLWrapper(self.target)
-        self.lib_generator = LibraryGenerator(self.target)
+        self.lib_generator = LibraryGenerator(self.target, verbose=verbose)
         self.lib_generator.assign_pass_configs(pass_configs)
+        self.lib_generator.assign_compile_flags(compile_flags)
 
         self.wrapper.assign_optimized_module(self.ir_module)
         self.wrapper.assign_pass_configs(pass_configs)
@@ -105,14 +108,15 @@ class CtypesKernelAdapter(BaseKernelAdapter):
 
     @classmethod
     def from_database(cls,
-                      params: List[TensorType],
-                      result_idx: List[int],
+                      params: list[TensorType],
+                      result_idx: list[int],
                       target: str,
-                      func_or_mod: Union[tir.PrimFunc, tvm.IRModule],
+                      func_or_mod: tir.PrimFunc | tvm.IRModule,
                       kernel_global_source: str,
                       kernel_lib_path: str,
                       verbose: bool = False,
-                      pass_configs: Optional[Dict[str, Any]] = None):
+                      pass_configs: dict[str, Any] | None = None,
+                      compile_flags: list[str] | None = None):
         adapter = cls.__new__(cls)
         adapter.params = params
         adapter.result_idx = adapter._legalize_result_idx(result_idx)
@@ -143,34 +147,45 @@ class CtypesKernelAdapter(BaseKernelAdapter):
 
         adapter.target = Target.canon_target(determine_target(target))
         adapter.verbose = verbose
-        adapter.lib_generator = LibraryGenerator(adapter.target)
+        adapter.lib_generator = LibraryGenerator(adapter.target, verbose=verbose)
         adapter.lib_generator.assign_pass_configs(pass_configs)
+        adapter.lib_generator.assign_compile_flags(compile_flags)
         adapter.lib = adapter.lib_generator.load_lib(lib_path=kernel_lib_path)
         adapter.lib.init()
 
         adapter._post_init()
         return adapter
 
-    def _process_dynamic_symbolic(self):
+    def _process_dynamic_symbolic(self) -> dict[tir.Var, tuple[int, int, int]]:
         """Extract information about dynamic shapes from the TIR function.
-        
-        Maps symbolic variables to their corresponding (buffer_index, shape_dimension)
+
+        Maps symbolic variables to their corresponding (id, buffer_index, dimension)
         for runtime shape resolution.
+        id represents shape or stride, 0 represents shape, 1 represents stride
         """
         func = self.prim_func
         params = func.params
         buffer_map = func.buffer_map
         dynamic_symbolic_map = {}
         for i, param in enumerate(params):
-            buffer = buffer_map[param]
-            for j, shape in enumerate(buffer.shape):
-                if isinstance(shape, tir.Var) and (shape not in dynamic_symbolic_map):
-                    dynamic_symbolic_map[shape] = (i, j)
+            if param in buffer_map:
+                buffer = buffer_map[param]
+                for j, shape in enumerate(buffer.shape):
+                    if (isinstance(shape, tir.Var) and (shape not in dynamic_symbolic_map) and
+                        (shape not in params)):
+                        dynamic_symbolic_map[shape] = (0, i, j)
+        for i, param in enumerate(params):
+            if param in buffer_map:
+                buffer = buffer_map[param]
+                for j, stride in enumerate(buffer.strides):
+                    if (isinstance(stride, tir.Var) and (stride not in dynamic_symbolic_map) and
+                        (stride not in params)):
+                        dynamic_symbolic_map[stride] = (1, i, j)
         return dynamic_symbolic_map
 
-    def _forward_from_prebuild_lib(self, *args, stream: Optional[int] = None):
+    def _forward_from_prebuild_lib(self, *args, stream: int | None = None):
         """Low-level function to call the compiled CUDA kernel.
-        
+
         Converts PyTorch tensor pointers to C void pointers for ctypes interface.
         """
         ctypes_args = [
@@ -179,21 +194,19 @@ class CtypesKernelAdapter(BaseKernelAdapter):
         ctypes_args.append(ctypes.c_void_p(stream))
         self.lib.call(*ctypes_args)
 
-    def _wrap_forward_from_prebuild_lib(self,
-                                        *ins: List[torch.Tensor],
-                                        stream: Optional[int] = None):
+    def _wrap_forward_from_prebuild_lib(self, *ins: list[torch.Tensor], stream: int | None = None):
         """High-level wrapper for kernel execution.
-        
+
         Handles:
         1. Input validation
         2. Output tensor allocation
         3. Dynamic shape resolution
         4. CUDA stream management
-        
+
         Args:
             ins: Input PyTorch tensors
             stream: Optional CUDA stream for asynchronous execution
-        
+
         Returns:
             Single tensor or list of tensors containing the kernel results
         """
@@ -224,8 +237,11 @@ class CtypesKernelAdapter(BaseKernelAdapter):
             args.append(tensor)
 
         # dynamic symbolics
-        for _, (buffer_idx, shape_idx) in self.dynamic_symbolic_map.items():
-            args.append(ins[buffer_idx].shape[shape_idx])
+        for _, (ref_id, buffer_idx, shape_idx) in self.dynamic_symbolic_map.items():
+            if ref_id == 0:
+                args.append(ins[buffer_idx].shape[shape_idx])
+            else:
+                args.append(ins[buffer_idx].stride(shape_idx))
 
         # if stream is not None, we need to pass the stream to the library
         if stream is None:
diff --git a/tilelang/jit/adapter/cython/adapter.py b/tilelang/jit/adapter/cython/adapter.py
index 3ad5ec0b..7857872c 100644
--- a/tilelang/jit/adapter/cython/adapter.py
+++ b/tilelang/jit/adapter/cython/adapter.py
@@ -1,180 +1,43 @@
 """The profiler and convert to torch utils"""
+from __future__ import annotations
 
-from ..base import BaseKernelAdapter
 import ctypes
-from typing import List, Optional, Union, Callable, Dict, Tuple, Any
+import logging
+import torch
+
+from typing import Callable, Any
 from tilelang import tvm as tvm
 from tvm.target import Target
 from tilelang.engine.param import KernelParam
 from tvm import tir
-from tvm.relay import TensorType
+from tvm.relax import TensorType
+
+from tilelang.jit.adapter.base import BaseKernelAdapter
 from tilelang.jit.adapter.wrapper import TLWrapper
 from tilelang.jit.adapter.libgen import LibraryGenerator
-from tilelang.jit.adapter.utils import is_cuda_target, is_hip_target, is_cpu_target
+from tilelang.jit.adapter.utils import is_cuda_target, is_hip_target, is_cpu_target, is_metal_target
 from tilelang.utils.target import determine_target
 from tilelang.utils.language import retrieve_func_from_module
 from tilelang.utils.tensor import map_torch_type
-from tilelang.contrib.cc import get_cplus_compiler
-import torch
-import sys
-import sysconfig
-import hashlib
-import os
-import fcntl
-from pathlib import Path
-import logging
-import site
 
 logger = logging.getLogger(__name__)
 
+try:
+    from tilelang_cython_wrapper import CythonKernelWrapper
+except ImportError:
+    raise
 
-def get_cython_compiler() -> Optional[str]:
-    """Return the path to the Cython compiler.
 
-    Returns
-    -------
-    out: Optional[str]
-        The path to the Cython compiler, or None if none was found.
+def is_symbolic_expr(expr) -> bool:
+    """Check if the expression is a symbolic expression.
+    A symbolic expression can be a simple tvm.Var, or an tvm.PrimExpr containing tvm.Var.
     """
-
-    cython_names = ["cython", "cython3"]
-
-    # Check system PATH
-    dirs_in_path = list(os.get_exec_path())
-
-    # Add user site-packages bin directory
-    user_base = site.getuserbase()
-    if user_base:
-        user_bin = os.path.join(user_base, "bin")
-        if os.path.exists(user_bin):
-            dirs_in_path = [user_bin] + dirs_in_path
-
-    # If in a virtual environment, add its bin directory
-    if sys.prefix != sys.base_prefix:
-        venv_bin = os.path.join(sys.prefix, "bin")
-        if os.path.exists(venv_bin):
-            dirs_in_path = [venv_bin] + dirs_in_path
-
-    for cython_name in cython_names:
-        for d in dirs_in_path:
-            cython_path = os.path.join(d, cython_name)
-            if os.path.isfile(cython_path) and os.access(cython_path, os.X_OK):
-                return cython_path
-    return None
-
-
-# Add cache management functions at module level
-def get_cache_dir() -> Path:
-    """Get the cache directory for the current Python version."""
-    py_version = f"py{sys.version_info.major}{sys.version_info.minor}"
-    # current directory
-    current_dir = os.path.dirname(os.path.abspath(__file__))
-    cache_dir = Path(current_dir) / ".cycache" / py_version
-    cache_dir.mkdir(parents=True, exist_ok=True)
-    return cache_dir
-
-
-def get_cached_lib(source_code: str) -> Tuple[Optional[ctypes.CDLL], Path]:
-    """Try to load cached library or return None if not found."""
-    code_hash = hashlib.sha256(source_code.encode()).hexdigest()
-    cache_path = get_cache_dir() / f"{code_hash}.so"
-    lock_file = cache_path.with_suffix('.lock')
-    with open(lock_file, 'w') as lock:
-        fcntl.flock(lock.fileno(), fcntl.LOCK_EX)
-        try:
-            if cache_path.exists():
-                try:
-                    if cache_path.stat().st_size > 1024:
-                        return ctypes.CDLL(str(cache_path)), cache_path
-                    else:
-                        cache_path.unlink()  # remove the incomplete file
-                except Exception as e:
-                    logger.error(f"Failed to load cached library: {e}")
-                    return None, cache_path
-            return None, cache_path
-        finally:
-            fcntl.flock(lock.fileno(), fcntl.LOCK_UN)
-
-
-# read the cython_wrapper.pyx file
-current_dir = os.path.dirname(os.path.abspath(__file__))
-cython_wrapper_path = os.path.join(current_dir, "cython_wrapper.pyx")
-
-with open(cython_wrapper_path, "r") as f:
-    cython_wrapper_code = f.read()
-    cache_dir = get_cache_dir()
-    source_path = cache_dir / "cython_wrapper.cpp"
-    library_path = cache_dir / "cython_wrapper.so"
-    md5_path = cache_dir / "md5.txt"
-    code_hash = hashlib.sha256(cython_wrapper_code.encode()).hexdigest()
-    cache_path = cache_dir / f"{code_hash}.so"
-    lock_file = cache_path.with_suffix('.lock')
-
-    # Check if cached version exists and is valid
-    need_compile = True
-    if md5_path.exists() and library_path.exists():
-        with open(md5_path, "r") as f:
-            cached_hash = f.read().strip()
-            if cached_hash == code_hash:
-                logger.debug("Cython jit adapter is up to date, no need to compile...")
-                need_compile = False
-            else:
-                logger.info("Cython jit adapter is out of date, need to recompile...")
-    else:
-        logger.info("No cached version found for cython jit adapter, need to compile...")
-
-    if need_compile:
-        logger.info("Waiting for lock to compile cython jit adapter...")
-        with open(lock_file, 'w') as lock:
-            fcntl.flock(lock.fileno(), fcntl.LOCK_EX)
-            try:
-                # After acquiring the lock, check again if the file has been compiled by another process
-                if md5_path.exists() and library_path.exists():
-                    with open(md5_path, "r") as f:
-                        cached_hash = f.read().strip()
-                        if cached_hash == code_hash:
-                            logger.info(
-                                "Another process has already compiled the file, using it...")
-                            need_compile = False
-
-                if need_compile:
-                    logger.info("Compiling cython jit adapter...")
-                    temp_path = cache_dir / f"temp_{code_hash}.so"
-
-                    with open(md5_path, "w") as f:
-                        f.write(code_hash)
-
-                    # compile the cython_wrapper.pyx file into .cpp
-                    cython = get_cython_compiler()
-                    if cython is None:
-                        raise Exception("Cython is not installed, please install it first.")
-                    os.system(f"{cython} {cython_wrapper_path} --cplus -o {source_path}")
-                    python_include_path = sysconfig.get_path("include")
-                    cc = get_cplus_compiler()
-                    command = f"{cc} -shared -pthread -fPIC -fwrapv -O2 -Wall -fno-strict-aliasing -I{python_include_path} {source_path} -o {temp_path}"
-                    os.system(command)
-
-                    # rename the temp file to the library file
-                    temp_path.rename(library_path)
-            except Exception as e:
-                if 'temp_path' in locals() and temp_path.exists():
-                    temp_path.unlink()
-                raise Exception(f"Failed to compile cython jit adapter: {e}") from e
-            finally:
-                if lock_file.exists():
-                    lock_file.unlink()
-
-    # add the .so file to the sys.path
-    cache_dir_str = str(cache_dir)
-    if cache_dir_str not in sys.path:
-        sys.path.append(cache_dir_str)
-
-from cython_wrapper import CythonKernelWrapper
+    return not isinstance(expr, tir.IntImm) and isinstance(expr, tir.PrimExpr)
 
 
 class CythonKernelAdapter(BaseKernelAdapter):
-    """Adapter class that converts TVM/TIR functions to callable CUDA kernels using ctypes.
-    
+    """Adapter class that converts TVM/TIR functions to callable CUDA kernels using cython.
+
     This adapter handles:
     1. Converting TIR functions to compiled CUDA libraries
     2. Managing dynamic shapes in tensor operations
@@ -182,41 +45,45 @@ class CythonKernelAdapter(BaseKernelAdapter):
     """
 
     # Class attributes to store compiled kernel information
-    target: Union[str, Target] = "cuda"
-    ir_module: Optional[tvm.IRModule] = None
+    target: str | Target = "cuda"
+    ir_module: tvm.IRModule | None = None
     # The global source code of the kernel -> global means the source code of the kernel
     # that is not wrapped by the wrapper code
-    kernel_global_source: Optional[str] = None
-    lib: Optional[ctypes.CDLL] = None  # Compiled library handle
-    wrapped_source: Optional[str] = None  # Generated C++ wrapper code
+    kernel_global_source: str | None = None
+    lib: ctypes.CDLL | None = None  # Compiled library handle
+    wrapped_source: str | None = None  # Generated C++ wrapper code
     # Maps symbolic variables to their corresponding buffer and shape indices
-    dynamic_symbolic_map: Optional[Dict[tir.Var, Tuple[int, int]]] = None
+    dynamic_symbolic_map: dict[tir.Var, tuple[int, int]] | None = None
     # Maps pointer arguments to their corresponding (buffer_index, shape_dimension)
-    ptr_map: Optional[Dict[int, str]] = None
+    ptr_map: dict[int, str] | None = None
     # Maps buffer variables to their corresponding dtypes
-    buffer_dtype_map: Optional[Dict[tir.Var, Tuple[int, torch.dtype]]] = None
-    # Maps buffer variables to their corresponding static shapes
-    # {
-    #     "A": [(0, 16), (1, 16)] -> represents A.shape = (16, 16)
+    buffer_dtype_map: dict[tir.Var, tuple[int, torch.dtype]] | None = None
+    # Maps buffer variables to their corresponding static shapes and strides,
+    # e.g., {
+    #     "A": [(0, 16), (1, 16)] -> represents A.shape/strides = (16, 16)
     # }
-    static_shape_map: Optional[Dict[tir.Var, Tuple[int, List[Tuple[int, int]]]]] = None
+    static_shape_map: dict[tir.Var, tuple[int, list[tuple[int, int]]]] | None = None
+    static_strides_map: dict[tir.Var, tuple[int, list[tuple[int, int]]]] | None = None
+    # Contains contiguous buffers
+    static_contiguous_list: list[tir.Var] | None = None
     # Maps buffer variables to their corresponding devices
-    buffer_device_map: Optional[Dict[tir.Var, Tuple[int, torch.device]]] = None
+    buffer_device_map: dict[tir.Var, tuple[int, torch.device]] | None = None
     # Pass configs for the compiler
-    pass_configs: Optional[Dict[str, Any]] = None
+    pass_configs: dict[str, Any] | None = None
 
     def __init__(self,
-                 params: List[KernelParam],
-                 result_idx: List[int],
-                 target: Union[str, Target],
-                 func_or_mod: Union[tir.PrimFunc, tvm.IRModule],
-                 host_mod: Optional[tvm.IRModule] = None,
-                 device_mod: Optional[tvm.IRModule] = None,
-                 kernel_global_source: Optional[str] = None,
+                 params: list[KernelParam],
+                 result_idx: list[int],
+                 target: str | Target,
+                 func_or_mod: tir.PrimFunc | tvm.IRModule,
+                 host_mod: tvm.IRModule | None = None,
+                 device_mod: tvm.IRModule | None = None,
+                 kernel_global_source: str | None = None,
                  verbose: bool = False,
-                 pass_configs: Optional[Dict[str, Any]] = None):
+                 pass_configs: dict[str, Any] | None = None,
+                 compile_flags: list[str] | None = None):
         """Initialize the adapter with the given TIR function or module.
-        
+
         Args:
             params: List of tensor types for inputs/outputs
             result_idx: Indices of output tensors
@@ -238,13 +105,18 @@ class CythonKernelAdapter(BaseKernelAdapter):
         self.dynamic_symbolic_map = self._process_dynamic_symbolic()
         self.buffer_dtype_map = self._process_buffer_dtype()
         self.ptr_map = self._process_ptr_map()
-        self.static_shape_map = self._process_static_shape()
         self.buffer_device_map = self._process_buffer_device()
 
+        static_buffer_infos = self._process_static_buffer_infos()
+        self.static_shape_map = static_buffer_infos[0]
+        self.static_strides_map = static_buffer_infos[1]
+        self.static_contiguous_list = static_buffer_infos[2]
+
         self.verbose = verbose
         self.wrapper = TLWrapper(self.target)
-        self.lib_generator = LibraryGenerator(self.target)
+        self.lib_generator = LibraryGenerator(self.target, verbose=verbose)
         self.lib_generator.assign_pass_configs(pass_configs)
+        self.lib_generator.assign_compile_flags(compile_flags)
 
         self.wrapper.assign_optimized_module(self.ir_module)
         self.wrapper.assign_pass_configs(pass_configs)
@@ -267,20 +139,23 @@ class CythonKernelAdapter(BaseKernelAdapter):
         self.cython_wrapper.set_dynamic_symbolic_map(self.dynamic_symbolic_map)
         self.cython_wrapper.set_buffer_dtype_map(self.buffer_dtype_map)
         self.cython_wrapper.set_static_shape_map(self.static_shape_map)
+        self.cython_wrapper.set_static_strides_map(self.static_strides_map)
+        self.cython_wrapper.set_static_contiguous_list(self.static_contiguous_list)
         self.cython_wrapper.set_buffer_device_map(self.buffer_device_map)
         self.cython_wrapper.set_ptr_map(self.ptr_map)
         self._post_init()
 
     @classmethod
     def from_database(cls,
-                      params: List[TensorType],
-                      result_idx: List[int],
+                      params: list[TensorType],
+                      result_idx: list[int],
                       target: str,
-                      func_or_mod: Union[tir.PrimFunc, tvm.IRModule],
+                      func_or_mod: tir.PrimFunc | tvm.IRModule,
                       kernel_global_source: str,
                       kernel_lib_path: str,
                       verbose: bool = False,
-                      pass_configs: Optional[Dict[str, Any]] = None):
+                      pass_configs: dict[str, Any] | None = None,
+                      compile_flags: list[str] | None = None):
         adapter = cls.__new__(cls)
         adapter.params = params
         adapter.result_idx = adapter._legalize_result_idx(result_idx)
@@ -298,13 +173,18 @@ class CythonKernelAdapter(BaseKernelAdapter):
 
         adapter.dynamic_symbolic_map = adapter._process_dynamic_symbolic()
         adapter.buffer_dtype_map = adapter._process_buffer_dtype()
-        adapter.static_shape_map = adapter._process_static_shape()
         adapter.ptr_map = adapter._process_ptr_map()
         adapter.buffer_device_map = adapter._process_buffer_device()
 
+        static_buffer_infos = adapter._process_static_buffer_infos()
+        adapter.static_shape_map = static_buffer_infos[0]
+        adapter.static_strides_map = static_buffer_infos[1]
+        adapter.static_contiguous_list = static_buffer_infos[2]
+
         adapter.verbose = verbose
-        adapter.lib_generator = LibraryGenerator(adapter.target)
+        adapter.lib_generator = LibraryGenerator(adapter.target, verbose=verbose)
         adapter.lib_generator.assign_pass_configs(pass_configs)
+        adapter.lib_generator.assign_compile_flags(compile_flags)
         adapter.lib = adapter.lib_generator.load_lib(lib_path=kernel_lib_path)
 
         adapter.lib.get_last_error.restype = ctypes.c_char_p
@@ -318,17 +198,20 @@ class CythonKernelAdapter(BaseKernelAdapter):
         adapter.cython_wrapper.set_dynamic_symbolic_map(adapter.dynamic_symbolic_map)
         adapter.cython_wrapper.set_buffer_dtype_map(adapter.buffer_dtype_map)
         adapter.cython_wrapper.set_static_shape_map(adapter.static_shape_map)
+        adapter.cython_wrapper.set_static_strides_map(adapter.static_strides_map)
+        adapter.cython_wrapper.set_static_contiguous_list(adapter.static_contiguous_list)
         adapter.cython_wrapper.set_buffer_device_map(adapter.buffer_device_map)
         adapter.cython_wrapper.set_ptr_map(adapter.ptr_map)
 
         adapter._post_init()
         return adapter
 
-    def _process_dynamic_symbolic(self) -> Dict[tir.Var, Tuple[int, int]]:
+    def _process_dynamic_symbolic(self) -> dict[tir.Var, tuple[int, int, int]]:
         """Extract information about dynamic shapes from the TIR function.
-        
-        Maps symbolic variables to their corresponding (buffer_index, shape_dimension)
+
+        Maps symbolic variables to their corresponding (id, buffer_index, dimension)
         for runtime shape resolution.
+        id represents shape or stride, 0 represents shape, 1 represents stride
         """
         func = self.prim_func
         params = func.params
@@ -340,12 +223,19 @@ class CythonKernelAdapter(BaseKernelAdapter):
                 for j, shape in enumerate(buffer.shape):
                     if (isinstance(shape, tir.Var) and (shape not in dynamic_symbolic_map) and
                         (shape not in params)):
-                        dynamic_symbolic_map[shape] = (i, j)
+                        dynamic_symbolic_map[shape] = (0, i, j)
+        for i, param in enumerate(params):
+            if param in buffer_map:
+                buffer = buffer_map[param]
+                for j, stride in enumerate(buffer.strides):
+                    if (isinstance(stride, tir.Var) and (stride not in dynamic_symbolic_map) and
+                        (stride not in params)):
+                        dynamic_symbolic_map[stride] = (1, i, j)
         return dynamic_symbolic_map
 
-    def _process_buffer_dtype(self) -> Dict[tir.Var, Tuple[int, torch.dtype]]:
+    def _process_buffer_dtype(self) -> dict[tir.Var, tuple[int, torch.dtype]]:
         """Extract information about buffer dtypes from the TIR function.
-        
+
         Maps buffer variables to their corresponding dtypes.
         """
         func = self.prim_func
@@ -359,9 +249,9 @@ class CythonKernelAdapter(BaseKernelAdapter):
                 buffer_dtype_map[name] = (i, map_torch_type(dtype))
         return buffer_dtype_map
 
-    def _process_ptr_map(self) -> Dict[int, str]:
+    def _process_ptr_map(self) -> dict[int, str]:
         """Extract information about pointer arguments from the TIR function.
-        
+
         Maps pointer arguments to their corresponding (buffer_index, shape_dimension)
         for runtime shape resolution.
         """
@@ -373,30 +263,47 @@ class CythonKernelAdapter(BaseKernelAdapter):
                 ptr_map[i] = param.name
         return ptr_map
 
-    def _process_static_shape(self) -> Dict[tir.Var, List[Tuple[int, int]]]:
+    def _process_static_buffer_infos(self) -> \
+            tuple[dict[tir.Var, tuple[int, list[tuple[int, int]]]],
+                  dict[tir.Var, tuple[int, list[tuple[int, int]]]],
+                  list[tuple[tir.Var]]]:
         """Extract information about static shapes from the TIR function.
-        
+
         Maps buffer variables to their corresponding static shapes.
         """
         func = self.prim_func
         params = func.params
         buffer_map = func.buffer_map
         static_shape_map = {}
+        static_strides_map = {}
+        static_contiguous_list = list()
         for i, param in enumerate(params):
             if param in buffer_map:
                 buffer = buffer_map[param]
-                name = buffer.name
-                shape = buffer.shape
-                static_shape = []
-                for j, s in enumerate(shape):
+                static_shape, static_strides = [], []
+                for j, s in enumerate(buffer.shape):
                     if isinstance(s, tir.IntImm):
                         static_shape.append((j, s.value))
-                static_shape_map[name] = (i, static_shape)
-        return static_shape_map
-
-    def _process_buffer_device(self) -> Dict[tir.Var, Tuple[int, torch.device]]:
+                    elif is_symbolic_expr(s):
+                        static_shape.append((j, -1))  # -1 for symbolic
+                    else:
+                        raise ValueError(f"Unsupported shape type: {type(s)}")
+                for j, s in enumerate(buffer.strides):
+                    if isinstance(s, tir.IntImm):
+                        static_strides.append((j, s.value))
+                is_contiguous, prod = True, 1
+                for dim, stride in reversed(list(zip(buffer.shape, buffer.strides))):
+                    is_contiguous &= bool(stride == prod)
+                    prod *= dim
+                static_shape_map[buffer.name] = (i, static_shape)
+                static_strides_map[buffer.name] = (i, static_strides)
+                if is_contiguous:
+                    static_contiguous_list.append((i, buffer.name))
+        return static_shape_map, static_strides_map, static_contiguous_list
+
+    def _process_buffer_device(self) -> dict[tir.Var, tuple[int, torch.device]]:
         """Extract information about buffer devices from the TIR function.
-        
+
         Maps buffer variables to their corresponding devices.
         """
         func = self.prim_func
@@ -408,6 +315,8 @@ class CythonKernelAdapter(BaseKernelAdapter):
             device = torch.device("cuda")
         elif is_cpu_target(self.target):
             device = torch.device("cpu")
+        elif is_metal_target(self.target):
+            device = torch.device("mps")
         else:
             raise ValueError(f"Unsupported target: {self.target}")
 
@@ -418,9 +327,9 @@ class CythonKernelAdapter(BaseKernelAdapter):
                 buffer_device_map[name] = (i, device)
         return buffer_device_map
 
-    def _forward_from_prebuild_lib(self, *args, stream: Optional[int] = None):
+    def _forward_from_prebuild_lib(self, *args, stream: int | None = None):
         """Low-level function to call the compiled CUDA kernel.
-        
+
         Converts PyTorch tensor pointers to C void pointers for ctypes interface.
         """
         ctypes_args = [
@@ -469,7 +378,7 @@ class CythonKernelAdapter(BaseKernelAdapter):
     @property
     def is_dynamic(self):
         """Indicates whether the kernel handles dynamic shapes."""
-        return (self.dynamic_symbolic_map is not None and len(self.dynamic_symbolic_map) > 0)
+        return self.dynamic_symbolic_map is not None and len(self.dynamic_symbolic_map) > 0
 
     def get_kernel_source(self, kernel_only: bool = False):
         """Returns the source code of the compiled kernel."""
diff --git a/tilelang/jit/adapter/cython/cython_wrapper.pyx b/tilelang/jit/adapter/cython/cython_wrapper.pyx
index 6e80765d..bc5ed215 100644
--- a/tilelang/jit/adapter/cython/cython_wrapper.pyx
+++ b/tilelang/jit/adapter/cython/cython_wrapper.pyx
@@ -7,21 +7,26 @@ from libc.stdint cimport int64_t, uintptr_t
 from libc.stdlib cimport malloc, free
 from tvm import tir
 from tilelang.utils.tensor import map_torch_type
+from tilelang.env import env
+if env.USE_NVSHMEM:
+    import pynvshmem
 
 cdef class CythonKernelWrapper:
     # Class attributes to store kernel configuration and library reference
     cdef:
-        object dynamic_symbolic_map  # Maps dynamic dimensions to their corresponding tensor indices
-        object buffer_device_map     # Maps buffer variables to their corresponding devices
-        object buffer_dtype_map     # Maps buffer variables to their corresponding dtypes
-        object static_shape_map     # Maps buffer variables to their corresponding static shapes
-        object ptr_map              # Maps pointer arguments to their corresponding buffer indices
-        list result_idx             # Indices of output tensors in the params list
-        list params                 # List of parameter specifications (includes both inputs and outputs)
-        object lib                  # Reference to the compiled library containing the kernel
+        object dynamic_symbolic_map    # Maps dynamic dimensions to their corresponding tensor indices
+        object buffer_device_map       # Maps buffer variables to their corresponding devices
+        object buffer_dtype_map        # Maps buffer variables to their corresponding dtypes
+        object static_shape_map        # Maps buffer variables to their corresponding static shapes
+        object static_strides_map      # Maps buffer variables to their corresponding static strides
+        object static_contiguous_list  # A list contains contiguous buffers
+        object ptr_map                 # Maps pointer arguments to their corresponding buffer indices
+        list result_idx                # Indices of output tensors in the params list
+        list params                    # List of parameter specifications (includes both inputs and outputs)
+        object lib                     # Reference to the compiled library containing the kernel
         # Add new cache attributes
-        list param_dtypes    # Cache for parameter dtypes
-        list param_shapes    # Cache for parameter shapes as native Python lists
+        list param_dtypes              # Cache for parameter dtypes
+        list param_shapes              # Cache for parameter shapes as native Python lists
         object get_current_device
 
     def __cinit__(self, result_idx, params, lib):
@@ -57,6 +62,14 @@ cdef class CythonKernelWrapper:
         self.static_shape_map = static_shape_map
         return self
 
+    def set_static_strides_map(self, static_strides_map):
+        self.static_strides_map = static_strides_map
+        return self
+
+    def set_static_contiguous_list(self, static_contiguous_list):
+        self.static_contiguous_list = static_contiguous_list
+        return self
+
     def set_ptr_map(self, ptr_map):
         self.ptr_map = ptr_map
         return self
@@ -94,15 +107,62 @@ cdef class CythonKernelWrapper:
     cpdef void _check_static_shape(self, list tensor_list):
         for param, (buffer_idx, shape_list) in self.static_shape_map.items():
             tensor = tensor_list[buffer_idx]
+            if not isinstance(tensor, torch.Tensor):
+                # otherwise, maybe torch.data_ptr() for T.ptr inputs
+                continue
+
+            # Check ndim
+            if tensor.dim() != len(shape_list):
+                raise ValueError(
+                    f"Static shape mismatch for parameter {param}: "
+                    f"expected {len(shape_list)} dimensions, "
+                    f"got {tensor.dim()}"
+                )
+                
+            # Check each dimension
+            for shape_idx, expected_shape in shape_list:
+                actual_shape = tensor.shape[shape_idx]
+                if expected_shape != -1 and actual_shape != expected_shape:
+                    raise ValueError(
+                        f"Static shape mismatch for parameter {param}: "
+                        f"expected {expected_shape} at index {shape_idx}, "
+                        f"got {actual_shape}"
+                    )
+
+    cpdef void _check_static_strides(self, list tensor_list):
+        for param, (buffer_idx, strides_list) in self.static_strides_map.items():
+            tensor = tensor_list[buffer_idx]
+            if not isinstance(tensor, torch.Tensor):
+                # otherwise, maybe torch.data_ptr() for T.ptr inputs
+                continue
+            for stride_idx, expected_stride in strides_list:
+                # Ensure the stride index is within the valid range of tensor dimensions
+                # (stride_idx should be less than the number of dimensions of the tensor)
+                assert stride_idx < tensor.dim(), f"Stride index {stride_idx} out of bounds for tensor with {tensor.dim()} dimensions"
+                if tensor.shape[stride_idx] == 1:
+                    continue
+                actual_stride = tensor.stride(stride_idx)
+                if actual_stride != expected_stride:
+                    raise ValueError(
+                        f"Static stride mismatch for parameter {param}: "
+                        f"expected {expected_stride} at index {stride_idx}, "
+                        f"got {actual_stride}"
+                    )
+
+    cpdef void _check_static_contiguous(self, list tensor_list):
+        for buffer_idx, param in self.static_contiguous_list:
+            tensor = tensor_list[buffer_idx]
+            if not isinstance(tensor, torch.Tensor):
+                # otherwise, maybe torch.data_ptr() for T.ptr inputs
+                continue
+            if not tensor.is_contiguous():
+                raise ValueError(f"Expected parameter {param} to be a contiguous tensor")
+
+    cdef object _infer_output_device(self, list inputs):
+        for tensor in inputs:
             if isinstance(tensor, torch.Tensor):
-                for shape_idx, expected_shape in shape_list:
-                    actual_shape = tensor.shape[shape_idx]
-                    if actual_shape != expected_shape:
-                        raise ValueError(
-                            f"Static shape mismatch for parameter {param}: "
-                            f"expected {expected_shape} at index {shape_idx}, "
-                            f"got {actual_shape}"
-                        )
+                return tensor.device
+        return torch.cuda.current_device()
 
     cpdef forward(self, list inputs, int64_t stream = -1, bint skip_tensor_validation = False):
         # Validate input dimensions and prepare for kernel execution
@@ -129,6 +189,7 @@ cdef class CythonKernelWrapper:
 
         cdef int ins_idx = 0
         cdef list tensor_list = []
+        device = None
 
         # Prepare input and output tensors
         for i in range(len(self.params)):
@@ -140,21 +201,36 @@ cdef class CythonKernelWrapper:
                     if isinstance(s, tir.Var):
                         for key in self.dynamic_symbolic_map:
                             if(str(s) == str(key)):
-                                ref_tensor_idx, ref_shape_idx = self.dynamic_symbolic_map[key]
+                                ref_id, ref_tensor_idx, ref_shape_idx = self.dynamic_symbolic_map[key]
                                 shape.append(tensor_list[ref_tensor_idx].shape[ref_shape_idx])
                     else:  # Already converted to Python int during initialization
                         shape.append(s)
-                device = inputs[0].device if len(inputs) > 0 else torch.cuda.current_device()
+
+                if device is None:
+                    device = self._infer_output_device(inputs)
+
                 if len(shape) == 0:
                     param_name = self.params[i].name if hasattr(self.params[i], 'name') else f'parameter_{i}'
                     raise ValueError(
                         f"Cannot create output tensor (name={param_name}) - 0-dimensional tensors are not supported. "
                         f"Expected shape: {shape}"
                     )
-                tensor = torch.empty(*shape, dtype=dtype, device=device)
+                if env.USE_NVSHMEM:
+                    tensor = pynvshmem.nvshmem_create_tensor(shape, dtype)
+                else:
+                    tensor = torch.empty(*shape, dtype=dtype, device=device)
             else:
                 tensor = inputs[ins_idx]
                 ins_idx += 1
+            # TODO(chenggang): remove this check or rewrite by ourselves?
+            '''
+            if isinstance(tensor, torch.Tensor) and tensor._base is not None and not tensor.is_contiguous():
+                base_tensor = tensor._base.as_strided(tensor._base.shape, tensor.stride())
+                if torch._debug_has_internal_overlap(base_tensor):
+                    raise ValueError(f"Cannot use an overlapping tensor"
+                                     f"(shape={tensor.shape}, strides={tensor.stride()}, "
+                                     f"overlap={torch._debug_has_internal_overlap(base_tensor)}) as the kernel input")
+            '''
             tensor_list.append(tensor)
 
         # Convert tensor pointers to C void pointers for kernel call
@@ -172,8 +248,6 @@ cdef class CythonKernelWrapper:
         call_args = []
         for i, tensor in enumerate(tensor_list):
             if isinstance(tensor, torch.Tensor):
-                if not tensor.is_contiguous():
-                    raise ValueError(f"Input tensor at index {i} must be contiguous")
                 call_args.append(ctypes.c_void_p(tensor.data_ptr()))
             elif isinstance(tensor, (int, float, bool)):
                 if i in self.ptr_map:
@@ -191,10 +265,15 @@ cdef class CythonKernelWrapper:
             self._check_buffer_device(tensor_list)
             self._check_buffer_dtype(tensor_list)
             self._check_static_shape(tensor_list)
+            self._check_static_strides(tensor_list)
+            self._check_static_contiguous(tensor_list)
 
         # Add dynamic dimension values to kernel arguments
-        for _, (buffer_idx, shape_idx) in self.dynamic_symbolic_map.items():
-            call_args.append(tensor_list[buffer_idx].shape[shape_idx])
+        for _, (ref_id, buffer_idx, shape_idx) in self.dynamic_symbolic_map.items():
+            if ref_id == 0:
+                call_args.append(tensor_list[buffer_idx].shape[shape_idx])
+            else:
+                call_args.append(tensor_list[buffer_idx].stride(shape_idx))
 
         # Add CUDA stream to kernel arguments
         call_args.append(ctypes.c_void_p(stream))
@@ -210,4 +289,4 @@ cdef class CythonKernelWrapper:
             return tensor_list[self.result_idx[0]]
         else:
             return [tensor_list[i] for i in self.result_idx]
-    
\ No newline at end of file
+    
diff --git a/tilelang/jit/adapter/dlpack.py b/tilelang/jit/adapter/dlpack.py
index b4574243..9fa767f0 100644
--- a/tilelang/jit/adapter/dlpack.py
+++ b/tilelang/jit/adapter/dlpack.py
@@ -1,7 +1,7 @@
 """The profiler and convert to torch utils"""
+from __future__ import annotations
 
 import torch
-from typing import List
 from tilelang.contrib.dlpack import to_pytorch_func
 from .base import BaseKernelAdapter
 
@@ -11,7 +11,7 @@ class TorchDLPackKernelAdapter(BaseKernelAdapter):
     def _convert_torch_func(self) -> callable:
         torch_func = to_pytorch_func(self.mod)
 
-        def func(*ins: List[torch.Tensor]):
+        def func(*ins: list[torch.Tensor]):
             if len(ins) + len(self.result_idx) != len(self.params):
                 raise ValueError(
                     f"Expected {len(self.params)} inputs, got {len(ins) + len(self.result_idx)} with {len(ins)} inputs and {len(self.result_idx)} outputs"
diff --git a/tilelang/jit/adapter/libgen.py b/tilelang/jit/adapter/libgen.py
index 4e596c40..fe839de1 100644
--- a/tilelang/jit/adapter/libgen.py
+++ b/tilelang/jit/adapter/libgen.py
@@ -1,3 +1,4 @@
+from __future__ import annotations
 import ctypes
 import importlib
 import logging
@@ -5,49 +6,54 @@ import os
 import os.path as osp
 import subprocess
 import tempfile
-from typing import Any, Dict, Optional
+from typing import Any
 
 from tvm.target import Target
 
 from tilelang import tvm as tvm
 from tilelang.transform import PassConfigKey
-from tilelang.contrib.nvcc import get_nvcc_compiler, get_target_compute_version
+from tilelang.contrib.nvcc import get_nvcc_compiler, get_target_arch, get_target_compute_version
 from tilelang.contrib.rocm import find_rocm_path, get_rocm_arch
-from tilelang.env import TILELANG_TEMPLATE_PATH
+from tilelang import env
+from tilelang.utils.deprecated import deprecated_warning
 
 from .utils import is_cpu_target, is_cuda_target, is_hip_target
 
 logger = logging.getLogger(__name__)
 
-is_nvrtc_available = False
-NVRTC_UNAVAILABLE_WARNING = "cuda-python is not available, nvrtc backend cannot be used. " \
-                            "Please install cuda-python via `pip install cuda-python` " \
-                            "if you want to use the nvrtc backend."
 try:
-    import cuda.bindings.driver as cuda
-    from tilelang.contrib.nvrtc import compile_cuda
-    is_nvrtc_available = True
+    from tilelang.jit.adapter.nvrtc import is_nvrtc_available
+    if is_nvrtc_available:
+        import cuda.bindings.driver as cuda
+        from tilelang.contrib.nvrtc import compile_cuda
 except ImportError:
-    pass
+    is_nvrtc_available = False
 
 
-class LibraryGenerator(object):
-    srcpath: Optional[str] = None
-    libpath: Optional[str] = None
-    lib_code: Optional[str] = None
-    pass_configs: Optional[Dict[str, Any]] = None
+class LibraryGenerator:
+    srcpath: str | None = None
+    libpath: str | None = None
+    lib_code: str | None = None
+    pass_configs: dict[str, Any] | None = None
+    compile_flags: list[str] | None = None
 
-    def __init__(self, target: Target):
+    def __init__(self, target: Target, verbose: bool = False):
         self.target = target
+        self.verbose = verbose
 
-    def assign_pass_configs(self, pass_configs: Optional[Dict[str, Any]] = None):
+    def assign_pass_configs(self, pass_configs: dict[str, Any] | None = None):
         self.pass_configs = pass_configs
 
+    def assign_compile_flags(self, compile_flags: list[str] | None = None):
+        if compile_flags is None:
+            compile_flags = []
+        self.compile_flags = compile_flags
+
     def update_lib_code(self, lib_code: str):
         self.lib_code = lib_code
 
     # Assume currently we only support CUDA compilation
-    def load_lib(self, lib_path: Optional[str] = None):
+    def load_lib(self, lib_path: str | None = None):
         if lib_path is None:
             lib_path = self.libpath
         else:
@@ -55,16 +61,29 @@ class LibraryGenerator(object):
         return ctypes.CDLL(lib_path)
 
     def compile_lib(self, timeout: float = None):
+        disable_rdc = self.pass_configs.get(PassConfigKey.TL_DISABLE_RDC, False)
+
         target = self.target
+        verbose = self.verbose
         if is_cuda_target(target):
             from tilelang.env import CUTLASS_INCLUDE_DIR
-            src = tempfile.NamedTemporaryFile(mode="w", suffix=".cu", delete=False)
-            compute_version = "".join(get_target_compute_version(target).split("."))
-            if compute_version == "90":
-                compute_version = "90a"
+            src = tempfile.NamedTemporaryFile(mode="w", suffix=".cu", delete=False)  # noqa: SIM115
+            target_arch = get_target_arch(get_target_compute_version(target))
             libpath = src.name.replace(".cu", ".so")
 
-            disable_fast_math = self.pass_configs.get(PassConfigKey.TL_DISABLE_FAST_MATH, False)
+            if self.pass_configs.get(PassConfigKey.TL_DISABLE_FAST_MATH):
+                deprecated_warning(
+                    "TL_DISABLE_FAST_MATH",
+                    "TL_ENABLE_FAST_MATH",
+                    "0.1.7",
+                )
+                enable_fast_math = not self.pass_configs.get(PassConfigKey.TL_DISABLE_FAST_MATH,
+                                                             True)
+            else:
+                enable_fast_math = self.pass_configs.get(PassConfigKey.TL_ENABLE_FAST_MATH, False)
+
+            ptxas_usage_level = self.pass_configs.get(PassConfigKey.TL_PTXAS_REGISTER_USAGE_LEVEL,
+                                                      None)
             verbose_ptxas_output = self.pass_configs.get(
                 PassConfigKey.TL_ENABLE_PTXAS_VERBOSE_OUTPUT, False)
 
@@ -75,25 +94,27 @@ class LibraryGenerator(object):
                 "-Xcudafe",
                 "--diag_suppress=177",
                 "--compiler-options",
-                "'-fPIC'",
+                "-fPIC",
                 "-lineinfo",
                 "--shared",
                 src.name,
                 "-lcuda",
                 "-gencode",
-                f"arch=compute_{compute_version},code=sm_{compute_version}",
+                f"arch=compute_{target_arch},code=sm_{target_arch}",
             ]
-            if not disable_fast_math:
+            if enable_fast_math:
                 command += ["--use_fast_math"]
+            if ptxas_usage_level is not None:
+                command += [f"--ptxas-options=--register-usage-level={ptxas_usage_level}"]
             if verbose_ptxas_output:
-                command += ["--ptxas-options", "-v"]
+                command += ["--ptxas-options=--verbose"]
             command += [
                 "-I" + CUTLASS_INCLUDE_DIR,
             ]
 
         elif is_hip_target(target):
             from tilelang.env import COMPOSABLE_KERNEL_INCLUDE_DIR
-            src = tempfile.NamedTemporaryFile(mode="w", suffix=".cpp", delete=False)
+            src = tempfile.NamedTemporaryFile(mode="w", suffix=".cpp", delete=False)  # noqa: SIM115
             libpath = src.name.replace(".cpp", ".so")
             rocm_path = find_rocm_path()
             arch = get_rocm_arch(rocm_path)
@@ -110,25 +131,42 @@ class LibraryGenerator(object):
             ]
         elif is_cpu_target(target):
             from tilelang.contrib.cc import get_cplus_compiler
-            src = tempfile.NamedTemporaryFile(mode="w", suffix=".cpp", delete=False)
+            src = tempfile.NamedTemporaryFile(mode="w", suffix=".cpp", delete=False)  # noqa: SIM115
             libpath = src.name.replace(".cpp", ".so")
 
             command = [get_cplus_compiler(), "-std=c++17", "-fPIC", "-shared", src.name]
             command += [
-                "-I" + TILELANG_TEMPLATE_PATH,
+                "-I" + env.TILELANG_TEMPLATE_PATH,
             ]
         else:
             raise ValueError(f"Unsupported target: {target}")
 
         command += [
-            "-I" + TILELANG_TEMPLATE_PATH,
+            "-I" + env.TILELANG_TEMPLATE_PATH,
         ]
+        if env.USE_NVSHMEM:
+            assert env.NVSHMEM_INCLUDE_DIR is not None, "env.NVSHMEM_INCLUDE_DIR is not set"
+            assert env.NVSHMEM_LIB_PATH is not None, "env.NVSHMEM_LIB_PATH is not set"
+            command += ["-diag-suppress=20013"]
+            if not disable_rdc:
+                command += ["-rdc=true"]
+            command += [
+                "-I" + env.NVSHMEM_INCLUDE_DIR, "-L" + env.NVSHMEM_LIB_PATH,
+                "-lnvshmem_host -lnvshmem_device"
+            ]
+
+        if self.compile_flags:
+            command += [
+                item for flag in self.compile_flags for item in flag.split() if item not in command
+            ]
+
         command += ["-o", libpath]
 
         src.write(self.lib_code)
         src.flush()
-
         try:
+            if verbose:
+                print(f"compile_lib compilation command: {' '.join(command)}")
             ret = subprocess.run(command, timeout=timeout)
         except Exception as e:
             raise RuntimeError(f"Compile kernel failed because of {e}") from e
@@ -159,14 +197,16 @@ class LibraryGenerator(object):
 
 
 class PyLibraryGenerator(LibraryGenerator):
-    host_func: Optional[str] = None
+    host_func: str | None = None
     culib = None
     pymodule = None
 
-    def __init__(self, target: Target):
+    def __init__(self, target: Target, verbose: bool = False):
         if not is_nvrtc_available:
-            raise ImportError(NVRTC_UNAVAILABLE_WARNING)
-        super().__init__(target)
+            raise ImportError("cuda-python is not available, nvrtc backend cannot be used. "
+                              "Please install cuda-python via `pip install cuda-python` "
+                              "if you want to use the nvrtc backend.")
+        super().__init__(target, verbose)
 
     @staticmethod
     def import_from_file(module_name, file_path):
@@ -178,7 +218,7 @@ class PyLibraryGenerator(LibraryGenerator):
     def update_host_func(self, host_func: str):
         self.host_func = host_func
 
-    def load_lib(self, lib_path: Optional[str] = None):
+    def load_lib(self, lib_path: str | None = None):
         if lib_path is None:
             lib_path = self.libpath
 
@@ -197,9 +237,10 @@ class PyLibraryGenerator(LibraryGenerator):
 
     def compile_lib(self, timeout: float = None):
         target = self.target
+        verbose = self.verbose
         if is_cuda_target(target):
             from tilelang.env import (CUDA_HOME, CUTLASS_INCLUDE_DIR, TILELANG_TEMPLATE_PATH)
-            src = tempfile.NamedTemporaryFile(mode="w", suffix=".cu", delete=False)
+            src = tempfile.NamedTemporaryFile(mode="w", suffix=".cu", delete=False)  # noqa: SIM115
             libpath = src.name.replace(".cu", ".cubin")
 
             project_root = osp.join(osp.dirname(__file__), "..", "..")
@@ -213,13 +254,24 @@ class PyLibraryGenerator(LibraryGenerator):
             else:
                 tl_template_path = TILELANG_TEMPLATE_PATH
 
-            cuda_home = "/usr/local/cuda" if CUDA_HOME is None else CUDA_HOME
+            cuda_home = CUDA_HOME if CUDA_HOME else "/usr/local/cuda"
+
+            options = [f"-I{tl_template_path}", f"-I{cutlass_path}", f"-I{cuda_home}/include"]
+            if self.compile_flags:
+                options += [
+                    item for flag in self.compile_flags for item in flag.split()
+                    if item not in options
+                ]
+
+            options = [f"-I{tl_template_path}", f"-I{cutlass_path}", f"-I{cuda_home}/include"]
+            if self.compile_flags:
+                options += [
+                    item for flag in self.compile_flags for item in flag.split()
+                    if item not in options
+                ]
 
             cubin_bytes = compile_cuda(
-                self.lib_code,
-                target_format="cubin",
-                options=[f"-I{tl_template_path}", f"-I{cutlass_path}", f"-I{cuda_home}/include"],
-                verbose=True)
+                self.lib_code, target_format="cubin", options=options, verbose=verbose)
             with open(libpath, "wb") as f:
                 f.write(cubin_bytes)
 
diff --git a/tilelang/jit/adapter/nvrtc/__init__.py b/tilelang/jit/adapter/nvrtc/__init__.py
index 762d6121..c9068faf 100644
--- a/tilelang/jit/adapter/nvrtc/__init__.py
+++ b/tilelang/jit/adapter/nvrtc/__init__.py
@@ -1 +1,47 @@
-from .adapter import NVRTCKernelAdapter  # noqa: F401
+"""NVRTC Backend for TileLang.
+
+This module provides runtime compilation support using NVIDIA's NVRTC API.
+"""
+
+import logging
+
+__all__ = ['NVRTCKernelAdapter', 'is_nvrtc_available', 'check_nvrtc_available']
+
+logger = logging.getLogger(__name__)
+
+# Check if cuda-python is available
+is_nvrtc_available = False
+NVRTC_UNAVAILABLE_MESSAGE = ("cuda-python is not available, NVRTC backend cannot be used. "
+                             "Please install cuda-python via `pip install cuda-python` "
+                             "if you want to use the NVRTC backend.")
+
+try:
+    import cuda.bindings.driver as cuda  # noqa: F401
+    import cuda.bindings.nvrtc as nvrtc  # noqa: F401
+    is_nvrtc_available = True
+except ImportError as e:
+    logger.debug(f"cuda-python import failed: {e}")
+
+
+def check_nvrtc_available():
+    """Check if NVRTC backend is available.
+
+    Raises
+    ------
+    ImportError
+        If cuda-python is not installed or cannot be imported
+    """
+    if not is_nvrtc_available:
+        raise ImportError(NVRTC_UNAVAILABLE_MESSAGE)
+
+
+# Conditionally import the adapter
+if is_nvrtc_available:
+    from .adapter import NVRTCKernelAdapter  # noqa: F401
+else:
+    # Provide a dummy class that raises error on instantiation
+    class NVRTCKernelAdapter:
+        """Dummy NVRTCKernelAdapter that raises ImportError on instantiation."""
+
+        def __init__(self, *args, **kwargs):
+            raise ImportError(NVRTC_UNAVAILABLE_MESSAGE)
diff --git a/tilelang/jit/adapter/nvrtc/adapter.py b/tilelang/jit/adapter/nvrtc/adapter.py
index 4f8f66ec..d6723a03 100644
--- a/tilelang/jit/adapter/nvrtc/adapter.py
+++ b/tilelang/jit/adapter/nvrtc/adapter.py
@@ -1,5 +1,6 @@
+from __future__ import annotations
 import logging
-from typing import Any, Callable, Dict, List, Optional, Union
+from typing import Any, Callable
 
 import torch
 from tvm import tir
@@ -11,20 +12,14 @@ from tilelang.jit.adapter.wrapper import TLPyWrapper
 from tilelang.jit.adapter.libgen import PyLibraryGenerator
 from tilelang.utils.language import retrieve_func_from_module
 from tilelang.utils.target import determine_target
-
-from ..base import BaseKernelAdapter
+from tilelang.jit.adapter.base import BaseKernelAdapter
+from tilelang.jit.adapter.nvrtc import is_nvrtc_available, check_nvrtc_available
 
 logger = logging.getLogger(__name__)
 
-is_nvrtc_available = False
-NVRTC_UNAVAILABLE_WARNING = "cuda-python is not available, nvrtc backend cannot be used. " \
-                            "Please install cuda-python via `pip install cuda-python` " \
-                            "if you want to use the nvrtc backend."
-try:
+# Import cuda bindings if available
+if is_nvrtc_available:
     import cuda.bindings.driver as cuda
-    is_nvrtc_available = True
-except ImportError:
-    pass
 
 
 class NVRTCKernelAdapter(BaseKernelAdapter):
@@ -32,18 +27,18 @@ class NVRTCKernelAdapter(BaseKernelAdapter):
     kernels = {}
 
     def __init__(self,
-                 params: List[KernelParam],
-                 result_idx: List[int],
-                 target: Union[str, Target],
-                 func_or_mod: Union[tir.PrimFunc, tvm.IRModule],
-                 host_mod: Optional[tvm.IRModule] = None,
-                 device_mod: Optional[tvm.IRModule] = None,
-                 kernel_global_source: Optional[str] = None,
+                 params: list[KernelParam],
+                 result_idx: list[int],
+                 target: str | Target,
+                 func_or_mod: tir.PrimFunc | tvm.IRModule,
+                 host_mod: tvm.IRModule | None = None,
+                 device_mod: tvm.IRModule | None = None,
+                 kernel_global_source: str | None = None,
                  verbose: bool = False,
-                 pass_configs: Optional[Dict[str, Any]] = None):
+                 pass_configs: dict[str, Any] | None = None,
+                 compile_flags: list[str] | None = None):
 
-        if not is_nvrtc_available:
-            raise ImportError(NVRTC_UNAVAILABLE_WARNING)
+        check_nvrtc_available()
 
         self.params = params
         self.result_idx = self._legalize_result_idx(result_idx)
@@ -80,9 +75,10 @@ class NVRTCKernelAdapter(BaseKernelAdapter):
         self.wrapper.assign_device_module(device_mod)
         self.host_func, self.function_names = self.wrapper.wrap(kernel_global_source)
 
-        self.lib_generator = PyLibraryGenerator(self.target)
+        self.lib_generator = PyLibraryGenerator(self.target, self.verbose)
         self.lib_generator.update_lib_code(self.kernel_global_source)
         self.lib_generator.update_host_func(self.host_func)
+        self.lib_generator.assign_compile_flags(compile_flags)
         self.lib_generator.compile_lib()
         self.lib_generator.load_lib()
         self.libpath = self.lib_generator.libpath
@@ -96,14 +92,15 @@ class NVRTCKernelAdapter(BaseKernelAdapter):
 
     @classmethod
     def from_database(cls,
-                      params: List[KernelParam],
-                      result_idx: List[int],
+                      params: list[KernelParam],
+                      result_idx: list[int],
                       target: str,
-                      func_or_mod: Union[tir.PrimFunc, tvm.IRModule],
+                      func_or_mod: tir.PrimFunc | tvm.IRModule,
                       kernel_global_source: str,
                       kernel_lib_path: str,
                       verbose: bool = False,
-                      pass_configs: Optional[Dict[str, Any]] = None):
+                      pass_configs: dict[str, Any] | None = None,
+                      compile_flags: list[str] | None = None):
         adapter = cls.__new__(cls)
         adapter.params = params
         adapter.result_idx = adapter._legalize_result_idx(result_idx)
@@ -133,7 +130,8 @@ class NVRTCKernelAdapter(BaseKernelAdapter):
 
         adapter.target = Target.canon_target(determine_target(target))
         adapter.verbose = verbose
-        adapter.lib_generator = PyLibraryGenerator(adapter.target)
+        adapter.lib_generator = PyLibraryGenerator(adapter.target, adapter.verbose)
+        adapter.lib_generator.assign_compile_flags(compile_flags)
         adapter.lib_generator.load_lib(lib_path=kernel_lib_path)
         adapter.pymodule = adapter.lib_generator.pymodule
         adapter.function_names = adapter.pymodule._function_names
@@ -146,11 +144,16 @@ class NVRTCKernelAdapter(BaseKernelAdapter):
         adapter._post_init()
         return adapter
 
-    def _process_dynamic_symbolic(self):
+    def _process_dynamic_symbolic(self) -> dict[tir.Var, tuple[int, int]]:
         """Extract information about dynamic shapes from the TIR function.
-        
+
         Maps symbolic variables to their corresponding (buffer_index, shape_dimension)
         for runtime shape resolution.
+
+        Returns
+        -------
+        Dict[tir.Var, Tuple[int, int]]
+            Mapping from symbolic variable to (buffer_index, shape_dimension)
         """
         func = self.prim_func
         params = func.params
@@ -163,29 +166,34 @@ class NVRTCKernelAdapter(BaseKernelAdapter):
                     dynamic_symbolic_map[shape] = (i, j)
         return dynamic_symbolic_map
 
-    def get_kernel_source(self):
+    def get_kernel_source(self) -> str | None:
+        """Get the CUDA kernel source code.
+
+        Returns
+        -------
+        Optional[str]
+            The kernel source code, or None if not available
+        """
         return self.kernel_global_source
 
-    def _forward_from_prebuild_lib(self, *args, stream: Optional[int] = None):
+    def _forward_from_prebuild_lib(self, *args, stream: int | None = None):
         """Low-level function to call the compiled CUDA kernel.
         """
         return self.pymodule.call(self.kernels, *args, stream=stream)
 
-    def _wrap_forward_from_prebuild_lib(self,
-                                        *ins: List[torch.Tensor],
-                                        stream: Optional[int] = None):
+    def _wrap_forward_from_prebuild_lib(self, *ins: list[torch.Tensor], stream: int | None = None):
         """High-level wrapper for kernel execution.
-        
+
         Handles:
         1. Input validation
         2. Output tensor allocation
         3. Dynamic shape resolution
         4. CUDA stream management
-        
+
         Args:
             ins: Input PyTorch tensors
             stream: Optional CUDA stream for asynchronous execution
-        
+
         Returns:
             Single tensor or list of tensors containing the kernel results
         """
@@ -233,7 +241,14 @@ class NVRTCKernelAdapter(BaseKernelAdapter):
         else:
             return [args[i] for i in self.result_idx]
 
-    def _convert_torch_func(self) -> Callable:
+    def _convert_torch_func(self) -> Callable[..., torch.Tensor | list[torch.Tensor]]:
+        """Convert to a PyTorch-compatible function.
+
+        Returns
+        -------
+        Callable[..., Union[torch.Tensor, List[torch.Tensor]]]
+            A callable function that takes tensors and returns tensor(s)
+        """
         return self._wrap_forward_from_prebuild_lib
 
     @property
diff --git a/tilelang/jit/adapter/torch/__init__.py b/tilelang/jit/adapter/torch/__init__.py
new file mode 100644
index 00000000..2390e3e7
--- /dev/null
+++ b/tilelang/jit/adapter/torch/__init__.py
@@ -0,0 +1,3 @@
+from .metal import MetalKernelAdapter
+
+__all__ = ['MetalKernelAdapter']
diff --git a/tilelang/jit/adapter/torch/metal.py b/tilelang/jit/adapter/torch/metal.py
new file mode 100644
index 00000000..30e84ad7
--- /dev/null
+++ b/tilelang/jit/adapter/torch/metal.py
@@ -0,0 +1,71 @@
+from __future__ import annotations
+from functools import wraps
+from typing import Callable
+
+import torch
+from tvm import tir
+
+from tilelang import tvm as tvm
+
+from ..base import BaseKernelAdapter
+from tilelang.engine.param import KernelParam
+
+
+class MetalKernelAdapter(BaseKernelAdapter):
+
+    def __init__(
+        self,
+        params: list[KernelParam],
+        result_idx: list[int],
+        #  target: Union[str, Target],
+        func_or_mod: tir.PrimFunc | tvm.IRModule,
+        #  host_mod: Optional[tvm.IRModule] = None,
+        device_mod: tvm.IRModule | None = None,
+        kernel_global_source: str | None = None,
+        verbose: bool = False,
+        #  pass_configs: Optional[Dict[str, Any]] = None,
+        #  compile_flags: Optional[List[str]] = None
+    ):
+        self.kernel_global_source = kernel_global_source
+        self.kernel_name = func_or_mod.__name__ + '_kernel'
+        self.verbose = verbose
+
+        self.block_info = [1, 1, 1]
+        self.grid_info = [1, 1, 1]
+
+        for var, func in device_mod.functions.items():
+            assert var.name_hint == self.kernel_name
+            thread_extent = func.attrs['thread_extent']
+            for tag, extent in thread_extent.items():
+                if "threadIdx" in tag:
+                    self.block_info["xyz".index(tag[-1])] = extent
+                elif "blockIdx" in tag:
+                    self.grid_info["xyz".index(tag[-1])] = extent
+            break
+        else:
+            raise AssertionError(f'no kernel with name {func_or_mod.__name__}')
+
+        # print(self.block_info, self.grid_info)
+        super().__init__(func_or_mod, result_idx=result_idx, params=params)
+
+    _kernel = None
+
+    def _convert_torch_func(self) -> Callable:
+
+        if self._kernel is None:
+
+            _kernel = getattr(torch.mps.compile_shader(self.kernel_global_source), self.kernel_name)
+            _threads = [x * y for (x, y) in zip(self.block_info, self.grid_info)]
+
+            @wraps(_kernel)
+            def launcher(*args: torch.Tensor):
+
+                return _kernel(
+                    *args,
+                    threads=_threads,
+                    group_size=self.block_info,
+                )
+
+            self._kernel = launcher
+
+        return self._kernel
diff --git a/tilelang/jit/adapter/utils.py b/tilelang/jit/adapter/utils.py
index aa3eeb1a..efc965e1 100644
--- a/tilelang/jit/adapter/utils.py
+++ b/tilelang/jit/adapter/utils.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import re
-from typing import Union, Optional, Literal, Dict
+from typing import Literal
 from tilelang import tvm as tvm
 from tvm import IRModule, tir
 from tvm.target import Target
@@ -60,12 +60,16 @@ def is_cpu_target(target: Target) -> bool:
     return target.kind.name in ["c"]
 
 
+def is_metal_target(target: Target) -> bool:
+    return target.kind.name == "metal"
+
+
 def get_annotated_mod(
-    func_or_mod: Union[tir.PrimFunc, tvm.IRModule],
-    target: Union[str, Target] = "auto",
-    target_host: Optional[Union[str, Target]] = None,
+    func_or_mod: tir.PrimFunc | tvm.IRModule,
+    target: str | Target = "auto",
+    target_host: str | Target | None = None,
     model_type: Literal["device", "host", "all"] = "all",
-) -> Union[IRModule, tuple[IRModule, IRModule]]:
+) -> IRModule | tuple[IRModule, IRModule]:
 
     # Validate model_type early
     if model_type not in {"device", "host", "all"}:
@@ -103,7 +107,7 @@ def get_annotated_mod(
     return dispatch[model_type](mod)
 
 
-def pythonic_expr(expr: tvm.tir.PrimExpr, dtype_map: Optional[Dict[str, str]] = None) -> str:
+def pythonic_expr(expr: tvm.tir.PrimExpr, dtype_map: dict[str, str] | None = None) -> str:
     """
     Converts a TVM PrimExpr into a Python-style string, correctly handling operator precedence.
 
diff --git a/tilelang/jit/adapter/wrapper.py b/tilelang/jit/adapter/wrapper.py
index 88a3edc0..4cd001cc 100644
--- a/tilelang/jit/adapter/wrapper.py
+++ b/tilelang/jit/adapter/wrapper.py
@@ -1,17 +1,20 @@
+from __future__ import annotations
 from abc import ABC, abstractmethod
 from tilelang import tvm as tvm
-from typing import Optional, List, Dict, Union, Any
+from tilelang import env
+from typing import Any
 from tvm import IRModule
 from tvm.target import Target
-from .utils import (match_declare_kernel, match_declare_kernel_cpu, is_cuda_target, is_hip_target,
-                    is_cpu_target, get_annotated_mod, pythonic_expr)
+from .utils import (is_metal_target, match_declare_kernel, match_declare_kernel_cpu, is_cuda_target,
+                    is_hip_target, is_cpu_target, get_annotated_mod, pythonic_expr)
 import re
 import logging
 import textwrap
+from tvm.tir.stmt_functor import post_order_visit
 
 PREDEF_ATTRIBUTE_SET_DYNAMIC_MEMORY = """
     cudaError_t result_{0} = cudaFuncSetAttribute({0}, cudaFuncAttributeMaxDynamicSharedMemorySize, {1});
-    if (result_{0} != CUDA_SUCCESS) {{
+    if (result_{0} != cudaSuccess) {{
         snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", {1}, cudaGetErrorString(result_{0}));
         return -1;
     }}
@@ -40,6 +43,28 @@ extern "C" int init() {{
 }}
 """
 
+PREDEF_INIT_TABLE_FUNC = """
+extern "C" int init_table(const void* host_table, size_t n, cudaStream_t stream) {{
+    if (error_buf) error_buf[0] = '\\0';
+
+    if (host_table == nullptr) {{
+        if (error_buf) std::snprintf(error_buf, 256, "host_table is null");
+        return -1;
+    }}
+    if (n == 0) {{
+        return 0;
+    }}
+
+    size_t bytes = n * sizeof(uint64_t);
+    cudaError_t err = cudaMemcpyToSymbolAsync(meta_data, host_table, bytes, 0, cudaMemcpyHostToDevice, stream);
+    if (err != cudaSuccess) {{
+        if (error_buf) std::snprintf(error_buf, 256, "cudaMemcpyToSymbolAsync failed: %s", cudaGetErrorString(err));
+        return static_cast<int>(err);
+    }}
+    return 0;
+}}
+"""
+
 PREDEF_HOST_FUNC = """
 extern "C" int call({}) {{
 {}
@@ -105,6 +130,35 @@ TMA_DESC_INIT_FUNC = """
 \t}}
 """
 
+TMA_IM2COL_DESC_INIT_FUNC = """
+\tCUtensorMap {0};
+\tCUtensorMapDataType {0}_type= (CUtensorMapDataType){1};
+\tcuuint32_t {0}_tensorRank= {2};
+\tvoid *{0}_globalAddress= {3};
+\tcuuint64_t {0}_globalDim[{2}]= {{{4}}};
+\tcuuint64_t {0}_globalStride[{2}]= {{{5}}};
+\tcuuint32_t {0}_elementStrides[{2}]= {{{6}}};
+\tint {0}_lowerCorner[{2} - 2]= {{{7}}};
+\tint {0}_upperCorner[{2} - 2]= {{{8}}};
+\tcuuint32_t {0}_channelsPerPixel= {9};
+\tcuuint32_t {0}_pixelsPerColumn= {10};
+\tCUtensorMapInterleave {0}_interleave= (CUtensorMapInterleave){11};
+\tCUtensorMapSwizzle {0}_swizzle= (CUtensorMapSwizzle){12};
+\tCUtensorMapL2promotion {0}_l2Promotion= (CUtensorMapL2promotion){13};
+\tCUtensorMapFloatOOBfill {0}_oobFill= (CUtensorMapFloatOOBfill){14};
+
+\tCUresult {0}_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeIm2col)(
+    &{0}, {0}_type, {0}_tensorRank, {0}_globalAddress, {0}_globalDim, {0}_globalStride + 1,
+    {0}_lowerCorner, {0}_upperCorner, {0}_channelsPerPixel, {0}_pixelsPerColumn, {0}_elementStrides, {0}_interleave, {0}_swizzle, {0}_l2Promotion, {0}_oobFill);
+
+\tif ({0}_result != CUDA_SUCCESS) {{
+\t\tstd::stringstream ss;
+\t\tss << "Error: Failed to initialize the TMA descriptor {0}";
+\t\tsnprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
+\t\treturn -1;
+\t}}
+"""
+
 TMA_DESC_INIT_FUNC_PY = """
 \t{0}_type = cuda.bindings.driver.CUtensorMapDataType({1})
 \t{0}_tensorRank = {2}
@@ -175,13 +229,13 @@ class BaseWrapper(ABC):
 logger = logging.getLogger(__name__)
 
 
-class TLCUDASourceWrapper(object):
+class TLCUDASourceWrapper:
     _TYPE_MAP = {
         "float32": "float",
         "float16": "half_t",
         "bfloat16": "bfloat16_t",
-        "e4m3_float8": "fp8_e4_t",
-        "e5m2_float8": "fp8_e5_t",
+        "float8_e4m3": "fp8_e4_t",
+        "float8_e5m2": "fp8_e5_t",
         "float64": "double",
         "int64": "int64_t",
         "int32": "int",
@@ -192,36 +246,39 @@ class TLCUDASourceWrapper(object):
         "int16": "int16_t",
         "uint16": "uint16_t",
         "uchar": "uint8_t",
+        "uint64": "uint64_t",
     }
 
     backend = "tl"
-    device_mod: Optional[IRModule] = None
-    host_mod: Optional[IRModule] = None
-    pass_configs: Optional[Dict[str, Any]] = None
+    device_mod: IRModule | None = None
+    host_mod: IRModule | None = None
+    pass_configs: dict[str, Any] | None = None
 
     def __init__(self,
                  scheduled_ir_module: IRModule,
                  source: str,
                  target: Target,
-                 device_mod: Optional[IRModule] = None,
-                 host_mod: Optional[IRModule] = None,
-                 pass_configs: Optional[Dict[str, Any]] = None):
+                 device_mod: IRModule | None = None,
+                 host_mod: IRModule | None = None,
+                 pass_configs: dict[str, Any] | None = None):
         self.mod = scheduled_ir_module
         self.target = target
         self.source = source
         self.pass_configs = pass_configs
         self.device_mod = device_mod
         self.host_mod = host_mod
-        self.function_names: Optional[str] = None
-        self.dynamic_smem_buf: Optional[int] = None
-        self.block_info: Union[List[int], Dict] = [1, 1, 1]
-        self.grid_info: Union[List[int], Dict] = [1, 1, 1]
-        self.tma_descriptor_args: Optional[Dict] = None
-        self.l2_persistent_map: Optional[Dict[str, Dict]] = {}
+        self.function_names: str | None = None
+        self.dynamic_smem_buf: int | None = None
+        self.block_info: list[int] | dict = [1, 1, 1]
+        self.grid_info: list[int] | dict = [1, 1, 1]
+        self.tma_descriptor_args: dict | None = None
+        self.use_distributed = env.USE_DISTRIBUTED
+        self.use_nvshmem = env.USE_NVSHMEM
+        self.l2_persistent_map: dict[str, dict] | None = {}
         self.parse_source_information()
-        self.srcpath: Optional[str] = None
-        self.libpath: Optional[str] = None
-        self.lib_code: Optional[str] = self.update_lib_code(source)
+        self.srcpath: str | None = None
+        self.libpath: str | None = None
+        self.lib_code: str | None = self.update_lib_code(source)
 
     def _pythonic_expr(self, expr: tvm.tir.PrimExpr) -> str:
         return pythonic_expr(expr, self._TYPE_MAP)
@@ -234,7 +291,10 @@ class TLCUDASourceWrapper(object):
         dynamic_symbolic_set = self.get_dynamic_symbolic_set(self.prim_func)
 
         function_args = []
+
         # Collect function arguments based on primary function's parameters and buffer mappings
+        # QA(@lei): Why not use device_mod.params?
+        # device func lack buffer map (to convert buffer handle to buffer)
         for param in self.prim_func.params:
             if param in self.prim_func.buffer_map:
                 buffer = self.prim_func.buffer_map[param]
@@ -257,9 +317,13 @@ class TLCUDASourceWrapper(object):
         # Format the function arguments for declaration
         def_args = ", ".join([f"{arg['type']} {arg['name']}" for arg in function_args])
 
-        def func_call_args(s, function_args, desc_name_map: Optional[Dict[str, str]] = None):
+        def func_call_args(s,
+                           function_args,
+                           function_params,
+                           desc_name_map: dict[str, str] | None = None,
+                           desc_name_var_map: dict[str, tvm.tir.Var] | None = None):
             # Extract the function call arguments matching the function definition
-            def maybe_desc(name: str, matches: List[str], i: int):
+            def maybe_desc(name: str, matches: list[str], i: int):
                 match = matches[i]
                 if not (match == name + "_desc" or match.startswith(name + "_desc_")):
                     return False
@@ -277,8 +341,15 @@ class TLCUDASourceWrapper(object):
             call_args = []
             for i, match in enumerate(matches):
                 for arg in function_args:
-                    if arg["name"] == match or maybe_desc(arg["name"], matches, i):
+                    if arg["name"] == match:
+                        call_args.append(match)
+                    elif maybe_desc(arg["name"], matches, i):
                         call_args.append(match)
+                        assert len(call_args) <= len(
+                            function_params
+                        ), f"Function {function_name} has {len(function_params)} parameters, but {len(call_args)} arguments"
+                        desc_name_var_map[match] = function_params[len(call_args) - 1]
+
             return call_args
 
         has_l2_persistent_map = False
@@ -290,11 +361,13 @@ class TLCUDASourceWrapper(object):
         kernel_launch_code = """"""
         if has_l2_persistent_map:
             kernel_launch_code += L2_PERSISTENT_MAP_CREATE_HANDLE
-        desc_name_map: Dict[str, str] = {}
+        desc_name_map: dict[str, str] = {}
+        desc_name_var_map: dict[str, tvm.tir.Var] = {}
         for function_name, function_info in function_informations.items():
             block_info = function_info["block_info"]
             grid_info = function_info["grid_info"]
             dynamic_smem_buf = function_info["dynamic_smem_buf"]
+            function_params = function_info["function_params"]
 
             # Find the location of the global kernel function in the code
             index = match_declare_kernel(code, function_name + "(")
@@ -305,20 +378,18 @@ class TLCUDASourceWrapper(object):
             # Identify the start of the function body to insert arguments
             index = code.index("{", index)
 
-            block_str = "dim3({}, {}, {})".format(
-                self._pythonic_expr(block_info[0]),
-                self._pythonic_expr(block_info[1]),
-                self._pythonic_expr(block_info[2]),
-            )
-            grid_str = "dim3({}, {}, {})".format(
-                self._pythonic_expr(grid_info[0]), self._pythonic_expr(grid_info[1]),
-                self._pythonic_expr(grid_info[2]))
+            block_str = f"dim3({self._pythonic_expr(block_info[0])}, {self._pythonic_expr(block_info[1])}, {self._pythonic_expr(block_info[2])})"
+            grid_str = f"dim3({self._pythonic_expr(grid_info[0])}, {self._pythonic_expr(grid_info[1])}, {self._pythonic_expr(grid_info[2])})"
             smem_str = 0 if dynamic_smem_buf is None else dynamic_smem_buf
             init_l2_persistent_map = self.generate_l2_persistent_map(function_name)
             kernel_launch_code += init_l2_persistent_map
 
             if self.use_cooperative_groups[function_name]:
-                args_list = func_call_args(declaration, function_args, desc_name_map)
+                args_list = func_call_args(declaration, function_args, function_params,
+                                           desc_name_map, desc_name_var_map)
+                assert len(function_params) == len(
+                    args_list
+                ), f"Function {function_name} has {len(function_params)} parameters, but {len(args_list)} arguments"
                 args_array = [f"(void*)&{arg}" for arg in args_list]
                 call_args = f"\tvoid* {function_name}_args[] = {{{', '.join(args_array)}}};\n"
                 kernel_launch_code += call_args
@@ -326,14 +397,19 @@ class TLCUDASourceWrapper(object):
                 kernel_launch_code += "\tTILELANG_CHECK(cudaLaunchCooperativeKernel((void*){}, {}, {}, {}, {}, stream));\n".format(
                     function_name, grid_str, block_str, function_name + "_args", smem_str)
             else:
-                call_args = ", ".join(func_call_args(declaration, function_args, desc_name_map))
-                kernel_launch_code += "\t{}<<<{}, {}, {}, stream>>>({});\n".format(
-                    function_name, grid_str, block_str, smem_str, call_args)
-                kernel_launch_code += "\tTILELANG_CHECK_LAST_ERROR(\"{}\");\n".format(function_name)
+                args_list = func_call_args(declaration, function_args, function_params,
+                                           desc_name_map, desc_name_var_map)
+                assert len(function_params) == len(
+                    args_list
+                ), f"Function {function_name} has {len(function_params)} parameters, but {len(args_list)} arguments"
+                call_args = ", ".join(args_list)
+                kernel_launch_code += f"\t{function_name}<<<{grid_str}, {block_str}, {smem_str}, stream>>>({call_args});\n"
+                kernel_launch_code += f"\tTILELANG_CHECK_LAST_ERROR(\"{function_name}\");\n"
             if has_l2_persistent_map:
                 kernel_launch_code += L2_PERSISTENT_MAP_RESET_HANDLE
 
-        init_tma_descriptor_args = self.generate_tma_descriptor_args(desc_name_map)
+        init_tma_descriptor_args = self.generate_tma_descriptor_args(desc_name_map,
+                                                                     desc_name_var_map)
         kernel_launch_code = init_tma_descriptor_args + kernel_launch_code
 
         # Wrap the kernel dispatch logic in an external C function
@@ -359,58 +435,107 @@ class TLCUDASourceWrapper(object):
 
         return init_l2_persistent_map
 
-    def generate_tma_descriptor_args(self, desc_name_map: Dict[str, str]) -> str:
+    def generate_tma_descriptor_args(self, desc_name_map: dict[str, str],
+                                     desc_name_var_map: dict[str, tvm.tir.Var]) -> str:
         tma_descripter_init = ""
         if self.tma_descriptor_args is None:
             return tma_descripter_init
+        for handle_name, _ in desc_name_map.items():
+            assert handle_name in desc_name_var_map, f"Handle name {handle_name} not found in desc_name_var_map"
+            desc_var = desc_name_var_map[handle_name]
 
-        for handle_name, name in desc_name_map.items():
-            desc_name = name + "_desc"
-            assert desc_name in self.tma_descriptor_args, f"TMA descriptor {desc_name} not found in {self.tma_descriptor_args}"
-            args = self.tma_descriptor_args[desc_name]
+            assert desc_var in self.tma_descriptor_args, f"TMA descriptor {desc_var} not found in {self.tma_descriptor_args}"
+            args = self.tma_descriptor_args[desc_var]
             # Skip __tvm_tensormap_create_tiled
             if len(args) < 3:
                 raise ValueError(
                     f"TMA descriptor args too short: {len(args)} elements, expected at least 3")
-            _, dtype, tensor_rank, globalAddress, *remaining_args = args[1:]
 
-            tensor_rank = int(tensor_rank)
+            tma_create_str, _, dtype, tensor_rank, globalAddress, *remaining_args = args
+
+            is_img2col = (tma_create_str.value == "__tvm_tensormap_create_im2col")
+            dtype = self._pythonic_expr(dtype)
+            tensor_rank = int(self._pythonic_expr(tensor_rank))
+
             # Validate tensor_rank
             if not isinstance(tensor_rank, int) or tensor_rank <= 0:
                 raise ValueError(f"Invalid tensor_rank: {tensor_rank}. Must be a positive integer")
 
-            # Calculate required length for remaining_args
-            expected_args_len = 4 * tensor_rank + 4  # 4 groups of tensor_rank size + 4 parameters
-            if len(remaining_args) < expected_args_len:
-                raise ValueError(f"Insufficient remaining args: got {len(remaining_args)}, "
-                                 f"expected {expected_args_len} for tensor_rank {tensor_rank}")
-
-            # Extract dimensions and strides using list slicing
-            global_dim = remaining_args[:tensor_rank]
-            global_stride = remaining_args[tensor_rank:2 * tensor_rank]
-            box_dim = remaining_args[2 * tensor_rank:3 * tensor_rank]
-            element_strides = remaining_args[3 * tensor_rank:4 * tensor_rank]
-
-            global_dim = [self._pythonic_expr(i) for i in global_dim]
-            global_stride = [self._pythonic_expr(i) for i in global_stride]
-            box_dim = [self._pythonic_expr(i) for i in box_dim]
-            element_strides = [self._pythonic_expr(i) for i in element_strides]
-
-            # Extract remaining parameters
-            try:
-                interleave, swizzle, l2Promotion, oobFill = remaining_args[4 * tensor_rank:4 *
-                                                                           tensor_rank + 4]
-            except ValueError as e:
-                raise ValueError(
-                    "Failed to unpack the final 4 TMA parameters (interleave, swizzle, l2Promotion, oobFill)"
-                ) from e
+            if not is_img2col:
+                # Calculate required length for remaining_args
+                expected_args_len = 4 * tensor_rank + 4  # 4 groups of tensor_rank size + 4 parameters
+                if len(remaining_args) < expected_args_len:
+                    raise ValueError(f"Insufficient remaining args: got {len(remaining_args)}, "
+                                     f"expected {expected_args_len} for tensor_rank {tensor_rank}")
+
+                # Extract dimensions and strides using list slicing
+                global_dim = remaining_args[:tensor_rank]
+                global_stride = remaining_args[tensor_rank:2 * tensor_rank]
+                box_dim = remaining_args[2 * tensor_rank:3 * tensor_rank]
+                element_strides = remaining_args[3 * tensor_rank:4 * tensor_rank]
+
+                global_dim = [self._pythonic_expr(i) for i in global_dim]
+                global_stride = [self._pythonic_expr(i) for i in global_stride]
+                box_dim = [self._pythonic_expr(i) for i in box_dim]
+                element_strides = [self._pythonic_expr(i) for i in element_strides]
+
+                # Extract remaining parameters
+                try:
+                    interleave, swizzle, l2Promotion, oobFill = remaining_args[4 * tensor_rank:4 *
+                                                                               tensor_rank + 4]
+                    interleave = self._pythonic_expr(interleave)
+                    swizzle = self._pythonic_expr(swizzle)
+                    l2Promotion = self._pythonic_expr(l2Promotion)
+                    oobFill = self._pythonic_expr(oobFill)
+                except ValueError as e:
+                    raise ValueError(
+                        "Failed to unpack the final 4 TMA parameters (interleave, swizzle, l2Promotion, oobFill)"
+                    ) from e
+
+                tma_descripter_init += TMA_DESC_INIT_FUNC.format(
+                    handle_name, dtype, tensor_rank, globalAddress, ",".join(global_dim),
+                    ",".join(global_stride), ",".join(box_dim), ",".join(element_strides),
+                    interleave, swizzle, l2Promotion, oobFill)
+            else:
+                # Calculate required length for remaining_args
+                expected_args_len = 5 * tensor_rank + 2
+                if len(remaining_args) < expected_args_len:
+                    raise ValueError(f"Insufficient remaining args: got {len(remaining_args)}, "
+                                     f"expected {expected_args_len} for tensor_rank {tensor_rank}")
+
+                # Extract dimensions and strides using list slicing
+                global_dim = remaining_args[:tensor_rank]
+                global_stride = remaining_args[tensor_rank:2 * tensor_rank]
+                element_strides = remaining_args[2 * tensor_rank:3 * tensor_rank]
+                lower_corner = remaining_args[3 * tensor_rank:4 * tensor_rank - 2]
+                upper_corner = remaining_args[4 * tensor_rank - 2:5 * tensor_rank - 4]
+                global_dim = [self._pythonic_expr(i) for i in global_dim]
+                global_stride = [self._pythonic_expr(i) for i in global_stride]
+                element_strides = [self._pythonic_expr(i) for i in element_strides]
+                lower_corner = [self._pythonic_expr(i) for i in lower_corner]
+                upper_corner = [self._pythonic_expr(i) for i in upper_corner]
+
+                # Extract remaining parameters
+                try:
+                    smem_box_pixel, smem_box_channel, interleave, swizzle, l2Promotion, oobFill = remaining_args[
+                        5 * tensor_rank - 4:5 * tensor_rank + 2]
+                    smem_box_pixel = self._pythonic_expr(smem_box_pixel)
+                    smem_box_channel = self._pythonic_expr(smem_box_channel)
+                    interleave = self._pythonic_expr(interleave)
+                    swizzle = self._pythonic_expr(swizzle)
+                    l2Promotion = self._pythonic_expr(l2Promotion)
+                    oobFill = self._pythonic_expr(oobFill)
+                except ValueError as e:
+                    raise ValueError(
+                        "Failed to unpack the final 6 TMA parameters (smem_box_pixel, smem_box_channel, interleave, swizzle, l2Promotion, oobFill)"
+                    ) from e
+
+                tma_descripter_init += TMA_IM2COL_DESC_INIT_FUNC.format(
+                    handle_name, dtype, tensor_rank, globalAddress, ",".join(global_dim),
+                    ",".join(global_stride), ",".join(element_strides), ",".join(lower_corner),
+                    ",".join(upper_corner), smem_box_channel, smem_box_pixel, interleave, swizzle,
+                    l2Promotion, oobFill)
 
-            tma_descripter_init += TMA_DESC_INIT_FUNC.format(handle_name, dtype, tensor_rank,
-                                                             globalAddress, ",".join(global_dim),
-                                                             ",".join(global_stride),
-                                                             ",".join(box_dim),
-                                                             ",".join(element_strides), interleave,
-                                                             swizzle, l2Promotion, oobFill)
         return tma_descripter_init
 
     def parse_source_information(self):
@@ -478,13 +603,27 @@ class TLCUDASourceWrapper(object):
 
     def get_dynamic_symbolic_set(self, prim_func):
         # Determine the set of dynamic symbols used in the function
-        dynamic_symbolic_set: List[str] = []
+        dynamic_symbolic_set: list[str] = []
+
+        def unique_push_back(name: str):
+            if name not in dynamic_symbolic_set:
+                dynamic_symbolic_set.append(name)
+
         for param in prim_func.params:
             if param in prim_func.buffer_map:
                 buffer = prim_func.buffer_map[param]
                 for dim in buffer.shape:
-                    if isinstance(dim, tvm.tir.Var) and (dim.name not in dynamic_symbolic_set):
-                        dynamic_symbolic_set.append(dim.name)
+                    if isinstance(dim, tvm.tir.Var):
+                        unique_push_back(dim.name)
+
+        # Note: In buffer definitions, any dynamic symbols appearing in strides are listed after those in the shape.
+        for param in prim_func.params:
+            if param in prim_func.buffer_map:
+                buffer = prim_func.buffer_map[param]
+                for stride in buffer.strides:
+                    if isinstance(stride, tvm.tir.Var):
+                        unique_push_back(stride.name)
+
         return dynamic_symbolic_set
 
     def get_init_func(self):
@@ -496,8 +635,11 @@ class TLCUDASourceWrapper(object):
                 # Format the cudaFuncSetAttribute call for dynamic shared memory
                 call_str += PREDEF_ATTRIBUTE_SET_DYNAMIC_MEMORY.format(
                     function_name, dynamic_smem_buf)
+        nvshmem_init_str = "nvshmem_init();\n\t" if self.use_nvshmem else ""
         # Format the initialization function using the call_str
-        init_funcs = PREDEF_INIT_FUNC.format(call_str)
+        init_funcs = PREDEF_INIT_FUNC.format(nvshmem_init_str + call_str)
+        if self.use_distributed:
+            init_funcs += PREDEF_INIT_TABLE_FUNC
         return init_funcs
 
     def update_lib_code(self, code: str):
@@ -514,12 +656,35 @@ class TLCUDASourceWrapper(object):
             # Do not update function with dispatch host function
             if (function_name not in self.block_info) or (function_name not in self.grid_info):
                 continue
+            assert function_name in self.device_mod, f"Function {function_name} not found in device module"
+            device_func = self.device_mod[function_name]
+            kernel_params_cnt = len(device_func.params)
+            function_params: list[str] = None
+
+            def visitor(node, fn=function_name, param_cnt=kernel_params_cnt):
+                nonlocal function_params
+                if isinstance(node, tvm.tir.Call):
+                    if not (hasattr(node, "op") and
+                            node.op == tvm.ir.Op.get("tir.tvm_call_packed")):
+                        return
+                    args = node.args
+                    if not args or args[0] != fn:
+                        return
+                    if len(args) < 1 + param_cnt:
+                        raise AssertionError(
+                            "tvm_call_packed should have at least 1 argument and match device function parameters"
+                        )
+                    function_params = args[1:1 + param_cnt]
+
+            post_order_visit(self.host_func.body, visitor)
+            assert function_params is not None, "function_params should not be None"
 
             function_informations[function_name] = {
                 "function_name": function_name,
                 "block_info": self.block_info[function_name],
                 "grid_info": self.grid_info[function_name],
                 "dynamic_smem_buf": self.dynamic_smem_buf[function_name],
+                "function_params": function_params,
             }
 
         # Create the host function wrapper for the CUDA kernel
@@ -528,8 +693,8 @@ class TLCUDASourceWrapper(object):
         lib_code = self.source + init_func + host_func
         return lib_code
 
-    def get_stream_type(self) -> Dict[str, str]:
-        return {"name": "stream=cudaStreamDefault", "type": "cudaStream_t"}
+    def get_stream_type(self) -> dict[str, str]:
+        return {"name": "stream", "type": "cudaStream_t"}
 
     @property
     def prim_func(self):
@@ -544,6 +709,32 @@ class TLCUDASourceWrapper(object):
                     return function
             raise ValueError("Cannot find primary function in the module.")
 
+    @property
+    def device_func(self):
+        if len(self.device_mod.get_global_vars()) == 1:
+            return self.device_mod[self.device_mod.get_global_vars()[0]]
+        elif "main" in self.device_mod:
+            return self.device_mod["main"]
+        else:
+            for _, function in self.device_mod.functions.items():
+                attr = function.attrs
+                if "tir.is_global_func" in attr and attr["tir.is_global_func"]:
+                    return function
+            raise ValueError("Cannot find primary function in the module.")
+
+    @property
+    def host_func(self):
+        if len(self.host_mod.get_global_vars()) == 1:
+            return self.host_mod[self.host_mod.get_global_vars()[0]]
+        elif "main" in self.host_mod:
+            return self.host_mod["main"]
+        else:
+            for _, function in self.host_mod.functions.items():
+                attr = function.attrs
+                if "tir.is_global_func" in attr and attr["tir.is_global_func"]:
+                    return function
+            raise ValueError("Cannot find primary function in the module.")
+
 
 class TLNVRTCSourceWrapper(TLCUDASourceWrapper):
     """
@@ -554,8 +745,8 @@ class TLNVRTCSourceWrapper(TLCUDASourceWrapper):
         "float32": "ctypes.c_float",
         "float16": "ctypes.c_uint16",
         "bfloat16": "ctypes.c_uint16",
-        "e4m3_float8": "ctypes.c_uint8",
-        "e5m2_float8": "ctypes.c_uint8",
+        "float8_e4m3": "ctypes.c_uint8",
+        "float8_e5m2": "ctypes.c_uint8",
         "float64": "ctypes.c_double",
         "int64": "ctypes.c_int64",
         "int32": "ctypes.c_int32",
@@ -572,9 +763,9 @@ class TLNVRTCSourceWrapper(TLCUDASourceWrapper):
                  scheduled_ir_module: IRModule,
                  source: str,
                  target: Target,
-                 device_mod: Optional[IRModule] = None,
-                 host_mod: Optional[IRModule] = None,
-                 pass_configs: Optional[Dict[str, Any]] = None):
+                 device_mod: IRModule | None = None,
+                 host_mod: IRModule | None = None,
+                 pass_configs: dict[str, Any] | None = None):
         super().__init__(scheduled_ir_module, source, target, device_mod, host_mod, pass_configs)
 
     def create_dispatch_func(self, code, function_informations):
@@ -601,13 +792,12 @@ class TLNVRTCSourceWrapper(TLCUDASourceWrapper):
                 function_args.append({"name": dyn_sym, "type": "ctypes.c_int"})
 
         function_args.append(self.get_stream_type())
-
         # Format the function arguments for declaration
         def_args = ", ".join([f"{arg['name']}" for arg in function_args])
 
-        def func_call_args(s, function_args, desc_name_map: Optional[Dict[str, str]] = None):
+        def func_call_args(s, function_args, desc_name_map: dict[str, str] | None = None):
             # Extract the function call arguments matching the function definition
-            def maybe_desc(name: str, matches: List[str], i: int):
+            def maybe_desc(name: str, matches: list[str], i: int):
                 match = matches[i]
                 if not (match == name + "_desc" or match.startswith(name + "_desc_")):
                     return False
@@ -633,7 +823,7 @@ class TLNVRTCSourceWrapper(TLCUDASourceWrapper):
                         call_args.append((match, "None"))
             return call_args
 
-        desc_name_map: Dict[str, str] = {}
+        desc_name_map: dict[str, str] = {}
         device_index = 0
         kernel_launch_code = """"""
         for function_name, function_info in function_informations.items():
@@ -670,7 +860,7 @@ class TLNVRTCSourceWrapper(TLCUDASourceWrapper):
             repr(list(function_informations.keys())), def_args, kernel_launch_code)
         return host_func
 
-    def generate_tma_descriptor_args(self, desc_name_map: Dict[str, str]) -> str:
+    def generate_tma_descriptor_args(self, desc_name_map: dict[str, str]) -> str:
         tma_descripter_init = ""
         if self.tma_descriptor_args is None:
             return tma_descripter_init
@@ -748,8 +938,8 @@ class TLNVRTCSourceWrapper(TLCUDASourceWrapper):
         self.host_func = self.create_dispatch_func(code, function_informations)
         return self.lib_code
 
-    def get_stream_type(self) -> Dict[str, str]:
-        return {"name": "stream=0", "type": "int"}
+    def get_stream_type(self) -> dict[str, str]:
+        return {"name": "stream", "type": "int"}
 
 
 class TLHIPSourceWrapper(TLCUDASourceWrapper):
@@ -761,8 +951,8 @@ class TLHIPSourceWrapper(TLCUDASourceWrapper):
         "float32": "float",
         "float16": "half_t",
         "bfloat16": "bfloat16_t",
-        "e4m3_float8": "fp8_e4_t",
-        "e5m2_float8": "fp8_e5_t",
+        "float8_e4m3": "fp8_e4_t",
+        "float8_e5m2": "fp8_e5_t",
         "float8_e4m3fnuz": "fp8_e4_t",
         "e4m3fnuz_float8": "fp8_e4_t",
         "float64": "double",
@@ -781,9 +971,9 @@ class TLHIPSourceWrapper(TLCUDASourceWrapper):
                  scheduled_ir_module: IRModule,
                  source: str,
                  target: Target,
-                 device_mod: Optional[IRModule] = None,
-                 host_mod: Optional[IRModule] = None,
-                 pass_configs: Optional[Dict[str, Any]] = None):
+                 device_mod: IRModule | None = None,
+                 host_mod: IRModule | None = None,
+                 pass_configs: dict[str, Any] | None = None):
         super().__init__(scheduled_ir_module, source, target, device_mod, host_mod, pass_configs)
 
     def get_init_func(self):
@@ -799,11 +989,11 @@ class TLHIPSourceWrapper(TLCUDASourceWrapper):
         init_funcs = PREDEF_INIT_FUNC.format(call_str)
         return init_funcs
 
-    def get_stream_type(self) -> Dict[str, str]:
+    def get_stream_type(self) -> dict[str, str]:
         return {"name": "stream=hipStreamDefault", "type": "hipStream_t"}
 
 
-class TLCPUSourceWrapper(object):
+class TLCPUSourceWrapper:
     _TYPE_MAP = {
         "float32": "float",
         "float16": "half",
@@ -829,29 +1019,29 @@ class TLCPUSourceWrapper(object):
     """)
 
     backend = "tl"
-    device_mod: Optional[IRModule] = None
-    host_mod: Optional[IRModule] = None
-    pass_configs: Optional[Dict[str, Any]] = None
+    device_mod: IRModule | None = None
+    host_mod: IRModule | None = None
+    pass_configs: dict[str, Any] | None = None
 
     def __init__(self,
                  scheduled_ir_module: IRModule,
                  source: str,
                  target: Target,
-                 device_mod: Optional[IRModule] = None,
-                 host_mod: Optional[IRModule] = None,
-                 pass_configs: Optional[Dict[str, Any]] = None):
+                 device_mod: IRModule | None = None,
+                 host_mod: IRModule | None = None,
+                 pass_configs: dict[str, Any] | None = None):
         self.mod = scheduled_ir_module
         self.target = target
         self.source = source
         self.device_mod = device_mod
         self.host_mod = host_mod
         self.pass_configs = pass_configs
-        self.function_names: Optional[str] = None
-        self.dynamic_smem_buf: Optional[int] = None
+        self.function_names: str | None = None
+        self.dynamic_smem_buf: int | None = None
         self.parse_source_information()
-        self.srcpath: Optional[str] = None
-        self.libpath: Optional[str] = None
-        self.lib_code: Optional[str] = self.update_lib_code(source)
+        self.srcpath: str | None = None
+        self.libpath: str | None = None
+        self.lib_code: str | None = self.update_lib_code(source)
 
     def create_call_func(self, code, function_informations):
         # Extract the set of dynamic symbolic names used in the primary function
@@ -901,7 +1091,7 @@ class TLCPUSourceWrapper(object):
             index = code.index("{", index)
 
             call_args = ", ".join(func_call_args(declaration, function_args))
-            _call_str += "{}({})".format(function_name, call_args)
+            _call_str += f"{function_name}({call_args})"
 
         # Wrap the kernel dispatch logic in an external C function
         host_func = self.CALL_PREFIX.format(def_args, _call_str)
@@ -922,7 +1112,7 @@ class TLCPUSourceWrapper(object):
 
     def get_dynamic_symbolic_set(self, prim_func):
         # Determine the set of dynamic symbols used in the function
-        dynamic_symbolic_set: List[str] = []
+        dynamic_symbolic_set: list[str] = []
         for param in prim_func.params:
             if param in prim_func.buffer_map:
                 buffer = prim_func.buffer_map[param]
@@ -970,15 +1160,37 @@ class TLCPUSourceWrapper(object):
             raise ValueError("Cannot find primary function in the module.")
 
 
+class TLMetalSourceWrapper:
+
+    def __init__(self,
+                 scheduled_ir_module: IRModule,
+                 source: str,
+                 target: Target,
+                 device_mod: IRModule | None = None,
+                 host_mod: IRModule | None = None,
+                 pass_configs: dict[str, Any] | None = None):
+        self.mod = scheduled_ir_module
+        self.target = target
+        self.source = source
+        self.pass_configs = pass_configs
+        self.device_mod = device_mod
+        self.host_mod = host_mod
+        self.lib_code = self.update_lib_code(source)
+
+    def update_lib_code(self, code: str):
+        self.lib_code = code
+        return self.lib_code
+
+
 class TLWrapper(BaseWrapper):
     """
     A wrapper class for the TileLang backend.
     """
-    device_mod: Optional[IRModule] = None
-    host_mod: Optional[IRModule] = None
-    pass_configs: Optional[Dict[str, Any]] = None
-    target: Optional[Target] = None
-    lib: Optional[object] = None
+    device_mod: IRModule | None = None
+    host_mod: IRModule | None = None
+    pass_configs: dict[str, Any] | None = None
+    target: Target | None = None
+    lib: object | None = None
 
     def __init__(self, target: Target):
         super().__init__()
@@ -990,7 +1202,7 @@ class TLWrapper(BaseWrapper):
     def assign_optimized_module(self, scheduled_ir_module: IRModule):
         self.scheduled_ir_module = scheduled_ir_module
 
-    def assign_pass_configs(self, pass_configs: Dict[str, Any]):
+    def assign_pass_configs(self, pass_configs: dict[str, Any]):
         self.pass_configs = pass_configs
 
     def assign_host_module(self, host_mod: IRModule):
@@ -1008,6 +1220,8 @@ class TLWrapper(BaseWrapper):
             wrapper_class = TLHIPSourceWrapper
         elif is_cpu_target(self.target):
             wrapper_class = TLCPUSourceWrapper
+        elif is_metal_target(self.target):
+            wrapper_class = TLMetalSourceWrapper
         else:
             raise ValueError(f"Unsupported platform: {self.arch.platform}")
         wrapper = wrapper_class(
diff --git a/tilelang/jit/env.py b/tilelang/jit/env.py
index 0870a66a..6af7adc7 100644
--- a/tilelang/jit/env.py
+++ b/tilelang/jit/env.py
@@ -17,7 +17,7 @@
 # This file is modified from the original version,
 # which is part of the flashinfer project
 # (https://github.com/flashinfer-ai/flashinfer).
-"""Library information. This is a standalone file that can be used to get various info. 
+"""Library information. This is a standalone file that can be used to get various info.
 Modified from flashinfer
 """
 
@@ -36,11 +36,7 @@ def _get_workspace_dir_name() -> pathlib.Path:
 
         target = determine_target(return_object=True)
         # create tmp source file for torch cpp extension
-        compute_version = "".join(nvcc.get_target_compute_version(target).split("."))
-        # set TORCH_CUDA_ARCH_LIST
-        major = compute_version[0]
-        minor = compute_version[1]
-        arch = f"{major}_{minor}"
+        arch = nvcc.get_target_arch(nvcc.get_target_compute_version(target))
     except Exception:
         arch = "noarch"
     # e.g.: $HOME/.cache/tilelang/75_80_89_90/
diff --git a/tilelang/jit/kernel.py b/tilelang/jit/kernel.py
index fcc24831..4e6e3208 100644
--- a/tilelang/jit/kernel.py
+++ b/tilelang/jit/kernel.py
@@ -1,18 +1,26 @@
-from typing import Any, Callable, Dict, List, Literal, Optional, Union
+from __future__ import annotations
+from typing import Any, Callable, Literal
 
+from tilelang.jit.adapter.utils import is_metal_target
 from tvm.target import Target
 from tvm.tir import PrimFunc
 
 import tilelang
-from tilelang import tvm as tvm
+from tilelang import tvm
+from tilelang import env
 from tilelang.engine.param import CompiledArtifact, KernelParam
 from tilelang.jit.adapter import (BaseKernelAdapter, CtypesKernelAdapter, CythonKernelAdapter,
-                                  NVRTCKernelAdapter, TorchDLPackKernelAdapter)
+                                  NVRTCKernelAdapter, TorchDLPackKernelAdapter, MetalKernelAdapter)
 from tilelang.profiler import Profiler, TensorSupplyType
-from tilelang.utils.target import AVALIABLE_TARGETS, determine_target
+from tilelang.utils.target import determine_target
+from tilelang.utils.allocator import BaseAllocator
+import ctypes
+import logging
 
+logger = logging.getLogger(__name__)
 
-class JITKernel(object):
+
+class JITKernel:
     """
     A wrapper class for compiling and invoking TileLang (TVM TIR) functions as PyTorch-compatible functions.
 
@@ -32,19 +40,20 @@ class JITKernel(object):
 
     # tuner result
     latency: float = None
-    config: Dict[str, Any] = None
+    config: dict[str, Any] = None
     ref_latency: float = None
 
     def __init__(
         self,
         func: PrimFunc = None,
-        out_idx: Union[List[int], int] = None,
+        out_idx: list[int] | int = None,
         execution_backend: Literal["dlpack", "ctypes", "cython", "nvrtc"] = "cython",
-        target: Union[str, Target] = "auto",
-        target_host: Union[str, Target] = None,
+        target: str | Target = "auto",
+        target_host: str | Target = None,
         verbose: bool = False,
-        pass_configs: Optional[Dict[str, Any]] = None,
+        pass_configs: dict[str, Any] | None = None,
         from_database: bool = False,
+        compile_flags: list[str] | None = None,
     ):
         """
         Initializes a TorchFunction instance.
@@ -65,11 +74,7 @@ class JITKernel(object):
             Whether to enable verbose output (default: False).
         pass_configs : dict, optional
             Additional keyword arguments to pass to the Compiler PassContext.
-            Available options:
-                "tir.disable_vectorize": bool, default: False
-                "tl.disable_tma_lower": bool, default: False
-                "tl.disable_dynamic_tail_split": bool, default: False
-                "tl.dynamic_vectorize_size_bits": int, default: 128
+            Refer to `tilelang.PassConfigKey` for supported options.
         from_database : bool, optional
             Whether to create a TorchFunction from a database.
         """
@@ -82,13 +87,10 @@ class JITKernel(object):
             pass_configs = {}
         self.pass_configs = pass_configs
 
-        # If the target is specified as a string, validate it and convert it to a TVM Target.
-        if isinstance(target, str):
-            assert target in AVALIABLE_TARGETS, f"Invalid target: {target}"
-            target = determine_target(target)
+        self.compile_flags = compile_flags
 
-        # Ensure the target is always a TVM Target object.
-        self.target = Target(target)
+        # Ensure the target is always a valid TVM Target object.
+        self.target = determine_target(target, return_object=True)
 
         # Validate the execution backend.
         assert execution_backend in [
@@ -96,6 +98,7 @@ class JITKernel(object):
             "ctypes",
             "cython",
             "nvrtc",
+            "torch",
         ], f"Invalid execution backend. {execution_backend}"
         if execution_backend == "cython":
             from tilelang.contrib.cc import get_cplus_compiler
@@ -107,12 +110,27 @@ class JITKernel(object):
         if from_database:
             return
 
+        # Print log on compilation starts
+        # NOTE(Chenggang): printing could let the training/inference framework easier to know
+        # whether the communication timeout is from compilation
+        if env.is_print_on_compilation_enabled():
+            # assert func must have "global_symbol"
+            func_name = func.attrs.get("global_symbol")
+            assert func_name is not None, "func must have global_symbol"
+            logger.info(f"TileLang begins to compile kernel `{func_name}` with `{out_idx=}`")
+
         # Compile the TileLang function and create a kernel adapter for execution.
         adapter = self._compile_and_create_adapter(func, out_idx)
 
+        if env.is_print_on_compilation_enabled():
+            func_name = func.attrs.get("global_symbol")
+            assert func_name is not None, "func must have global_symbol"
+            logger.info(f"TileLang completes to compile kernel `{func_name}`")
+
         # The adapter's function is assigned as the callable function for this instance.
         self.adapter = adapter
         self.torch_function = adapter.func
+        self.allocator = None
 
     @classmethod
     def from_database(
@@ -120,12 +138,13 @@ class JITKernel(object):
         func: PrimFunc,
         kernel_global_source: str,
         kernel_lib_path: str,
-        params: List[KernelParam],
-        target: Union[str, Target],
-        target_host: Union[str, Target],
-        out_idx: Union[List[int], int],
+        params: list[KernelParam],
+        target: str | Target,
+        target_host: str | Target,
+        out_idx: list[int] | int,
         execution_backend: Literal["dlpack", "ctypes", "cython", "nvrtc"],
-        pass_configs: Optional[Dict[str, Any]] = None,
+        pass_configs: dict[str, Any] | None = None,
+        compile_flags: list[str] | None = None,
     ):
         """
         Alternative constructor to create a TorchFunction directly from a database.
@@ -138,6 +157,7 @@ class JITKernel(object):
             target_host=target_host,
             pass_configs=pass_configs,
             from_database=True,
+            compile_flags=compile_flags,
         )
 
         instance.adapter = instance._create_adapter_from_database(
@@ -148,6 +168,7 @@ class JITKernel(object):
             kernel_global_source=kernel_global_source,
             kernel_lib_path=kernel_lib_path,
             pass_configs=pass_configs,
+            compile_flags=compile_flags,
         )
         instance.torch_function = instance.adapter.func
         return instance
@@ -171,7 +192,7 @@ class JITKernel(object):
         return self.torch_function(*args, **kwds)
 
     def _compile_and_create_adapter(self, tilelang_func: PrimFunc,
-                                    out_idx: List[int]) -> BaseKernelAdapter:
+                                    out_idx: list[int]) -> BaseKernelAdapter:
         """
         Compiles the given TileLang PrimFunc using TVM and creates a kernel adapter.
 
@@ -192,6 +213,8 @@ class JITKernel(object):
         execution_backend = self.execution_backend
         pass_configs = self.pass_configs
 
+        compile_flags = self.compile_flags
+
         # Compile the function with TVM, optimizing with shared memory lowering.
         enable_host_codegen = execution_backend == "dlpack"
         enable_device_compile = execution_backend == "dlpack"
@@ -224,6 +247,7 @@ class JITKernel(object):
                 kernel_global_source=artifact.kernel_source,
                 verbose=verbose,
                 pass_configs=pass_configs,
+                compile_flags=compile_flags,
             )
         elif execution_backend == "cython":
             adapter = CythonKernelAdapter(
@@ -236,6 +260,7 @@ class JITKernel(object):
                 kernel_global_source=artifact.kernel_source,
                 verbose=verbose,
                 pass_configs=pass_configs,
+                compile_flags=compile_flags,
             )
         elif execution_backend == "nvrtc":
             adapter = NVRTCKernelAdapter(
@@ -248,6 +273,21 @@ class JITKernel(object):
                 kernel_global_source=artifact.kernel_source,
                 verbose=verbose,
                 pass_configs=pass_configs,
+                compile_flags=compile_flags,
+            )
+        elif execution_backend == "torch":
+            assert is_metal_target(target)
+            adapter = MetalKernelAdapter(
+                params=artifact.params,
+                result_idx=out_idx,
+                # target=target,
+                func_or_mod=tilelang_func,
+                # host_mod=artifact.host_mod,
+                device_mod=artifact.device_mod,
+                kernel_global_source=artifact.kernel_source,
+                verbose=verbose,
+                # pass_configs=pass_configs,
+                # compile_flags=compile_flags,
             )
         else:
             # Handle invalid backend.
@@ -255,16 +295,15 @@ class JITKernel(object):
 
         return adapter
 
-    def _create_adapter_from_database(
-        self,
-        params: List[KernelParam],
-        result_idx: Union[List[int], int],
-        target: Union[str, Target],
-        func_or_mod: Union[PrimFunc, tvm.runtime.Module],
-        kernel_global_source: str,
-        kernel_lib_path: str,
-        pass_configs: Optional[Dict[str, Any]] = None,
-    ) -> BaseKernelAdapter:
+    def _create_adapter_from_database(self,
+                                      params: list[KernelParam],
+                                      result_idx: list[int] | int,
+                                      target: str | Target,
+                                      func_or_mod: PrimFunc | tvm.runtime.Module,
+                                      kernel_global_source: str,
+                                      kernel_lib_path: str,
+                                      pass_configs: dict[str, Any] | None = None,
+                                      compile_flags: list[str] | None = None) -> BaseKernelAdapter:
         target = self.target
         execution_backend = self.execution_backend
 
@@ -280,6 +319,7 @@ class JITKernel(object):
                 kernel_global_source=kernel_global_source,
                 kernel_lib_path=kernel_lib_path,
                 pass_configs=pass_configs,
+                compile_flags=compile_flags,
             )
         elif execution_backend == "cython":
             adapter = CythonKernelAdapter.from_database(
@@ -300,6 +340,7 @@ class JITKernel(object):
                 kernel_global_source=kernel_global_source,
                 kernel_lib_path=kernel_lib_path,
                 pass_configs=pass_configs,
+                compile_flags=compile_flags,
             )
         else:
             # Handle invalid backend.
@@ -363,11 +404,24 @@ class JITKernel(object):
         """
         return str(self.artifact.host_mod)
 
-    def run_once(self, func: Optional[Callable] = None) -> None:
+    def initialize(
+        self,
+        allocator: BaseAllocator,
+        stream: int = None,
+    ):
+        assert allocator.initialized(), "Allocator is not initialized"
+        result = self.adapter.lib.init_table(
+            ctypes.c_void_p(allocator.table.data_ptr()), allocator.table_size,
+            ctypes.c_void_p(stream) if stream is not None else ctypes.c_void_p(0))
+        if result != 0:
+            error_msg = self.adapter.lib.get_last_error().decode('utf-8')
+            raise RuntimeError(f"Initialization failed: {error_msg}")
+
+    def run_once(self, func: Callable | None = None) -> None:
         return self.get_profiler().run_once(func)
 
-    def update_tuner_result(self, latency: float, config: Dict[str, Any],
-                            ref_latency: float) -> "JITKernel":
+    def update_tuner_result(self, latency: float, config: dict[str, Any],
+                            ref_latency: float) -> JITKernel:
         """
         Updates the tuning results for this kernel.
 
@@ -390,7 +444,7 @@ class JITKernel(object):
 
         return self
 
-    def get_tuner_result(self) -> Dict[str, Any]:
+    def get_tuner_result(self) -> dict[str, Any]:
         """
         Gets the tuning results for this kernel.
 
@@ -412,11 +466,11 @@ class JITKernel(object):
         }
 
     @property
-    def out_idx(self) -> List[int]:
+    def out_idx(self) -> list[int]:
         return self.adapter.result_idx
 
     @property
-    def params(self) -> List[KernelParam]:
+    def params(self) -> list[KernelParam]:
         return self.artifact.params if self.artifact else self.adapter.params
 
     @property
