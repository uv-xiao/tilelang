diff --git a/src/op/atomic_add.cc b/src/op/atomic_add.cc
new file mode 100644
index 00000000..31c5bfb4
--- /dev/null
+++ b/src/op/atomic_add.cc
@@ -0,0 +1,555 @@
+/*!
+ * \file tl/op/atomic_add.cc
+ *
+ * Define element-wise operators.
+ */
+
+#include "./atomic_add.h"
+#include "./region.h"
+#include <tvm/tir/builtin.h>
+#include <tvm/tir/op.h>
+#include <tvm/tir/op_attr_types.h>
+
+#include "../target/utils.h"
+#include "../transform/atomicadd_vectorize.h"
+#include "../transform/common/loop_fusion_utils.h"
+#include "../transform/common/loop_parallel_transform_utils.h"
+#include "../transform/loop_partition.h"
+#include "builtin.h"
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+/**
+ * @brief Construct an AtomicAdd operator from call arguments and a buffer map.
+ *
+ * Builds the internal AtomicAddNode, extracts the source and destination
+ * regions and their backing Buffers from the first two call-style expressions
+ * in `args` (via RegionOp), and stores them along with their ranges. If a third
+ * argument is provided, it is interpreted as an integer immediate and stored as
+ * the node's coalesced width.
+ *
+ * @param args Call-style PrimExprs where:
+ *             - args[0] is the source region call,
+ *             - args[1] is the destination region call,
+ *             - args[2] (optional) is an IntImm specifying coalesced width.
+ * @param vmap Mapping from buffers used by RegionOp to concrete Buffer objects.
+ *
+ * Notes:
+ * - The constructor checks that args[0] and args[1] are CallNodes.
+ * - The constructed node is stored in this->data_.
+ */
+AtomicAdd::AtomicAdd(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<AtomicAddNode> node = make_object<AtomicAddNode>();
+  Array<Range> rgs[2];
+  Buffer bf[2];
+  for (int i = 0; i < 2; i++) {
+    auto expr = args[i];
+    auto call = expr.as<CallNode>();
+    ICHECK(call);
+    auto region = RegionOp(call->args, vmap);
+    rgs[i] = region->GetRanges();
+    bf[i] = region->GetBuffer();
+  }
+  std::tie(node->src, node->dst) = std::tie(bf[0], bf[1]);
+  std::tie(node->src_range, node->dst_range) = std::tie(rgs[0], rgs[1]);
+  if (args.size() >= 3) {
+    node->use_tma = Downcast<IntImm>(args[2]);
+  }
+  node->memory_order = IntImm(0);
+  if (args.size() >= 4) {
+    node->memory_order = Downcast<IntImm>(args[3]);
+  }
+  if (args.size() >= 5) {
+    node->coalesced_width = Downcast<IntImm>(args[4]);
+  }
+  data_ = std::move(node);
+}
+
+/**
+ * @brief Create a deep copy of this AtomicAdd node wrapped as a TileOperator.
+ *
+ * Produces a new AtomicAddNode object copied from this node. If this node has
+ * an associated ParallelOp (par_op_), the parallel op is cloned and attached to
+ * the new node so the cloned operator preserves parallelization state.
+ *
+ * @return TileOperator A TileOperator owning the cloned AtomicAddNode.
+ */
+TileOperator AtomicAddNode::Clone() const {
+  auto op = make_object<AtomicAddNode>(*this);
+  if (par_op_.defined()) {
+    op->par_op_ = Downcast<ParallelOp>(par_op_->Clone());
+  }
+  return AtomicAdd(op);
+}
+
+/**
+ * @brief Create data-parallel iteration variables for non-singleton dimensions
+ * of the source.
+ *
+ * Constructs an Array of IterVar corresponding to each dimension in `src_range`
+ * whose extent is not equal to 1. Each IterVar has domain Range(0, extent), a
+ * Var named sequentially ("i", "j", "k", ...) with the same dtype as the
+ * extent, and type IterVarType::kDataPar. The ordering of returned itervars
+ * matches the order of dimensions in `src_range`.
+ *
+ * @return Array<IterVar> Iteration variables for all non-singleton extents in
+ * `src_range`.
+ */
+Array<IterVar> AtomicAddNode::MakeIterVars() const {
+  Array<IterVar> loop_vars;
+  size_t idx = 0;
+  for (size_t i = 0; i < src_range.size(); i++) {
+    if (is_one(src_range[i]->extent))
+      continue;
+    Var var = Var(std::string{char('i' + idx)}, src_range[i]->extent->dtype);
+    idx++;
+    loop_vars.push_back(
+        {Range(0, src_range[i]->extent), var, IterVarType::kDataPar});
+  }
+  return loop_vars;
+}
+
+// ivs: itervars returned by MakeIterVars()
+/**
+ * @brief Build index expressions for either source or destination from loop
+ * iter vars.
+ *
+ * Given a list of iteration variables that correspond to the non-singleton
+ * extents of the selected region (source when src_dst == 0, destination when
+ * src_dst == 1), return an array of index expressions matching the full rank of
+ * that region. For dimensions with extent == 1, the corresponding index is the
+ * range's minimum; otherwise the index is `min + ivar`.
+ *
+ * @param ivs Iteration variables in order for all non-singleton dimensions of
+ * the chosen region.
+ * @param src_dst Selects which region to index: 0 for source (src_range), 1 for
+ * destination (dst_range).
+ * @return Array<PrimExpr> Index expressions for every dimension of the selected
+ * region, in original dimension order.
+ *
+ * @note The function checks that the number of provided iter vars equals the
+ * number of non-singleton extents; it will abort (ICHECK) if they differ.
+ */
+Array<PrimExpr> AtomicAddNode::MakeIndices(const Array<IterVar> &ivs,
+                                           int src_dst) const {
+  Array<PrimExpr> indices;
+  Array<Range> ranges = src_dst == 0 ? src_range : dst_range;
+  size_t idx = 0;
+  for (size_t i = 0; i < ranges.size(); i++) {
+    if (is_one(ranges[i]->extent))
+      indices.push_back(ranges[i]->min);
+    else {
+      indices.push_back(ranges[i]->min + ivs[idx]->var);
+      idx++;
+    }
+  }
+  ICHECK(idx == ivs.size())
+      << "idx = " << idx << ", ivs.size() = " << ivs.size()
+      << "src name = " << src->name << ", dst name = " << dst->name;
+  return indices;
+}
+
+std::pair<Array<PrimExpr>, PrimExpr>
+AtomicAddNode::ReturnIndicesAndSize(int src_dst) const {
+  Array<PrimExpr> indices;
+  Array<Range> ranges = src_dst == 0 ? src_range : dst_range;
+  PrimExpr size = 1;
+  for (size_t i = 0; i < ranges.size(); i++) {
+    indices.push_back(ranges[i]->min);
+    size *= ranges[i]->extent;
+  }
+  return {indices, size};
+}
+
+/**
+ * @brief Build a combined bound-check predicate for indexed access.
+ *
+ * Constructs an AND'd predicate ensuring each non-singleton index (derived from
+ * `ivs`) stays within [0, extent) for the selected operand (source when
+ * `src_dst==0`, destination otherwise). For each non-unit Range in the chosen
+ * range list this produces two conditions:
+ *   - range.min + iv >= 0
+ *   - range.min + iv < extent
+ *
+ * Conditions that the analyzer can prove (with symbolic bounds) are omitted.
+ * If no uncertain conditions remain, an empty PrimExpr is returned.
+ *
+ * Note: the function ICHECKs that `extents.size()` equals the number of ranges
+ * for the selected operand.
+ *
+ * @param ivs Iteration variables corresponding to non-singleton extents (order
+ *            matches the non-unit ranges of the chosen operand).
+ * @param extents Per-dimension upper bounds to check against; must have the
+ *                same size as the selected range list.
+ * @param src_dst Selects which ranges to validate: 0 => `src_range`, else
+ *                `dst_range`.
+ * @return PrimExpr A conjunction of remaining (non-provable) bounds checks, or
+ *         an empty PrimExpr when no checks are required.
+ */
+PrimExpr AtomicAddNode::MakePredicate(arith::Analyzer *analyzer,
+                                      const Array<IterVar> &ivs,
+                                      Array<PrimExpr> extents,
+                                      int src_dst) const {
+  Array<Range> ranges = src_dst == 0 ? src_range : dst_range;
+  Array<PrimExpr> cond_list;
+  ICHECK(extents.size() == ranges.size()) << extents << " " << ranges;
+  size_t idx = 0;
+  for (size_t i = 0; i < ranges.size(); i++) {
+    if (is_one(ranges[i]->extent))
+      continue;
+    PrimExpr cond = ranges[i]->min + ivs[idx]->var < extents[i];
+    if (!analyzer->CanProve(cond, arith::ProofStrength::kSymbolicBound)) {
+      cond_list.push_back(cond);
+    }
+    cond = ranges[i]->min + ivs[idx]->var >= 0;
+    if (!analyzer->CanProve(cond, arith::ProofStrength::kSymbolicBound)) {
+      cond_list.push_back(cond);
+    }
+    idx++;
+  }
+  if (cond_list.empty())
+    return {};
+  else {
+    PrimExpr cond = cond_list[0];
+    for (size_t i = 1; i < cond_list.size(); i++)
+      cond = And(cond, cond_list[i]);
+    return cond;
+  }
+}
+
+/**
+ * @brief Build a SIMT-style loop nest that performs element-wise atomic
+ * additions from src to dst.
+ *
+ * Constructs a nested loop (parallelized per iter var) that loads a value from
+ * the source buffer, optionally casts it to the destination dtype, and performs
+ * an extern atomic add into the destination buffer address. For scalar
+ * (zero-dimensional) operations a trivial serial For with a single BufferStore
+ * is returned.
+ *
+ * The method:
+ * - Creates iter vars for all non-singleton extents and binds them into the
+ * provided analyzer.
+ * - Validates loop variable counts against src/dst ranges (ICHECK on mismatch).
+ * - Computes indexed accesses and emits optional bound predicates;
+ * out-of-bounds accesses are masked to zero when predicates are uncertain.
+ * - Emits an extern `call_extern("AtomicAdd", address_of(dst_value),
+ * src_value)` call wrapped in an Evaluate statement.
+ * - Wraps the body with a parallel For at each loop level. If `coalesced_width`
+ * is defined it is attached as the "coalesced_width" annotation on each loop.
+ *
+ * Note: This function mutates the analyzer binding state by binding loop
+ * variables and may fail via ICHECK if internal assumptions about shapes are
+ * violated.
+ *
+ * @return A nested For loop (parallel loops) implementing the atomic-add
+ * kernel. For scalar cases a serial For of extent 1 is returned.
+ */
+For AtomicAddNode::MakeSIMTLoop(arith::Analyzer *analyzer) const {
+  Array<IterVar> loop_vars = MakeIterVars();
+  bool is_scalar = loop_vars.empty();
+  if (is_scalar) {
+    return For(Var("i"), 0, 1, ForKind::kSerial,
+               BufferStore(dst, BufferLoad(src, {0}), {0}));
+  }
+
+  for (const auto &iv : loop_vars)
+    analyzer->Bind(iv->var, iv->dom);
+
+  ICHECK(loop_vars.size() <= src_range.size())
+      << "loop_vars.size() = " << loop_vars.size()
+      << ", src_range.size() = " << src_range.size() << ", src = " << src->name
+      << ", dst = " << dst->name;
+
+  ICHECK(loop_vars.size() <= dst_range.size())
+      << "loop_vars.size() = " << loop_vars.size()
+      << ", dst_range.size() = " << dst_range.size() << ", src = " << src->name
+      << ", dst = " << dst->name;
+
+  Array<PrimExpr> src_indices = MakeIndices(loop_vars, 0);
+  Array<PrimExpr> dst_indices = MakeIndices(loop_vars, 1);
+
+  PrimExpr src_predicate = MakePredicate(analyzer, loop_vars, src->shape, 0);
+  PrimExpr dst_predicate = MakePredicate(analyzer, loop_vars, dst->shape, 1);
+
+  Array<PrimExpr> new_args;
+
+  PrimExpr src_value = BufferLoad(src, src_indices);
+  if (src->dtype != dst->dtype)
+    src_value = Cast(dst->dtype, src_value);
+  if (src_predicate.defined())
+    src_value = if_then_else(src_predicate, src_value, make_zero(dst->dtype));
+
+  PrimExpr dst_value = BufferLoad(dst, dst_indices);
+  if (dst_predicate.defined())
+    dst_value = if_then_else(dst_predicate, dst_value, make_zero(dst->dtype));
+
+  new_args.push_back(dst_value);
+  new_args.push_back(src_value);
+  new_args.push_back(memory_order);
+
+  Call atomicadd_call =
+      tvm::tir::Call(dst->dtype, atomicadd_elem_op(), new_args);
+
+  Stmt body = tvm::tir::Evaluate(atomicadd_call);
+
+  for (int i = loop_vars.size() - 1; i >= 0; i--) {
+    Map<String, ObjectRef> annotations = {};
+    if (coalesced_width.defined()) {
+      annotations.Set("coalesced_width", coalesced_width);
+    }
+
+    body = For(loop_vars[i]->var, 0, loop_vars[i]->dom->extent,
+               ForKind::kParallel, body, std::nullopt, annotations);
+  }
+  return Downcast<For>(body);
+}
+
+/**
+ * @brief Infer and return the layout map for the atomic add operator.
+ *
+ * Constructs a cached ParallelOp (by building the SIMT loop) if not already
+ * present, validates that local.fragment layouts for src and dst match when
+ * both are provided, and then delegates layout inference to the underlying
+ * ParallelOp.
+ *
+ * @param T Layout inference inputs, including an optional mapping of buffers to
+ * layouts.
+ * @param level Inference strictness level.
+ * @return LayoutMap The inferred layout mapping for buffers used by this
+ * operator.
+ *
+ * @note This method mutates the AtomicAddNode by creating and storing a
+ * ParallelOp on first invocation.
+ * @throws If both src and dst have layouts in `local.fragment` and their
+ * fragment layouts differ, an ICHECK failure is raised with diagnostic output.
+ */
+LayoutMap AtomicAddNode::InferLayout(const LayoutInferArgs &T,
+                                     InferLevel level) const {
+  if (T.layout_map.count(src) && T.layout_map.count(dst)) {
+    if (src.scope() == "local.fragment" && dst.scope() == "local.fragment") {
+      const FragmentNode *src_layout = T.layout_map[src].as<FragmentNode>();
+      const FragmentNode *dst_layout = T.layout_map[dst].as<FragmentNode>();
+      if (src_layout && dst_layout) {
+        ICHECK(src_layout->IsEqual(dst_layout, true))
+            << "Get different layout for " << src << " and " << dst
+            << "\nLHS = " << src_layout->DebugOutput()
+            << "\nRHS = " << dst_layout->DebugOutput()
+            << "\nYou may need to use a shared memory to transform the layout";
+      }
+    }
+  }
+  return {};
+}
+
+/**
+ * @brief Lower the atomic-add top-level operator into a parallel, vectorized
+ * TIR loop.
+ *
+ * Constructs a SIMT-style loop for the atomic-add, fuses parallel loops, runs
+ * layout inference at multiple levels, partitions the root loop by the provided
+ * thread variable, vectorizes the thread loop, and returns the final
+ * (optionally predicate-guarded) statement.
+ *
+ * The lowering pipeline:
+ *  - Build the SIMT loop via MakeSIMTLoop.
+ *  - Fuse parallel loops into a single For and wrap as a ParallelOp.
+ *  - Run layout inference at kCommon, kStrict, and kFree levels using fields
+ * from `T`.
+ *  - Obtain the loop layout, partition the root loop with PartitionLoop by
+ * `T.thread_var`.
+ *  - Vectorize the partitioned thread loop via VectorizeLoop.
+ *  - If the ParallelOp produced a predicate for `T.thread_var`, return an
+ * IfThenElse that guards the vectorized loop with that predicate; otherwise
+ * return the vectorized loop.
+ *
+ * @param T Lowering context whose fields are used:
+ *   - T.target: target architecture for layout inference and lowering
+ * decisions.
+ *   - T.thread_var: the Var used to partition the outer loop for thread-level
+ * parallelism.
+ *   - T.thread_bounds: bounds associated with the thread dimension (used during
+ * partitioning).
+ *   - T.layout_map, T.buffer_remap: layout and buffer remapping inputs used
+ * during InferLayout.
+ * @param analyzer Analyzer used for symbolic reasoning during partitioning and
+ * folding (omitted from detailed param docs as a common analysis utility).
+ * @return Stmt A lowered TIR statement representing the parallelized and
+ * vectorized atomic-add.
+ */
+Stmt AtomicAddNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  Target target = T.target;
+  if (use_tma->value != 0) {
+    Array<PrimExpr> src_indices, dst_indices;
+    PrimExpr src_size, dst_size;
+    std::tie(src_indices, src_size) = ReturnIndicesAndSize(0);
+    std::tie(dst_indices, dst_size) = ReturnIndicesAndSize(1);
+    ICHECK(analyzer->CanProveEqual(src_size, dst_size))
+        << "src_size = " << src_size << ", dst_size = " << dst_size;
+    BufferLoad src_node = BufferLoad(src, src_indices);
+    BufferLoad dst_node = BufferLoad(dst, dst_indices);
+    Call address_of_src =
+        Call(DataType::Handle(), builtin::address_of(), {src_node});
+    Call address_of_dst =
+        Call(DataType::Handle(), builtin::address_of(), {dst_node});
+
+    int need_reduce = 1;
+    int eviction_policy = 0;
+    auto body = Evaluate(Call(DataType::Handle(), tma_store(),
+                              {address_of_src, address_of_dst,
+                               ceildiv(src_size * src->dtype.bits(), 8),
+                               need_reduce, eviction_policy}));
+    return IfThenElse(EQ(T.thread_var, T.thread_bounds->min), body);
+  }
+  auto simt_loop = MakeSIMTLoop(analyzer);
+  auto fused_loop = Downcast<For>(ParallelLoopFuser::Fuse(simt_loop));
+  auto transformed_loop =
+      Downcast<For>(ParallelLoopTransformer::Substitute(fused_loop));
+
+  auto GetArchInt = [&](const Target &tgt) -> int {
+    int arch_int = 0;
+    if (auto s = tgt->GetAttr<String>("arch")) {
+      std::string arch = s.value();
+      if (arch.rfind("sm_", 0) == 0)
+        arch_int = std::stoi(arch.substr(3));
+    }
+    return arch_int;
+  };
+
+  struct AtomicLoopNestCollector : tir::StmtExprVisitor {
+    Array<IterVar> loop_vars;
+    Map<Buffer, Array<PrimExpr>> indice_map;
+    std::unordered_set<Buffer, ObjectPtrHash, ObjectPtrEqual> writes;
+    arith::Analyzer analyzer;
+
+    void Run(const Stmt &s) { StmtExprVisitor::VisitStmt(s); }
+
+    void VisitStmt_(const ForNode *op) final {
+      if (op->kind == ForKind::kParallel) {
+        loop_vars.push_back(IterVar(Range(op->min, op->extent), op->loop_var,
+                                    IterVarType::kDataPar));
+      }
+      analyzer.Bind(op->loop_var, Range::FromMinExtent(op->min, op->extent));
+      StmtExprVisitor::VisitStmt_(op);
+    }
+    void VisitStmt_(const BufferStoreNode *op) final {
+      if (op->buffer.scope() == "local.fragment") {
+        indice_map.Set(op->buffer, op->indices);
+        writes.insert(op->buffer);
+      }
+      StmtExprVisitor::VisitStmt_(op);
+    }
+    void VisitExpr_(const BufferLoadNode *op) final {
+      if (op->buffer.scope() == "local.fragment") {
+        indice_map.Set(op->buffer, op->indices);
+      }
+      StmtExprVisitor::VisitExpr_(op);
+    }
+  };
+
+  auto ComputeLoopLayoutFromBuffer =
+      [&](const Buffer &buf, const Array<PrimExpr> &indices,
+          const LayoutMap &layout_map, const Range &thread_bounds,
+          const Array<IterVar> &loop_vars) -> Fragment {
+    Fragment src = layout_map[buf].as<Fragment>().value();
+    Var rep;
+    auto rep_iter =
+        IterVar(Range(0, src->ReplicateExtent()), rep, IterVarType::kDataPar);
+    PrimExpr fth = src->ForwardThread(indices, rep);
+    fth = analyzer->Simplify(fth);
+    Fragment out = Fragment(loop_vars, /*forward_index=*/{}, fth, rep_iter)
+                       ->BindThreadRange(thread_bounds);
+    return out;
+  };
+
+  struct AtomicInferResult {
+    Fragment loop_layout;
+    Optional<PrimExpr> predicate;
+  };
+
+  auto AtomicAddInferLayout =
+      [&](const For &loop, const LayoutInferArgs &args) -> AtomicInferResult {
+    AtomicLoopNestCollector C;
+    C.Run(loop);
+    Optional<Buffer> read_src;
+    int best_rank = -1;
+    for (auto kv : C.indice_map) {
+      const Buffer &buf = kv.first;
+      if (buf.scope() != "local.fragment")
+        continue;
+      if (!args.layout_map.count(buf))
+        continue;
+      int rank = static_cast<int>(kv.second.size());
+      if (rank > best_rank) {
+        best_rank = rank;
+        read_src = buf;
+      }
+    }
+    AtomicAddVectorizePlanner planner;
+    int sm = GetArchInt(target);
+    auto plan = planner.Plan(loop, sm);
+    int vec = std::max(plan.vector_size, 1);
+    if (auto cw = loop->annotations.Get("coalesced_width")) {
+      if (const auto *imm = cw->as<IntImmNode>()) {
+        int expected = imm->value;
+        ICHECK_GT(expected, 0);
+        ICHECK(vec % expected == 0)
+            << "vector_size " << vec << " not divisible by coalesced_width "
+            << expected;
+        vec = expected;
+      } else {
+        LOG(FATAL) << "coalesced_width should be IntImmNode.";
+      }
+    }
+    PrimExpr total = 1;
+    for (Stmt s = loop; s.as<For>().has_value(); s = s.as<For>().value()->body)
+      total = total * s.as<For>().value()->extent;
+    PrimExpr denom = args.thread_bounds->extent * vec;
+    while (!analyzer->CanProve(floormod(total, denom) == 0) && vec > 1) {
+      vec >>= 1;
+      denom = args.thread_bounds->extent * vec;
+    }
+    if (vec < 1)
+      vec = 1;
+    Fragment loop_layout;
+    if (read_src) {
+      loop_layout = ComputeLoopLayoutFromBuffer(
+          read_src.value(), C.indice_map[read_src.value()], args.layout_map,
+          args.thread_bounds, C.loop_vars);
+    } else {
+      const For &remapped = loop;
+      loop_layout = PlanLoopPartition(remapped, vec, args.thread_bounds);
+    }
+
+    Optional<PrimExpr> pred;
+    if (plan.dynamic && plan.condition.defined()) {
+      pred = plan.condition;
+    }
+    DLOG(INFO) << "[AtomicAddInferLayout] vec=" << vec
+               << " loop_layout=" << loop_layout->DebugOutput();
+    return {loop_layout, pred};
+  };
+
+  auto ret = AtomicAddInferLayout(transformed_loop,
+                                  {T.target, T.thread_bounds, T.layout_map,
+                                   analyzer, false, T.buffer_remap});
+  Fragment loop_layout = ret.loop_layout;
+  auto thread_loop =
+      PartitionLoop(transformed_loop, T.thread_var, analyzer, loop_layout);
+  auto vectorized_thread_loop =
+      VectorizeAtomicAdd(thread_loop, GetArchInt(target));
+  return vectorized_thread_loop;
+}
+
+TIR_REGISTER_TL_OP(AtomicAdd, atomicadd)
+    .set_num_inputs(2)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TVM_FFI_STATIC_INIT_BLOCK({ AtomicAddNode::RegisterReflection(); });
+
+} // namespace tl
+} // namespace tvm
\ No newline at end of file
diff --git a/src/op/atomic_add.h b/src/op/atomic_add.h
new file mode 100644
index 00000000..ae9cc99a
--- /dev/null
+++ b/src/op/atomic_add.h
@@ -0,0 +1,96 @@
+/*!
+ * \file tl/op/atomic_add.h
+ * \brief Atomic addition operations for concurrent memory updates
+ */
+
+#ifndef TVM_TL_OP_ATOMIC_ADD_H_
+#define TVM_TL_OP_ATOMIC_ADD_H_
+
+#include "operator.h"
+#include "parallel.h"
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+/// Node class for atomic addition operations
+class AtomicAddNode : public TileOperatorNode {
+public:
+  Buffer src, dst; ///< Source and destination buffers
+  Array<Range> src_range,
+      dst_range;          ///< Access ranges for source and destination
+  IntImm use_tma;         ///< Whether to use TMA for memory operations
+  IntImm coalesced_width; ///< Width for memory coalescing optimization
+  IntImm memory_order;    ///< Memory order for atomic operations
+
+  mutable ParallelOp par_op_; ///< Associated parallel operation
+  static constexpr const char *_type_key = "tl.AtomicAdd";
+  TVM_DECLARE_FINAL_OBJECT_INFO(AtomicAddNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const;
+  LayoutMap InferLayout(const LayoutInferArgs &T, InferLevel level) const;
+
+  static const Op &Get();
+  TileOperator Clone() const;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<AtomicAddNode>()
+        .def_ro("src", &AtomicAddNode::src)
+        .def_ro("dst", &AtomicAddNode::dst)
+        .def_ro("src_range", &AtomicAddNode::src_range)
+        .def_ro("dst_range", &AtomicAddNode::dst_range)
+        .def_ro("use_tma", &AtomicAddNode::use_tma)
+        .def_ro("coalesced_width", &AtomicAddNode::coalesced_width)
+        .def_ro("memory_order", &AtomicAddNode::memory_order);
+  }
+
+  bool SEqualReduce(const AtomicAddNode *other, SEqualReducer equal) const {
+    return equal(src, other->src) && equal(dst, other->dst) &&
+           equal(src_range, other->src_range) &&
+           equal(dst_range, other->dst_range) &&
+           equal(use_tma, other->use_tma) &&
+           equal(coalesced_width, other->coalesced_width) &&
+           equal(memory_order, other->memory_order);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(src);
+    hash_reduce(dst);
+    hash_reduce(src_range);
+    hash_reduce(dst_range);
+    hash_reduce(use_tma);
+    hash_reduce(coalesced_width);
+    hash_reduce(memory_order);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+protected:
+  /// Create SIMT-style parallel loop structure
+  For MakeSIMTLoop(arith::Analyzer *analyzer) const;
+  /// Generate iteration variables for loop nest
+  Array<IterVar> MakeIterVars() const;
+  /// Generate buffer indices from iteration variables
+  Array<PrimExpr> MakeIndices(const Array<IterVar> &ivs, int src_dst) const;
+  /// Return buffer indices and size
+  std::pair<Array<PrimExpr>, PrimExpr> ReturnIndicesAndSize(int src_dst) const;
+  /// Create boundary predicate for memory safety
+  PrimExpr MakePredicate(arith::Analyzer *analyzer, const Array<IterVar> &ivs,
+                         Array<PrimExpr> extents, int src_dst) const;
+};
+
+/// Wrapper class for atomic addition operations
+class AtomicAdd : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(AtomicAdd, TileOperator, AtomicAddNode);
+  TVM_DLL AtomicAdd(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+} // namespace tl
+} // namespace tvm
+
+#endif //  TVM_TL_OP_ATOMIC_ADD_H_
\ No newline at end of file
diff --git a/src/op/builtin.cc b/src/op/builtin.cc
index c4aa81d8..18baaae3 100644
--- a/src/op/builtin.cc
+++ b/src/op/builtin.cc
@@ -20,12 +20,23 @@ TVM_REGISTER_PASS_CONFIG_OPTION(kDebugMergeSharedMemoryAllocations, Bool);
 TVM_REGISTER_PASS_CONFIG_OPTION(kDisableTMALower, Bool);
 TVM_REGISTER_PASS_CONFIG_OPTION(kDisableSafeMemoryLegalize, Bool);
 TVM_REGISTER_PASS_CONFIG_OPTION(kDisableWarpSpecialized, Bool);
+TVM_REGISTER_PASS_CONFIG_OPTION(kDisableThreadStorageSync, Bool);
 TVM_REGISTER_PASS_CONFIG_OPTION(kConfigIndexBitwidth, Integer);
 TVM_REGISTER_PASS_CONFIG_OPTION(kDisableDynamicTailSplit, Bool);
 TVM_REGISTER_PASS_CONFIG_OPTION(kDynamicAlignment, Integer);
 TVM_REGISTER_PASS_CONFIG_OPTION(kEnableAggressiveSharedMemoryMerge, Bool);
+TVM_REGISTER_PASS_CONFIG_OPTION(kDisableRDC, Bool);
+TVM_REGISTER_PASS_CONFIG_OPTION(kForceLetInline, Bool);
 TVM_REGISTER_PASS_CONFIG_OPTION(kDisableFastMath, Bool);
+TVM_REGISTER_PASS_CONFIG_OPTION(kEnableFastMath, Bool);
+TVM_REGISTER_PASS_CONFIG_OPTION(kPtxasRegisterUsageLevel, Integer);
 TVM_REGISTER_PASS_CONFIG_OPTION(kEnablePTXASVerboseOutput, Bool);
+TVM_REGISTER_PASS_CONFIG_OPTION(kDisableVectorize256, Bool);
+TVM_REGISTER_PASS_CONFIG_OPTION(kDisableWGMMA, Bool);
+TVM_REGISTER_PASS_CONFIG_OPTION(kDisableShuffleElect, Bool);
+TVM_REGISTER_PASS_CONFIG_OPTION(kStorageRewriteDetectInplace, Bool);
+
+DataType cuTensorMapType() { return DataType::UInt(8, 128); }
 
 #define TIR_DEFINE_TL_BUILTIN(OpName)                                          \
   const Op &OpName() {                                                         \
@@ -35,6 +46,60 @@ TVM_REGISTER_PASS_CONFIG_OPTION(kEnablePTXASVerboseOutput, Bool);
   TVM_REGISTER_OP("tl." #OpName)                                               \
       .set_attr<TScriptPrinterName>("TScriptPrinterName", #OpName)
 
+// fast math related op
+TIR_DEFINE_TL_BUILTIN(__exp).set_num_inputs(1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(__exp10).set_num_inputs(1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(__log).set_num_inputs(1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(__log2).set_num_inputs(1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(__log10).set_num_inputs(1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(__tan).set_num_inputs(1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(__cos).set_num_inputs(1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(__sin).set_num_inputs(1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+// high precision with IEEE-compliant
+TIR_DEFINE_TL_BUILTIN(ieee_add).set_num_inputs(3).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kPure));
+
+TIR_DEFINE_TL_BUILTIN(ieee_sub).set_num_inputs(3).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kPure));
+
+TIR_DEFINE_TL_BUILTIN(ieee_mul).set_num_inputs(3).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kPure));
+
+TIR_DEFINE_TL_BUILTIN(ieee_fmaf).set_num_inputs(4).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kPure));
+
+TIR_DEFINE_TL_BUILTIN(ieee_frcp).set_num_inputs(2).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kPure));
+
+TIR_DEFINE_TL_BUILTIN(ieee_fsqrt)
+    .set_num_inputs(2)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kPure));
+
+TIR_DEFINE_TL_BUILTIN(ieee_frsqrt)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kPure));
+
+TIR_DEFINE_TL_BUILTIN(ieee_fdiv).set_num_inputs(3).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kPure));
+
 TIR_DEFINE_TL_BUILTIN(create_list_of_mbarrier)
     .set_num_inputs(-1)
     .set_attr<TCallEffectKind>("TCallEffectKind",
@@ -66,6 +131,11 @@ TIR_DEFINE_TL_BUILTIN(tma_load_im2col)
 TIR_DEFINE_TL_BUILTIN(tma_store).set_num_inputs(-1).set_attr<TCallEffectKind>(
     "TCallEffectKind", Integer(CallEffectKind::kOpaque));
 
+TIR_DEFINE_TL_BUILTIN(ptx_fence_barrier_init)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
 TIR_DEFINE_TL_BUILTIN(mbarrier_wait_parity)
     .set_num_inputs(2)
     .set_attr<TCallEffectKind>("TCallEffectKind",
@@ -76,21 +146,46 @@ TIR_DEFINE_TL_BUILTIN(mbarrier_expect_tx)
     .set_attr<TCallEffectKind>("TCallEffectKind",
                                Integer(CallEffectKind::kOpaque));
 
-TIR_DEFINE_TL_BUILTIN(ptx_ldmatirx)
+TIR_DEFINE_TL_BUILTIN(ptx_wgmma_ss)
+    .set_num_inputs(15)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(ptx_wgmma_rs)
+    .set_num_inputs(15)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(ptx_init_tensor_memory)
+    .set_num_inputs(2)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(ptx_deallocate_tensor_memory)
+    .set_num_inputs(2)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(ptx_ldmatrix)
     .set_num_inputs(4)
     .set_attr<TCallEffectKind>("TCallEffectKind",
                                Integer(CallEffectKind::kOpaque));
 
-TIR_DEFINE_TL_BUILTIN(ptx_stmatirx)
+TIR_DEFINE_TL_BUILTIN(ptx_stmatrix)
     .set_num_inputs(-1)
     .set_attr<TCallEffectKind>("TCallEffectKind",
                                Integer(CallEffectKind::kOpaque));
 
-TIR_DEFINE_TL_BUILTIN(sync_thread_partial)
+TIR_DEFINE_TL_BUILTIN(ptx_cp_async_barrier_noinc)
     .set_num_inputs(1)
     .set_attr<TCallEffectKind>("TCallEffectKind",
                                Integer(CallEffectKind::kOpaque));
 
+TIR_DEFINE_TL_BUILTIN(copy_unrolled)
+    .set_num_inputs(4)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
 TIR_DEFINE_TL_BUILTIN(fence_proxy_async)
     .set_num_inputs(0)
     .set_attr<TCallEffectKind>("TCallEffectKind",
@@ -115,6 +210,41 @@ TIR_DEFINE_TL_BUILTIN(no_set_max_nreg)
     .set_attr<TCallEffectKind>("TCallEffectKind",
                                Integer(CallEffectKind::kOpaque));
 
+TIR_DEFINE_TL_BUILTIN(warpgroup_arrive)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(warpgroup_commit_batch)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(warpgroup_wait)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(get_lane_idx)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kPure));
+
+TIR_DEFINE_TL_BUILTIN(get_warp_idx_sync)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kPure));
+
+TIR_DEFINE_TL_BUILTIN(get_warp_idx)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kPure));
+
+TIR_DEFINE_TL_BUILTIN(get_warp_group_idx)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kPure));
+
 TIR_DEFINE_TL_BUILTIN(wait_wgmma)
     .set_num_inputs(1)
     .set_attr<TCallEffectKind>("TCallEffectKind",
@@ -123,12 +253,110 @@ TIR_DEFINE_TL_BUILTIN(wait_wgmma)
 TIR_DEFINE_TL_BUILTIN(pack_b16).set_num_inputs(2).set_attr<TCallEffectKind>(
     "TCallEffectKind", Integer(CallEffectKind::kPure));
 
-TIR_DEFINE_TL_BUILTIN(sync_grid).set_num_inputs(0).set_attr<TCallEffectKind>(
-    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+TIR_DEFINE_TL_BUILTIN(sync_grid_cg)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
 
 TIR_DEFINE_TL_BUILTIN(loop_break)
     .set_num_inputs(0)
     .set_attr<TCallEffectKind>("TCallEffectKind",
                                Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(tl_gemm).set_num_inputs(4).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(tl_gemm_sp)
+    .set_num_inputs(5)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(tvm_mfma).set_num_inputs(12).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(tvm_mfma_store)
+    .set_num_inputs(6)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(tvm_rdna_wmma)
+    .set_num_inputs(12)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(tvm_rdna_wmma_store)
+    .set_num_inputs(6)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(tl_shuffle_elect)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kPure));
+
+TIR_DEFINE_TL_BUILTIN(get_clock).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kPure));
+
+TIR_DEFINE_TL_BUILTIN(initialize_descriptor)
+    .set_num_inputs(5)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(increase_descriptor_offset)
+    .set_num_inputs(2)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(atomicadd_elem_op)
+    .set_num_inputs(3)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(atom_add).set_num_inputs(4).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(warp_reduce_sum)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(warp_reduce_max)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(warp_reduce_min)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(warp_reduce_bitand)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(warp_reduce_bitor)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(elect_one_sync)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(sync_warp).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(loop_continue)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(warp_any).set_num_inputs(2).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kPure));
+
+TIR_DEFINE_TL_BUILTIN(warp_all).set_num_inputs(2).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kPure));
 } // namespace tl
 } // namespace tvm
diff --git a/src/op/builtin.h b/src/op/builtin.h
index e368d847..b4b9bf93 100644
--- a/src/op/builtin.h
+++ b/src/op/builtin.h
@@ -7,14 +7,27 @@
 #ifndef TVM_TL_OP_BUILTIN_H_
 #define TVM_TL_OP_BUILTIN_H_
 
-#include "op.h"
+#include "operator.h"
 #include <tvm/ir/transform.h>
 
 namespace tvm {
+/*!
+ * \brief Create the TVM intrinsic that initializes a PTX fence barrier.
+ *
+ * Initializes a PTX fence-style barrier used to coordinate asynchronous memory
+ * operations (for example, TMA/TMA_STORE). Returns the Op representing this
+ * intrinsic for use in TIR lowering and code generation.
+ *
+ */
 namespace tl {
 
 namespace attr {
-static constexpr const char *kPaddingMap = "padding_map";
+static constexpr const char *kSafeValueMap = "safe_value_map";
+static constexpr const char *kWarpSpecializationScope =
+    "kWarpSpecializationScope";
+static constexpr const char *kCustomWarpSpecialization =
+    "kCustomWarpSpecialization";
+static constexpr const char *kLocalVarInit = "tl.local_var_init";
 } // namespace attr
 
 static constexpr const char *kDebugMergeSharedMemoryAllocations =
@@ -27,10 +40,18 @@ static constexpr const char *kDisableWarpSpecialized =
 static constexpr const char *kConfigIndexBitwidth = "tl.config_index_bitwidth";
 static constexpr const char *kEnableAggressiveSharedMemoryMerge =
     "tl.enable_aggressive_shared_memory_merge";
+static constexpr const char *kDisableRDC = "tl.disable_rdc";
 static constexpr const char *kDisableFastMath = "tl.disable_fast_math";
+static constexpr const char *kEnableFastMath = "tl.enable_fast_math";
+static constexpr const char *kPtxasRegisterUsageLevel =
+    "tl.ptxas_register_usage_level";
 static constexpr const char *kEnablePTXASVerboseOutput =
     "tl.enable_ptxas_verbose_output";
-
+static constexpr const char *kDisableVectorize256 = "tl.disable_vectorize_256";
+static constexpr const char *kDisableWGMMA = "tl.disable_wgmma";
+static constexpr const char *kDisableShuffleElect = "tl.disable_shuffle_elect";
+static constexpr const char *kStorageRewriteDetectInplace =
+    "tl.storage_rewrite_detect_inplace";
 /*!
  * \brief Whether to disable dynamic tail split
  *
@@ -40,6 +61,28 @@ static constexpr const char *kEnablePTXASVerboseOutput =
 static constexpr const char *kDisableDynamicTailSplit =
     "tl.disable_dynamic_tail_split";
 
+/*!
+ * \brief Whether to disable thread storage synchronization
+ *
+ * When enabled, disables the automatic insertion of thread synchronization
+ * barriers (e.g., __syncthreads()) for shared memory access coordination.
+ * This can be useful for performance optimization in cases where manual
+ * synchronization is preferred or when synchronization is not needed.
+ *
+ * kDisableThreadStorageSync = "tl.disable_thread_storage_sync"
+ *
+ */
+static constexpr const char *kDisableThreadStorageSync =
+    "tl.disable_thread_storage_sync";
+
+/*!
+ * \brief Force inline Let bindings during simplification.
+ *
+ * kForceLetInline = "tl.force_let_inline"
+ *
+ */
+static constexpr const char *kForceLetInline = "tl.force_let_inline";
+
 /*!
  * \brief The size of the vectorized dimension in buffer, designed by user
  *
@@ -52,6 +95,50 @@ static constexpr const char *kDisableDynamicTailSplit =
  */
 static constexpr const char *kDynamicAlignment = "tl.dynamic_alignment";
 
+/*!
+ * \brief Get the type of the CUDA tensor map
+ *
+ * DataType cuTensorMapType()
+ *
+ */
+DataType cuTensorMapType();
+
+// fast math related op
+// __exp(x) - fast exponential
+TVM_DLL const Op &__exp();
+// __exp10(x) - fast base-10 exponential
+TVM_DLL const Op &__exp10();
+// __log(x) - fast natural logarithm
+TVM_DLL const Op &__log();
+// __log2(x) - fast base-2 logarithm
+TVM_DLL const Op &__log2();
+// __log10(x) - fast base-10 logarithm
+TVM_DLL const Op &__log10();
+// __tan(x) - fast tangent
+TVM_DLL const Op &__tan();
+// __cos(x) - fast cosine
+TVM_DLL const Op &__cos();
+// __sin(x) - fast sine
+TVM_DLL const Op &__sin();
+
+// high precision with IEEE-compliant.
+// ieee_add(x, y, rounding_mode) - IEEE-compliant addition
+TVM_DLL const Op &ieee_add();
+// ieee_sub(x, y, rounding_mode) - IEEE-compliant subtraction
+TVM_DLL const Op &ieee_sub();
+// ieee_mul(x, y, rounding_mode) - IEEE-compliant multiplication
+TVM_DLL const Op &ieee_mul();
+// ieee_fmaf(x, y, z, rounding_mode) - IEEE-compliant fused multiply-add
+TVM_DLL const Op &ieee_fmaf();
+// ieee_frcp(x, rounding_mode) - IEEE-compliant reciprocal
+TVM_DLL const Op &ieee_frcp();
+// ieee_fsqrt(x, rounding_mode) - IEEE-compliant square root
+TVM_DLL const Op &ieee_fsqrt();
+// ieee_frsqrt(x) - IEEE-compliant reciprocal square root (rn only)
+TVM_DLL const Op &ieee_frsqrt();
+// ieee_fdiv(x, y, rounding_mode) - IEEE-compliant division
+TVM_DLL const Op &ieee_fdiv();
+
 /*!
  * \brief tvm intrinsics for TMADescriptor creation for tiled load
  *
@@ -60,7 +147,7 @@ static constexpr const char *kDynamicAlignment = "tl.dynamic_alignment";
  * swizzle, l2_promotion, oob_fill)
  *
  */
-const Op &create_tma_descriptor();
+TVM_DLL const Op &create_tma_descriptor();
 
 /*!
  * \brief tvm intrinsics for TMADescriptor creation for image to column load
@@ -71,7 +158,7 @@ const Op &create_tma_descriptor();
  * l2_promotion, oob_fill)
  *
  */
-const Op &create_tma_im2col_descriptor();
+TVM_DLL const Op &create_tma_im2col_descriptor();
 
 /*!
  * \brief Create a list of mbarrier with num_threads
@@ -79,7 +166,7 @@ const Op &create_tma_im2col_descriptor();
  * create_list_of_mbarrier(num_threads0, num_threads1, ...)
  *
  */
-const Op &create_list_of_mbarrier();
+TVM_DLL const Op &create_list_of_mbarrier();
 
 /*!
  * \brief Get the mbarrier with barrier_id
@@ -87,7 +174,7 @@ const Op &create_list_of_mbarrier();
  * int64_t* GetMBarrier(barrier_id)
  *
  */
-const Op &get_mbarrier();
+TVM_DLL const Op &get_mbarrier();
 
 /*!
  * \brief tvm intrinsics for loading data from global tensor descriptor to
@@ -96,7 +183,7 @@ const Op &get_mbarrier();
  * tma_load(descriptor, mbarrier, smem_data, coord_0, coord_1, ...)
  *
  */
-const Op &tma_load();
+TVM_DLL const Op &tma_load();
 
 /*!
  * \brief tvm intrinsics for loading image from global tensor to columns in
@@ -106,7 +193,7 @@ const Op &tma_load();
  * image_offset, ...)
  *
  */
-const Op &tma_load_im2col();
+TVM_DLL const Op &tma_load_im2col();
 
 /*!
  * \brief tvm intrinsics for storing data from shared memory to global tensor
@@ -115,7 +202,15 @@ const Op &tma_load_im2col();
  * tma_store(descriptor, smem_data, coord_0, coord_1, ...)
  *
  */
-const Op &tma_store();
+TVM_DLL const Op &tma_store();
+
+/*!
+ * \brief tvm intrinsics for barrier initialization fence
+ *
+ * ptx_fence_barrier_init()
+ *
+ */
+const Op &ptx_fence_barrier_init();
 
 /*!
  * \brief tvm intrinsics for mbarrier wait with parity bit
@@ -123,7 +218,7 @@ const Op &tma_store();
  * mbarrier_wait_parity(mbarrier, parity)
  *
  */
-const Op &mbarrier_wait_parity();
+TVM_DLL const Op &mbarrier_wait_parity();
 
 /*!
  * \brief tvm intrinsics for mbarrier expect tx
@@ -131,39 +226,93 @@ const Op &mbarrier_wait_parity();
  * mbarrier_expect_tx(mbarrier, transaction_bytes)
  *
  */
-const Op &mbarrier_expect_tx();
+TVM_DLL const Op &mbarrier_expect_tx();
+
+/*!
+ * \brief tvm intrinsic for ptx tensor core wgmma instructions.
+ *
+ *  void ptx_wgmma_ss(StringImm accum_dtype, StringImm wgmma_prefix, bool
+ * a_is_k_major, bool b_is_k_major, StringImm a_dtype_abbrv, StringImm
+ * b_dtype_abbrv, StringImm accum_dtype_abbrv, Var A_descriptor, PrimExpr
+ * A_offset, Var B_descriptor, Var B_offset, Var C_data, Var C_offset, bool
+ * scale_out, bool scale_in_a, bool scale_in_b);
+ */
+TVM_DLL const Op &ptx_wgmma_ss();
+
+/*!
+ * \brief tvm intrinsics for ptx tensor core wgmma instructions.
+ *
+ *  void ptx_wgmma_rs(StringImm accum_dtype, StringImm wgmma_prefix, bool
+ * a_is_k_major, bool b_is_k_major, StringImm a_dtype_abbrv, StringImm
+ * b_dtype_abbrv, StringImm accum_dtype_abbrv, Var A_descriptor, PrimExpr
+ * A_offset, Var B_descriptor, Var B_offset, Var C_data, Var C_offset, bool
+ * scale_out, bool scale_in_a, bool scale_in_b);
+ */
+TVM_DLL const Op &ptx_wgmma_rs();
+
+/*!
+ * \brief tvm intrinsics for initializing tensor memory
+ *
+ * ptx_init_tensor_memory(tmem_buffer, num_cols)
+ *
+ */
+TVM_DLL const Op &ptx_init_tensor_memory();
+
+/*!
+ * \brief tvm intrinsics for deallocating tensor memory
+ *
+ * tmem_deallocate(tmem_buffer)
+ *
+ */
+TVM_DLL const Op &ptx_deallocate_tensor_memory();
 
 /*!
  * \brief tvm intrinsics for ldmatrix
  *
- * ptx_ldmatirx(transposed, num, shared_addr, local_addr)
+ * ptx_ldmatrix(transposed, num, shared_addr, local_addr)
  *
  */
-const Op &ptx_ldmatirx();
+TVM_DLL const Op &ptx_ldmatrix();
 
 /*!
  * \brief tvm intrinsics for stmatrix
  *
- * ptx_ldmatirx(transposed, num, shared_addr, int32_values...)
+ * ptx_ldmatrix(transposed, num, shared_addr, int32_values...)
  *
  */
-const Op &ptx_stmatirx();
+TVM_DLL const Op &ptx_stmatrix();
 
 /*!
- * \brief Pack two b16 value into a b32 value
+ * \brief tvm intrinsics for sync threads partial
  *
- * int32 pack_b16(b16_value, b16_value)
+ * sync_thread_partial()
  *
  */
-const Op &pack_b16();
+TVM_DLL const Op &sync_thread_partial();
 
 /*!
- * \brief Similar to __syncthreads(), but can be used to sync partial threads
+ * \brief tvm intrinsics for copy unrolled
  *
- * sync_thread_partial(num_partial_threads or mbarrier)
+ * copy_unrolled(dst, src, size, unroll_factor)
  *
  */
-const Op &sync_thread_partial();
+TVM_DLL const Op &copy_unrolled();
+
+/*!
+ * \brief tvm intrinsic for ptx async copy barrier using
+ * cp.async.mbarrier.arrive.noinc
+ *
+ *  This op is used to represent a ptx async copy barrier operation in tilelang.
+ */
+TVM_DLL const Op &ptx_cp_async_barrier_noinc();
+
+/*!
+ * \brief Pack two b16 value into a b32 value
+ *
+ * int32 pack_b16(b16_value, b16_value)
+ *
+ */
+TVM_DLL const Op &pack_b16();
 
 /*!
  * \brief Issue a shared memory fence for async operations
@@ -171,7 +320,7 @@ const Op &sync_thread_partial();
  * FenceProxyAsync()
  *
  */
-const Op &fence_proxy_async();
+TVM_DLL const Op &fence_proxy_async();
 
 /*!
  * \brief Indicate arrival of warp issuing TMA_STORE
@@ -179,7 +328,7 @@ const Op &fence_proxy_async();
  * tma_store_arrive()
  *
  */
-const Op &tma_store_arrive();
+TVM_DLL const Op &tma_store_arrive();
 
 /*!
  * \brief Wait for TMA_STORE to finish
@@ -187,7 +336,7 @@ const Op &tma_store_arrive();
  * tma_store_wait()
  *
  */
-const Op &tma_store_wait();
+TVM_DLL const Op &tma_store_wait();
 
 /*!
  * \brief Set reg hint for warp-specialized branched
@@ -195,7 +344,7 @@ const Op &tma_store_wait();
  * SetMaxNRegInc(num_reg, is_inc)
  *
  */
-const Op &set_max_nreg();
+TVM_DLL const Op &set_max_nreg();
 
 /*!
  * \brief No set reg hint for warp-specialized branched
@@ -203,7 +352,63 @@ const Op &set_max_nreg();
  * no_set_max_nreg()
  *
  */
-const Op &no_set_max_nreg();
+TVM_DLL const Op &no_set_max_nreg();
+
+/*!
+ * \brief Arrive at a warpgroup fence for WGMMA sequences
+ *
+ * warpgroup_arrive()
+ *
+ */
+TVM_DLL const Op &warpgroup_arrive();
+
+/*!
+ * \brief Commit the current warpgroup batch for WGMMA sequences
+ *
+ * warpgroup_commit_batch()
+ *
+ */
+TVM_DLL const Op &warpgroup_commit_batch();
+
+/*!
+ * \brief Wait for the warpgroup batch identified by num_mma
+ *
+ * warpgroup_wait(num_mma)
+ *
+ */
+TVM_DLL const Op &warpgroup_wait();
+
+/*!
+ * \brief Return the canonical lane index for the calling thread.
+ *
+ * get_lane_idx([warp_size])
+ *
+ */
+TVM_DLL const Op &get_lane_idx();
+
+/*!
+ * \brief Return the canonical warp index, assuming converged threads.
+ *
+ * get_warp_idx_sync([warp_size])
+ *
+ */
+TVM_DLL const Op &get_warp_idx_sync();
+
+/*!
+ * \brief Return the canonical warp index without synchronizing the warp.
+ *
+ * get_warp_idx([warp_size])
+ *
+ */
+TVM_DLL const Op &get_warp_idx();
+
+/*!
+ * \brief Return the canonical warp group index for converged threads.
+ *
+ * get_warp_group_idx([warp_size, warps_per_group])
+ *
+ */
+TVM_DLL const Op &get_warp_group_idx();
 
 /*!
  * \brief Wait the previous wgmma to finish
@@ -211,15 +416,15 @@ const Op &no_set_max_nreg();
  * wait_wgmma(num_mma)
  *
  */
-const Op &wait_wgmma();
+TVM_DLL const Op &wait_wgmma();
 
 /*!
  * \brief Synchronize all threads in a grid
  *
- * sync_grid()
+ * sync_grid_cg()
  *
  */
-const Op &sync_grid();
+TVM_DLL const Op &sync_grid_cg();
 
 /*!
  * \brief tvm intrinsic for loop continue
@@ -227,7 +432,7 @@ const Op &sync_grid();
  * loop_break()
  *
  */
-const Op &loop_break();
+TVM_DLL const Op &loop_break();
 
 /*!
  * \brief tvm intrinsic for amd matrix core mfma instructions.
@@ -277,7 +482,123 @@ TVM_DLL const Op &tvm_rdna_wmma();
  */
 TVM_DLL const Op &tvm_rdna_wmma_store();
 
+/*!
+ * \brief tilelang intrinsic for general matrix multiplication (GEMM).
+ *
+ *  This op is used to represent a generic GEMM operation in tilelang.
+ */
+TVM_DLL const Op &tl_gemm();
+
+/*!
+ * \brief tilelang intrinsic for sparse matrix multiplication (GEMM with
+ * sparsity).
+ *
+ *  This op is used to represent a sparse GEMM operation in tilelang.
+ */
+TVM_DLL const Op &tl_gemm_sp();
+
+/*!
+ * \brief tilelang intrinsic for shuffle elect.
+ *
+ *  This op is used to represent a shuffle elect operation in tilelang.
+ */
+TVM_DLL const Op &tl_shuffle_elect();
+
+/*!
+ * \brief tvm intrinsic to get the current clock cycle count.
+ *
+ *  uint64 get_clock()
+ *
+ */
+TVM_DLL const Op &get_clock();
+
+/*!
+ * \brief tilelang intrinsic for initializing a descriptor buffer for
+ * wgmma/utcmma.
+ *
+ *  This op is used to represent a descriptor initialization operation in
+ * tilelang.
+ */
+TVM_DLL const Op &initialize_descriptor();
+
+/*!
+ * \brief tilelang intrinsic for setting the start address of a descriptor
+ * buffer for wgmma/utcmma.
+ *
+ *  This op is used to represent a descriptor start address setting operation in
+ * tilelang.
+ */
+TVM_DLL const Op &increase_descriptor_offset();
+/*!
+ * \brief tilelang intrinsic for element-wise atomic addition.
+ *
+ *  This op is used to represent an element-wise atomic add operation in
+ * tilelang.
+ */
+TVM_DLL const Op &atomicadd_elem_op();
+
+/*!
+ * \brief tilelang intrinsic for atomic add that returns the original value.
+ *
+ *  This op is used to represent an atomic add operation that returns the
+ * original value before addition in tilelang.
+ */
+TVM_DLL const Op &atom_add();
+
+/*!
+ * \brief tilelang intrinsic for warp reduction sum.
+ */
+TVM_DLL const Op &warp_reduce_sum();
+
+/*!
+ * \brief tilelang intrinsic for warp reduction max.
+ */
+TVM_DLL const Op &warp_reduce_max();
+
+/*!
+ * \brief tilelang intrinsic for warp reduction min.
+ */
+TVM_DLL const Op &warp_reduce_min();
+
+/*!
+ * \brief tilelang intrinsic for warp reduction bitand.
+ */
+TVM_DLL const Op &warp_reduce_bitand();
+
+/*!
+ * \brief tilelang intrinsic for warp reduction bitor.
+ */
+TVM_DLL const Op &warp_reduce_bitor();
+
+/*!
+ * \brief tilelang intrinsic for electing exactly one lane within a logical
+ * thread group.
+ */
+TVM_DLL const Op &elect_one_sync();
+
+/*!
+ * \brief tilelang intrinsic for synchronizing all threads in a warp.
+ */
+TVM_DLL const Op &sync_warp();
+
+/*!
+ * \brief tilelang intrinsic for continuing the innermost loop.
+ */
+TVM_DLL const Op &loop_continue();
+
+/*!
+ * \brief tilelang intrinsic for checking if any lane in the warp has a true
+ * value.
+ */
+TVM_DLL const Op &warp_any();
+
+/*!
+ * \brief tilelang intrinsic for checking if all lanes in the warp have a true
+ * value.
+ */
+TVM_DLL const Op &warp_all();
+
 } // namespace tl
 } // namespace tvm
 
-#endif //  TVM_TL_OP_BUILTIN_H_
\ No newline at end of file
+#endif //  TVM_TL_OP_BUILTIN_H_
diff --git a/src/op/bulk_copy.cc b/src/op/bulk_copy.cc
deleted file mode 100644
index 9a8bdbe0..00000000
--- a/src/op/bulk_copy.cc
+++ /dev/null
@@ -1,489 +0,0 @@
-/*!
- * \file tl/op/bulk_copy.cc
- * \brief Bulk copy operator.
- *
- */
-
-#include "bulk_copy.h"
-
-#include <tvm/tir/builtin.h>
-#include <tvm/tir/op.h>
-#include <tvm/tir/op_attr_types.h>
-
-#include "../target/cuda.h"
-#include "../target/utils.h"
-#include "builtin.h"
-
-namespace tvm {
-namespace tl {
-
-using namespace tir;
-
-static int to_CUtensorMapDataType(DataType dtype) {
-  CUtensorMapDataType tp;
-  if (dtype.is_float()) {
-    switch (dtype.bits()) {
-    case 64:
-      tp = CU_TENSOR_MAP_DATA_TYPE_FLOAT64;
-      break;
-    case 32:
-      tp = CU_TENSOR_MAP_DATA_TYPE_FLOAT32;
-      break;
-    case 16:
-      tp = CU_TENSOR_MAP_DATA_TYPE_FLOAT16;
-      break;
-    case 8:
-      tp = CU_TENSOR_MAP_DATA_TYPE_UINT8;
-      break;
-    default:
-      ICHECK(0) << dtype;
-    }
-  } else if (dtype.is_bfloat16()) {
-    tp = CU_TENSOR_MAP_DATA_TYPE_BFLOAT16;
-  } else if (dtype.is_e4m3_float8() or dtype.is_e5m2_float8()) {
-    tp = CU_TENSOR_MAP_DATA_TYPE_UINT8;
-  } else if (dtype.is_int()) {
-    switch (dtype.bits()) {
-    case 64:
-      tp = CU_TENSOR_MAP_DATA_TYPE_INT64;
-      break;
-    case 32:
-      tp = CU_TENSOR_MAP_DATA_TYPE_INT32;
-      break;
-    case 16:
-      tp = CU_TENSOR_MAP_DATA_TYPE_UINT16;
-      break;
-    case 8:
-      tp = CU_TENSOR_MAP_DATA_TYPE_UINT8;
-      break;
-    default:
-      ICHECK(0) << dtype;
-    }
-  } else if (dtype.is_uint()) {
-    switch (dtype.bits()) {
-    case 64:
-      tp = CU_TENSOR_MAP_DATA_TYPE_UINT64;
-      break;
-    case 32:
-      tp = CU_TENSOR_MAP_DATA_TYPE_UINT32;
-      break;
-    case 16:
-      tp = CU_TENSOR_MAP_DATA_TYPE_UINT16;
-      break;
-    case 8:
-      tp = CU_TENSOR_MAP_DATA_TYPE_UINT8;
-      break;
-    default:
-      ICHECK(0) << dtype;
-    }
-  } else {
-    ICHECK(0) << dtype;
-  }
-  return static_cast<int>(tp);
-}
-
-template <typename T> static Array<T> ReverseArray(Array<T> array) {
-  return Array<T>{array.rbegin(), array.rend()};
-}
-
-Stmt Copy::LowerBulkCopy(const LowerArgs &T, arith::Analyzer *analyzer) const {
-  if (T.disable_tma_lower)
-    return Stmt();
-  if (!TargetIsHopper(T.target))
-    return Stmt();
-  bool is_load;
-  if (src.scope() == "global" &&
-      (dst.scope() == "shared.dyn" || dst.scope() == "shared")) {
-    // Use the Hopper TMA bulk copy instructions
-    is_load = true;
-  } else if (dst.scope() == "global" &&
-             (src.scope() == "shared.dyn" || src.scope() == "shared")) {
-    is_load = false;
-  } else {
-    return Stmt();
-  }
-  Buffer global_tensor = is_load ? src : dst;
-  Buffer shared_tensor = is_load ? dst : src;
-  Array<Range> global_range = is_load ? src_range : dst_range;
-  Array<Range> shared_range = is_load ? dst_range : src_range;
-
-  if (T.layout_map.count(global_tensor)) {
-    LOG(WARNING) << "TMA bulk copy cannot support a non-swizzled global "
-                    "layout, fallback to normal copy.";
-    return Stmt();
-  }
-
-  Array<PrimExpr> indices;
-  for (auto r : shared_range)
-    indices.push_back(r->min);
-
-  std::vector<PrimExpr> strides;
-  PrimExpr stride = 1;
-  for (size_t i = 0; i < shared_tensor->shape.size(); i++) {
-    auto s = shared_tensor->shape[shared_tensor->shape.size() - i - 1];
-    strides.insert(strides.begin(), stride);
-    stride *= s;
-  }
-
-  ICHECK(strides.size() == indices.size())
-      << "strides.size() != indices.size()" << strides.size() << " "
-      << indices.size();
-  PrimExpr offset = 0;
-  for (size_t i = 0; i < indices.size(); i++) {
-    offset += indices[i] * strides[i];
-  }
-
-  Layout shared_layout;
-  if (T.layout_map.count(shared_tensor)) {
-    shared_layout = T.layout_map[shared_tensor];
-    shared_tensor = T.buffer_remap[shared_tensor];
-  }
-
-  TMADesc desc;
-
-  // Verify copy rank
-  desc.rank = global_tensor->shape.size();
-  ICHECK(desc.rank >= 1 && desc.rank <= 5) << desc.rank;
-
-  // Verify datatype
-  ICHECK(global_tensor->dtype == shared_tensor->dtype)
-      << "Copy between buffer " << global_tensor->name << " and "
-      << shared_tensor->name << " with different data type "
-      << global_tensor->dtype << " and " << shared_tensor->dtype;
-
-  desc.data_type = to_CUtensorMapDataType(global_tensor->dtype);
-
-  // Global Tensor Shape and Stride
-  desc.global_addr = global_tensor->data;
-  desc.global_shape = ReverseArray(global_tensor->shape);
-  Array<PrimExpr> global_coords =
-      ReverseArray(global_range.Map([](Range r) { return r->min; }));
-  if (!global_tensor->strides.empty()) {
-    desc.global_stride = ReverseArray(global_tensor->strides);
-  } else {
-    // Create stride from shape
-    PrimExpr stride = 1;
-    desc.global_stride.reserve(desc.rank);
-    for (size_t i = 0; i < desc.rank; i++) {
-      desc.global_stride.push_back(stride);
-      stride *= desc.global_shape[i];
-    }
-  }
-  // The first stride element should be 1
-  ICHECK(is_one(desc.global_stride[0])) << desc.global_stride;
-  // Make global stride in bytes
-  desc.global_stride = desc.global_stride.Map([&](PrimExpr e) {
-    return cast(DataType::Int(64), e) * global_tensor->dtype.bytes();
-  });
-
-  // Smem Box
-  // check smem range and global range is legal
-  auto s_range_idx = 0;
-  for (size_t i = 0; i < global_range.size(); i++) {
-    auto g_range = global_range[i];
-    if (is_one(g_range->extent)) {
-      continue;
-    }
-    auto s_range = shared_range[s_range_idx++];
-    ICHECK(StructuralEqual()(g_range->extent, s_range->extent))
-        << global_tensor->name << "[" << i << "] is illegal, "
-        << global_tensor->name << "[" << i << "] = " << g_range->extent << ", "
-        << shared_tensor->name << "[" << s_range_idx
-        << "] = " << s_range->extent;
-  }
-
-  desc.smem_box =
-      ReverseArray(global_range.Map([](Range r) { return r->extent; }));
-
-  desc.smem_stride = Array<PrimExpr>(desc.rank, PrimExpr(1));
-
-  // L2 & OOB
-  desc.l2_promotion = static_cast<int>(CU_TENSOR_MAP_L2_PROMOTION_L2_128B);
-  desc.oob_fill = static_cast<int>(CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE);
-
-  // Detect smem layout
-  desc.interleave = static_cast<int>(CU_TENSOR_MAP_INTERLEAVE_NONE);
-  if (!shared_layout.defined()) {
-    desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_NONE);
-  } else {
-    ICHECK(shared_layout->InputDim() == 2) << "Cannot detect TMA layout.";
-    auto stride = as_const_int(shared_layout->InputShape()[0]);
-    auto continuous = as_const_int(shared_layout->InputShape()[1]);
-    ICHECK(stride != nullptr && continuous != nullptr);
-    if (StructuralEqual()(shared_layout, makeGemmABLayoutPadded(
-                                             *stride, *continuous,
-                                             shared_tensor->dtype.bits()))) {
-      desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_NONE);
-    } else if (StructuralEqual()(
-                   shared_layout,
-                   makeQuarterBankSwizzleLayout(*stride, *continuous,
-                                                shared_tensor->dtype.bits()))) {
-      desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_32B);
-    } else if (StructuralEqual()(
-                   shared_layout,
-                   makeHalfBankSwizzleLayout(*stride, *continuous,
-                                             shared_tensor->dtype.bits()))) {
-      desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_64B);
-    } else if (StructuralEqual()(
-                   shared_layout,
-                   makeFullBankSwizzleLayout(*stride, *continuous,
-                                             shared_tensor->dtype.bits()))) {
-      desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_128B);
-    } else {
-      ICHECK(0) << "Cannot detect TMA layout.";
-    }
-  }
-
-  auto inner_box_dim = as_const_int(desc.smem_box[0]);
-  ICHECK(inner_box_dim != nullptr);
-  int instruction_dim = *inner_box_dim;
-  if (desc.swizzle == static_cast<int>(CU_TENSOR_MAP_SWIZZLE_64B)) {
-    instruction_dim = 64 / src->dtype.bytes();
-  } else if (desc.swizzle == static_cast<int>(CU_TENSOR_MAP_SWIZZLE_128B)) {
-    instruction_dim = 128 / src->dtype.bytes();
-  }
-  if (instruction_dim > 256) {
-    // smem_box dim must be in [0, 256]
-    // if is 512, we need to split the copy into two parts
-    ICHECK((*inner_box_dim) % 256 == 0)
-        << "inner_box_dim: " << *inner_box_dim << " is not divisible by 256";
-    instruction_dim = 256;
-  }
-  ICHECK((*inner_box_dim) % instruction_dim == 0);
-  desc.smem_box.Set(0, PrimExpr(instruction_dim));
-
-  Call create_descriptor =
-      Call(DataType::Handle(), create_tma_descriptor(), desc.EncodeCallArgs());
-
-  Array<PrimExpr> args;
-  args.reserve(desc.rank + 3);
-  args.push_back(create_descriptor);
-  if (is_load)
-    args.push_back(0); // mbarrier id placeholder
-  auto op = is_load ? tma_load() : tma_store();
-
-  Stmt tma_copy;
-  PrimExpr total_elements = 1;
-  for (auto e : desc.smem_box)
-    total_elements *= e;
-
-  if ((*inner_box_dim) != instruction_dim) {
-    Var loop_var("i");
-    int loop_extent = (*inner_box_dim) / instruction_dim;
-
-    PrimExpr shared_addr = shared_tensor.access_ptr(
-        is_load ? 2 : 1, DataType::Handle(), 1,
-        offset + total_elements * loop_var, total_elements);
-    args.push_back(shared_addr);
-    global_coords.Set(0, global_coords[0] + instruction_dim * loop_var);
-    for (auto coord : global_coords)
-      args.push_back(coord);
-    tma_copy = For(loop_var, 0, loop_extent, ForKind::kUnrolled,
-                   Evaluate(Call(DataType::Handle(), op, args)));
-  } else {
-    PrimExpr shared_addr = shared_tensor.access_ptr(
-        is_load ? 2 : 1, DataType::Handle(), 1, offset, total_elements);
-    args.push_back(shared_addr);
-    for (auto coord : global_coords)
-      args.push_back(coord);
-    tma_copy = Evaluate(Call(DataType::Handle(), op, args));
-  }
-  tma_copy = IfThenElse(EQ(T.thread_var, T.thread_bounds->min), tma_copy);
-
-  return tma_copy;
-}
-
-Array<PrimExpr> TMADesc::EncodeCallArgs() const {
-  Array<PrimExpr> args;
-  args.reserve(rank * 4 + 7);
-
-  args.push_back(data_type);
-  args.push_back(static_cast<int>(rank));
-  args.push_back(global_addr);
-  for (auto e : global_shape)
-    args.push_back(e);
-  for (auto e : global_stride)
-    args.push_back(e);
-  for (auto e : smem_box)
-    args.push_back(e);
-  for (auto e : smem_stride)
-    args.push_back(e);
-  args.push_back(interleave);
-  args.push_back(swizzle);
-  args.push_back(l2_promotion);
-  args.push_back(oob_fill);
-
-  return args;
-}
-
-DataType cuTensorMapType() { return DataType::UInt(8, 128); }
-
-Conv2DIm2ColOp::Conv2DIm2ColOp(Array<PrimExpr> args, BufferMap vmap) {
-  src = vmap[GetVarFromAccessPtr(args[0])];
-  dst = vmap[GetVarFromAccessPtr(args[1])];
-  nhw_step = args[2];
-  c_step = args[3];
-  kernel = args[4].as<IntImm>().value()->value;
-  stride = args[5].as<IntImm>().value()->value;
-  dilation = args[6].as<IntImm>().value()->value;
-  padding = args[7].as<IntImm>().value()->value;
-}
-
-Stmt Conv2DIm2ColOp::Lower(const LowerArgs &T,
-                           arith::Analyzer *analyzer) const {
-  ICHECK(TargetIsHopper(T.target));
-  ICHECK(src.scope() == "global" &&
-         (dst.scope() == "shared.dyn" || dst.scope() == "shared"));
-  ICHECK(src->shape.size() == 4);
-  ICHECK(dst->shape.size() == 2);
-  ICHECK(src->dtype == dst->dtype);
-  Layout shared_layout;
-  if (T.layout_map.count(dst)) {
-    shared_layout = T.layout_map[dst];
-  }
-
-  TMAIm2ColDesc desc;
-  desc.rank = src->shape.size();
-  desc.data_type = to_CUtensorMapDataType(src->dtype);
-  desc.global_addr = src->data;
-  desc.global_shape = ReverseArray(src->shape);
-
-  if (!src->strides.empty()) {
-    desc.global_stride = ReverseArray(src->strides);
-  } else {
-    // Create stride from shape
-    PrimExpr stride = 1;
-    desc.global_stride.reserve(desc.rank);
-    for (size_t i = 0; i < desc.rank; i++) {
-      desc.global_stride.push_back(stride);
-      stride *= desc.global_shape[i];
-    }
-  }
-  // The first stride element should be 1
-  ICHECK(is_one(desc.global_stride[0])) << desc.global_stride;
-  // Make global stride in bytes
-  desc.global_stride = desc.global_stride.Map([&](PrimExpr e) {
-    return cast(DataType::Int(64), e) * src->dtype.bytes();
-  });
-  desc.elem_stride = {1, stride, stride, 1};
-  desc.lower_corner = {-padding, -padding};
-  desc.upper_corner = {-padding, -padding};
-  desc.smem_box_pixel = Downcast<IntImm>(dst->shape[0])->value;
-  desc.smem_box_channel = Downcast<IntImm>(dst->shape[1])->value;
-  desc.l2_promotion = static_cast<int>(CU_TENSOR_MAP_L2_PROMOTION_L2_128B);
-  desc.oob_fill = static_cast<int>(CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE);
-  desc.interleave = static_cast<int>(CU_TENSOR_MAP_INTERLEAVE_NONE);
-  if (!shared_layout.defined()) {
-    desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_NONE);
-  } else {
-    ICHECK(shared_layout->InputDim() == 2) << "Cannot detect TMA layout.";
-    auto stride = as_const_int(shared_layout->InputShape()[0]);
-    auto continuous = as_const_int(shared_layout->InputShape()[1]);
-    ICHECK(stride != nullptr && continuous != nullptr);
-
-    if (StructuralEqual()(shared_layout,
-                          makeQuarterBankSwizzleLayout(*stride, *continuous,
-                                                       dst->dtype.bits()))) {
-      desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_32B);
-    } else if (StructuralEqual()(shared_layout, makeHalfBankSwizzleLayout(
-                                                    *stride, *continuous,
-                                                    dst->dtype.bits()))) {
-      desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_64B);
-    } else if (StructuralEqual()(shared_layout, makeFullBankSwizzleLayout(
-                                                    *stride, *continuous,
-                                                    dst->dtype.bits()))) {
-      desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_128B);
-    } else {
-      ICHECK(0) << "Cannot detect TMA layout.";
-    }
-  }
-
-  Call create_desc = Call(DataType::Handle(), create_tma_im2col_descriptor(),
-                          desc.EncodeCallArgs());
-
-  Array<PrimExpr> global_coords; // c, w, h, n
-  Array<PrimExpr> image_offset;  // w, h
-  global_coords.reserve(desc.rank);
-
-  ICHECK(analyzer->CanProveEqual(
-      FloorMod(desc.global_shape[0], desc.smem_box_channel), 0))
-      << "Currently can only support divisible channel case";
-
-  global_coords.push_back(
-      FloorMod(c_step * desc.smem_box_channel, desc.global_shape[0]));
-  image_offset.push_back(
-      dilation *
-      FloorMod(FloorDiv(c_step * desc.smem_box_channel, desc.global_shape[0]),
-               kernel));
-  image_offset.push_back(dilation * FloorDiv(c_step * desc.smem_box_channel,
-                                             desc.global_shape[0] * kernel));
-
-  PrimExpr h_dim =
-      FloorDiv(src->shape[1] + 2 * padding - (kernel - 1) * dilation - 1,
-               stride) +
-      1;
-  PrimExpr w_dim =
-      FloorDiv(src->shape[2] + 2 * padding - (kernel - 1) * dilation - 1,
-               stride) +
-      1;
-  global_coords.push_back(
-      stride * FloorMod(nhw_step * desc.smem_box_pixel, w_dim) - padding);
-  global_coords.push_back(
-      stride *
-          FloorMod(FloorDiv(nhw_step * desc.smem_box_pixel, w_dim), h_dim) -
-      padding);
-  global_coords.push_back(
-      FloorDiv(nhw_step * desc.smem_box_pixel, w_dim * h_dim));
-
-  Array<PrimExpr> args;
-  args.reserve(desc.rank * 2 + 1);
-  args.push_back(create_desc);
-  args.push_back(0); // mbar placeholder
-  auto dst_buffer = T.buffer_remap.count(dst) ? T.buffer_remap[dst] : dst;
-  auto shared_addr = dst_buffer.access_ptr(2);
-  args.push_back(shared_addr);
-  for (auto coord : global_coords)
-    args.push_back(coord);
-  for (auto offset : image_offset)
-    args.push_back(offset);
-
-  Stmt tma_copy =
-      IfThenElse(EQ(T.thread_var, T.thread_bounds->min),
-                 Evaluate(Call(DataType::Handle(), tma_load_im2col(), args)));
-  return tma_copy;
-}
-
-Array<PrimExpr> TMAIm2ColDesc::EncodeCallArgs() const {
-  Array<PrimExpr> args;
-  args.reserve(rank * 5 + 5);
-
-  args.push_back(data_type);
-  args.push_back(static_cast<int>(rank));
-  args.push_back(global_addr);
-  for (auto e : global_shape)
-    args.push_back(e);
-  for (auto e : global_stride)
-    args.push_back(e);
-  for (auto e : elem_stride)
-    args.push_back(e);
-  for (auto e : lower_corner)
-    args.push_back(e);
-  for (auto e : upper_corner)
-    args.push_back(e);
-  args.push_back(smem_box_pixel);
-  args.push_back(smem_box_channel);
-  args.push_back(interleave);
-  args.push_back(swizzle);
-  args.push_back(l2_promotion);
-  args.push_back(oob_fill);
-
-  return args;
-}
-
-TIR_REGISTER_TL_OP(Conv2DIm2ColOp, c2d_im2col)
-    .set_num_inputs(8)
-    .set_attr<TCallEffectKind>("TCallEffectKind",
-                               Integer(CallEffectKind::kOpaque));
-
-} // namespace tl
-} // namespace tvm
diff --git a/src/op/bulk_copy.h b/src/op/bulk_copy.h
deleted file mode 100644
index 279f1792..00000000
--- a/src/op/bulk_copy.h
+++ /dev/null
@@ -1,63 +0,0 @@
-/*!
- * \file tl/op/bulk_copy.h
- * \brief Bulk copy operator.
- *
- */
-
-#ifndef TVM_TL_OP_BULK_COPY_H_
-#define TVM_TL_OP_BULK_COPY_H_
-
-#include "elem.h"
-
-namespace tvm {
-namespace tl {
-
-using namespace tir;
-
-struct TMADesc {
-  size_t rank;
-  int data_type;
-  Array<PrimExpr> global_shape, global_stride;
-  Array<PrimExpr> smem_box, smem_stride;
-  PrimExpr global_addr;
-  int swizzle;
-  int interleave;
-  int oob_fill;
-  int l2_promotion;
-
-  Array<PrimExpr> EncodeCallArgs() const;
-};
-
-DataType cuTensorMapType();
-
-struct TMAIm2ColDesc {
-  size_t rank;
-  int data_type;
-  Array<PrimExpr> global_shape, global_stride, elem_stride; // rank
-  Array<PrimExpr> lower_corner, upper_corner;               // rank - 2
-  PrimExpr global_addr;
-  int smem_box_pixel, smem_box_channel;
-  int swizzle;
-  int interleave;
-  int oob_fill;
-  int l2_promotion;
-
-  Array<PrimExpr> EncodeCallArgs() const;
-};
-
-class Conv2DIm2ColOp : public Operator {
-public:
-  Conv2DIm2ColOp(Array<PrimExpr> args, BufferMap vmap);
-  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const final;
-  static const Op &Get();
-
-private:
-  Buffer src, dst;
-  int stride, padding, dilation, kernel;
-  PrimExpr nhw_step, c_step;
-};
-
-} // namespace tl
-} // namespace tvm
-
-#endif //  TVM_TL_OP_BULK_COPY_H_
\ No newline at end of file
diff --git a/src/op/copy.cc b/src/op/copy.cc
new file mode 100644
index 00000000..754dd733
--- /dev/null
+++ b/src/op/copy.cc
@@ -0,0 +1,1981 @@
+/*!
+ * \file tl/op/copy.cc
+ * \brief Define copy operator for various memory transfer strategies (Normal,
+ *        Bulk/TMA, LDSM/STSM) and lowering logic for GPU code generation.
+ *
+ * This module is part of TVM TensorIR's Tensor Layout (TL) operations,
+ * implementing memory copy operations that can target CPUs or GPUs with
+ * optimization for different instructions like bulk copy, matrix load/store,
+ * and Hopper's new TMA (Tensor Memory Accelerator).
+ */
+
+#include "copy.h"
+#include "../layout/tcgen05_layout.h"
+#include "../target/utils.h"
+#include "../transform/common/loop_fusion_utils.h"
+#include "../transform/common/loop_parallel_transform_utils.h"
+#include "../transform/loop_partition.h"
+#include "../transform/loop_vectorize.h"
+#include "region.h"
+
+#include "../target/cuda.h"
+#include "../target/utils.h"
+#include "builtin.h"
+#include <tvm/tir/builtin.h>
+#include <tvm/tir/op.h>
+#include <tvm/tir/op_attr_types.h>
+#include <tvm/tir/transform.h>
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+/*!
+ * \brief Helper to map TVM's DataType to CUDA's CUtensorMapDataType enum value.
+ * This function converts TVM data types to CUDA tensor map data types for TMA
+ * operations.
+ */
+static int to_CUtensorMapDataType(DataType dtype) {
+  CUtensorMapDataType tp;
+  if (dtype.is_float()) {
+    switch (dtype.bits()) {
+    case 64:
+      tp = CU_TENSOR_MAP_DATA_TYPE_FLOAT64;
+      break;
+    case 32:
+      tp = CU_TENSOR_MAP_DATA_TYPE_FLOAT32;
+      break;
+    case 16:
+      tp = CU_TENSOR_MAP_DATA_TYPE_FLOAT16;
+      break;
+    case 8:
+      tp = CU_TENSOR_MAP_DATA_TYPE_UINT8;
+      break;
+    default:
+      ICHECK(0) << dtype;
+    }
+  } else if (dtype.is_bfloat16()) {
+    tp = CU_TENSOR_MAP_DATA_TYPE_BFLOAT16;
+  } else if (dtype.is_float8_e4m3() || dtype.is_float8_e5m2()) {
+    tp = CU_TENSOR_MAP_DATA_TYPE_UINT8;
+  } else if (dtype.is_int()) {
+    switch (dtype.bits()) {
+    case 64:
+      tp = CU_TENSOR_MAP_DATA_TYPE_INT64;
+      break;
+    case 32:
+      tp = CU_TENSOR_MAP_DATA_TYPE_INT32;
+      break;
+    case 16:
+      tp = CU_TENSOR_MAP_DATA_TYPE_UINT16;
+      break;
+    case 8:
+      tp = CU_TENSOR_MAP_DATA_TYPE_UINT8;
+      break;
+    default:
+      ICHECK(0) << dtype;
+    }
+  } else if (dtype.is_uint()) {
+    switch (dtype.bits()) {
+    case 64:
+      tp = CU_TENSOR_MAP_DATA_TYPE_UINT64;
+      break;
+    case 32:
+      tp = CU_TENSOR_MAP_DATA_TYPE_UINT32;
+      break;
+    case 16:
+      tp = CU_TENSOR_MAP_DATA_TYPE_UINT16;
+      break;
+    case 8:
+      tp = CU_TENSOR_MAP_DATA_TYPE_UINT8;
+      break;
+    default:
+      ICHECK(0) << dtype;
+    }
+  } else {
+    ICHECK(0) << dtype;
+  }
+  return static_cast<int>(tp);
+}
+
+/*!
+ * \brief Utility function to reverse an array.
+ * This is commonly used to convert between row-major and column-major layouts.
+ */
+template <typename T> static Array<T> ReverseArray(Array<T> array) {
+  return Array<T>{array.rbegin(), array.rend()};
+}
+
+/*!
+ * \brief Construct a Copy operator node from call arguments and a buffer map.
+ *
+ * This constructor parses the first two entries of `args` as Call nodes
+ * describing source and destination Regions (via RegionOp), extracts their
+ * Buffers and Ranges, and stores them on the newly created CopyNode. It also
+ * reads optional arguments:
+ * - args[2] (IntImm): coalesced width (stored only if > 0),
+ * - args[3] (Bool): disable TMA lowering flag,
+ * - args[4] (IntImm): eviction policy.
+ *
+ * Preconditions:
+ * - `args` must contain at least two Call-compatible PrimExpr entries
+ * describing regions; an ICHECK will fail if they are not CallNodes.
+ *
+ * @param args Array of PrimExpr where:
+ *   - args[0] is the source Region call,
+ *   - args[1] is the destination Region call,
+ *   - optional args[2..4] are coalesced width, disable_tma, and eviction
+ * policy.
+ * @param vmap BufferMap used to resolve RegionOp buffers and ranges.
+ */
+Copy::Copy(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<CopyNode> node = make_object<CopyNode>();
+  Array<Range> rgs[2];
+  Buffer bf[2];
+  for (int i = 0; i < 2; i++) {
+    auto expr = args[i];
+    auto call = expr.as<CallNode>();
+    ICHECK(call);
+    auto region = RegionOp(call->args, vmap);
+    rgs[i] = region->GetRanges();
+    bf[i] = region->GetBuffer();
+  }
+  std::tie(node->src, node->dst) = std::tie(bf[0], bf[1]);
+  std::tie(node->src_range, node->dst_range) = std::tie(rgs[0], rgs[1]);
+  if (args.size() >= 3) {
+    auto coalesced_width = Downcast<IntImm>(args[2]);
+    if (coalesced_width->value > 0) {
+      node->coalesced_width = coalesced_width;
+    }
+  }
+  if (args.size() >= 4) {
+    node->disable_tma = Downcast<Bool>(args[3]);
+  }
+  if (args.size() >= 5) {
+    node->eviction_policy = args[4].as<IntImmNode>()->value;
+  }
+  data_ = std::move(node);
+}
+
+/**
+ * @brief Create a shallow clone of this CopyNode as a TileOperator.
+ *
+ * Produces a new CopyNode object copy-constructed from this node. If a parallel
+ * sub-operation (par_op_) is present, the sub-operation is cloned as well and
+ * attached to the new node. The returned value is a TileOperator wrapper
+ * around the newly created node.
+ *
+ * @return TileOperator A TileOperator owning the cloned CopyNode.
+ */
+TileOperator CopyNode::Clone() const {
+  auto op = make_object<CopyNode>(*this);
+  if (par_op_.defined()) {
+    op->par_op_ = Downcast<ParallelOp>(par_op_->Clone());
+  }
+  return Copy(op);
+}
+
+/*!
+ * \brief Create iterator variables for the copy operation.
+ * This function creates iteration variables for dimensions that have extent
+ * > 1. \return Array of IterVar representing the iterator variables for the
+ * copy operation.
+ */
+Array<IterVar> CopyNode::MakeIterVars() const {
+  Array<IterVar> loop_vars;
+  size_t idx = 0;
+  for (size_t i = 0; i < src_range.size(); i++) {
+    if (is_one(src_range[i]->extent))
+      continue;
+    Var var = Var(std::string{char('i' + idx)}, src_range[i]->extent->dtype);
+    idx++;
+    loop_vars.push_back(
+        {Range(0, src_range[i]->extent), var, IterVarType::kDataPar});
+  }
+  return loop_vars;
+}
+
+/*!
+ * \brief Create s for the copy operation.
+ * This function generates the actual index expressions for accessing source or
+ * destination buffers. For dimensions with extent=1, it uses the range minimum;
+ * for others, it adds the iteration variable. \param ivs Array of IterVar
+ * returned by MakeIterVars(). \param src_dst 0 for src_indices, 1 for
+ * dst_indices. \return Array of PrimExpr representing the indices for the copy
+ * operation.
+ */
+Array<PrimExpr> CopyNode::MakeIndices(const Array<IterVar> &ivs,
+                                      int src_dst) const {
+  Array<PrimExpr> indices;
+  Array<Range> ranges = src_dst == 0 ? src_range : dst_range;
+  size_t idx = 0;
+  for (size_t i = 0; i < ranges.size(); i++) {
+    if (is_one(ranges[i]->extent))
+      indices.push_back(ranges[i]->min);
+    else {
+      indices.push_back(ranges[i]->min + ivs[idx]->var);
+      idx++;
+    }
+  }
+  ICHECK(idx == ivs.size())
+      << "idx = " << idx << ", ivs.size() = " << ivs.size()
+      << "src name = " << src->name << ", dst name = " << dst->name;
+  return indices;
+}
+
+/**
+ * @brief Build a boundary predicate that guards memory accesses for the copy.
+ *
+ * Constructs a conjunction of per-dimension bounds checks (e.g. `min + iv <
+ * extent` and `min + iv >= 0`) for every dynamic dimension involved in the
+ * copy. Uses the provided arithmetic analyzer to elide checks that can be
+ * proven statically.
+ *
+ * The function ICHECKs that the supplied `extents` align with the operator's
+ * recorded ranges for the selected side (source when `src_dst == 0`,
+ * destination when `src_dst == 1`).
+ *
+ * @param ivs IterVars corresponding to the varying dimensions of the copy. Each
+ *   IterVar maps to a non-unit extent dimension in the stored ranges.
+ * @param extents Extents of the tensor being accessed (must match the number of
+ *   ranges); used as the upper bounds for generated checks.
+ * @param src_dst Selects which side's ranges to use: `0` for source, `1` for
+ *   destination.
+ * @return PrimExpr A conjunction of necessary bounds checks, or an empty
+ * `PrimExpr` (null) if all checks are provably true and no predicate is
+ * required.
+ */
+PrimExpr CopyNode::MakePredicate(arith::Analyzer *analyzer,
+                                 const Array<IterVar> &ivs,
+                                 Array<PrimExpr> extents, int src_dst) const {
+  Array<Range> ranges = src_dst == 0 ? src_range : dst_range;
+  Array<PrimExpr> cond_list;
+  ICHECK(extents.size() == ranges.size()) << extents << " " << ranges;
+  size_t idx = 0;
+  for (size_t i = 0; i < ranges.size(); i++) {
+    if (is_one(ranges[i]->extent))
+      continue;
+    PrimExpr cond = ranges[i]->min + ivs[idx]->var < extents[i];
+    if (!analyzer->CanProve(cond, arith::ProofStrength::kSymbolicBound)) {
+      cond_list.push_back(cond);
+    }
+    cond = ranges[i]->min + ivs[idx]->var >= 0;
+    if (!analyzer->CanProve(cond, arith::ProofStrength::kSymbolicBound)) {
+      cond_list.push_back(cond);
+    }
+    idx++;
+  }
+  if (cond_list.empty())
+    return {};
+  else {
+    PrimExpr cond = cond_list[0];
+    for (size_t i = 1; i < cond_list.size(); i++)
+      cond = And(cond, cond_list[i]);
+    return cond;
+  }
+}
+
+/**
+ * @brief Construct a SIMT-style nested loop that implements the copy.
+ *
+ * Builds a loop nest that performs element-wise loads from the source buffer
+ * and stores into the destination buffer. For a scalar copy (no varying
+ * iteration dimensions) this returns a single serial loop executing one
+ * store. For multi-dimensional copies it:
+ * - creates data-parallel loops (Parallel For) for each varying dimension,
+ * - binds the resulting iteration variables to the provided arithmetic
+ *   analyzer for simplification,
+ * - computes source and destination index expressions,
+ * - applies per-buffer boundary predicates (if needed) to mask out-of-range
+ *   accesses,
+ * - inserts a cast when src and dst dtypes differ,
+ * - applies an optional `coalesced_width` annotation to generated parallel
+ *   loops when present.
+ *
+ * @param analyzer Analyzer used to simplify and bind loop variable domains.
+ * @return For A nested For statement representing the generated SIMT loop nest.
+ */
+For CopyNode::MakeSIMTLoop(arith::Analyzer *analyzer) const {
+  Array<IterVar> loop_vars = MakeIterVars();
+  bool is_scalar = loop_vars.empty();
+
+  for (const auto &iv : loop_vars)
+    analyzer->Bind(iv->var, iv->dom);
+
+  ICHECK(loop_vars.size() <= src_range.size())
+      << "loop_vars.size() = " << loop_vars.size()
+      << ", src_range.size() = " << src_range.size() << ", src = " << src->name
+      << ", dst = " << dst->name;
+
+  ICHECK(loop_vars.size() <= dst_range.size())
+      << "loop_vars.size() = " << loop_vars.size()
+      << ", dst_range.size() = " << dst_range.size() << ", src = " << src->name
+      << ", dst = " << dst->name;
+
+  Array<PrimExpr> src_indices = MakeIndices(loop_vars, 0);
+  Array<PrimExpr> dst_indices = MakeIndices(loop_vars, 1);
+
+  PrimExpr src_predicate = MakePredicate(analyzer, loop_vars, src->shape, 0);
+  PrimExpr dst_predicate = MakePredicate(analyzer, loop_vars, dst->shape, 1);
+
+  PrimExpr value = BufferLoad(src, src_indices);
+  if (src->dtype != dst->dtype)
+    value = Cast(dst->dtype, value);
+  if (src_predicate.defined())
+    value = if_then_else(src_predicate, value, make_zero(dst->dtype));
+
+  Stmt body = BufferStore(dst, value, dst_indices);
+  if (dst_predicate.defined())
+    body = IfThenElse(dst_predicate, body);
+  if (is_scalar) {
+    return For(Var("i"), 0, 1, ForKind::kSerial, body);
+  }
+  for (int i = loop_vars.size() - 1; i >= 0; i--) {
+    Map<String, ObjectRef> annotations = {};
+    if (coalesced_width.defined()) {
+      annotations.Set("coalesced_width", coalesced_width);
+    }
+    body = For(loop_vars[i]->var, 0, loop_vars[i]->dom->extent,
+               ForKind::kParallel, body, std::nullopt, annotations);
+  }
+  return Downcast<For>(body);
+}
+
+/**
+ * @brief Compute a linearized shared-memory layout used for TMA transfers.
+ *
+ * Creates a Layout that maps an N-D shared tensor into a 1-D-like ordering
+ * suitable for TMA by blocking each dimension into 256-element tiles and
+ * splitting each original index into a quotient and remainder. Effectively
+ * transforms each index i_k into two coordinates: floor(i_k / 256) and
+ * i_k % 256, producing an ordering equivalent to concatenating all quotients
+ * followed by all remainders.
+ *
+ * @param shared_tensor The shared-memory buffer whose shape defines the input
+ *        dimensions for the layout inference.
+ * @return Layout A Layout describing the linearized ordering for the TMA copy.
+ */
+Layout CopyNode::ComputeLinearLayout(const Buffer &shared_tensor) const {
+  Array<PrimExpr> input_size = shared_tensor->shape;
+  Array<PrimExpr> forward_vars;
+  for (size_t i = 0; i < input_size.size(); i++) {
+    forward_vars.push_back(InputPlaceholder(i));
+  }
+  // [i, j] -> [i // 256, j // 256, i % 256, j % 256]
+  Array<PrimExpr> forward_index;
+  for (size_t i = 0; i < input_size.size(); i++) {
+    forward_index.push_back(FloorDiv(forward_vars[i], 256));
+  }
+  for (size_t i = 0; i < input_size.size(); i++) {
+    forward_index.push_back(FloorMod(forward_vars[i], 256));
+  }
+  return Layout(input_size, forward_index);
+}
+
+/**
+ * @brief Infer memory layouts for this Copy operation.
+ *
+ * Determines an appropriate LayoutMap for the copy based on the target and
+ * enabled lowering paths. For TMA-capable targets when the chosen copy
+ * instruction is BulkLoad or BulkStore, this may produce a linearized shared
+ * memory layout suitable for TMA transfers (only when inference is invoked at
+ * InferLevel::kFree and no layout for the shared buffer is already annotated).
+ * For other cases (including LDSM/STSM and the normal copy path), layout
+ * inference is delegated to the SIMT parallel operation produced by
+ * MakeSIMTLoop().
+ *
+ * This method may read PassContext configuration (kDisableTMALower) and may
+ * lazily construct and cache the parallel operation in par_op_ as a side
+ * effect.
+ *
+ * @param T LayoutInferArgs containing target and the current layout map.
+ * @param level The inference level controlling how aggressive/layouts may be
+ *              proposed.
+ * @return LayoutMap mapping buffers to inferred layouts (may be empty if no
+ *         additional layouts are suggested).
+ */
+LayoutMap CopyNode::InferLayout(const LayoutInferArgs &T,
+                                InferLevel level) const {
+  auto target = T.target;
+  using namespace tvm::transform;
+  PassContext pass_ctx = PassContext::Current();
+  bool disable_tma_lower =
+      pass_ctx->GetConfig<bool>(kDisableTMALower, false).value();
+  auto copy_inst = GetCopyInst(target, disable_tma_lower || disable_tma,
+                               T.layout_map, T.analyzer, T.buffer_oob);
+
+  // Handle tensor memory (tmem) layout inference
+  if (copy_inst == CopyInst::kTMemLoad || copy_inst == CopyInst::kTMemStore) {
+    // Tensor memory copy
+    // TODO (mzw) Add support for tcgen05.st/cp (in conj. with LowerTmemCopy)
+    ICHECK(copy_inst == CopyInst::kTMemLoad)
+        << "Only support tensor memory copy from shared.tmem to local.fragment "
+           "currently";
+    LayoutMap results;
+    if (!T.layout_map.count(dst) && T.layout_map.count(src)) {
+      // Use the default layout (32dp32b) if not specified
+      // NOTE (mzw) We will check the layout in LowerTmemCopy(), so don't
+      // worry for tmem-incompatible layout
+      Layout src_layout = T.layout_map[src];
+      Array<IterVar> logical_coords = MakeIterVars();
+      Array<PrimExpr> logical_coords_var = {logical_coords[0]->var,
+                                            logical_coords[1]->var};
+      Array<PrimExpr> phy_indices = src_layout->Forward(logical_coords_var);
+
+      // Tmem physical coord range analysis
+      auto analyzer = std::make_shared<arith::Analyzer>();
+      for (const auto &iv : logical_coords)
+        analyzer->Bind(iv->var, iv->dom);
+      arith::ConstIntBound phy_row_bounds =
+          analyzer->const_int_bound(phy_indices[0]);
+      arith::ConstIntBound phy_col_bounds =
+          analyzer->const_int_bound(phy_indices[1]);
+      Range row_dom = Range((int)(phy_row_bounds->min_value),
+                            (int)(phy_row_bounds->max_value + 1));
+      Range col_dom = Range((int)(phy_col_bounds->min_value),
+                            (int)(phy_col_bounds->max_value + 1));
+
+      constexpr int WARP_SIZE = 32; // Set to 32 since only sm100 is supported
+      constexpr int WARPGROUP_SIZE = 4 * WARP_SIZE;
+      ICHECK(is_const_int(T.thread_bounds->extent))
+          << "Tensor memory copy requires thread_bounds->extent (num_threads) "
+             "to be constant integers";
+      int num_threads = *as_const_int(T.thread_bounds->extent);
+      ICHECK(num_threads % WARPGROUP_SIZE == 0)
+          << "Tensor memory copy requires thread bounds to be aligned to "
+             "warpgroups, but found "
+          << "thread range = " << T.thread_bounds;
+
+      for (int num_useful_wgs = num_threads / WARPGROUP_SIZE;
+           num_useful_wgs >= 1; --num_useful_wgs) {
+        int num_useful_threads = num_useful_wgs * WARPGROUP_SIZE;
+        Tcgen05Meta meta = getTcgen05Meta_32dp32b();
+        auto [is_success, tmem_coord2frag, num_chunks_each_wg] =
+            expandTcgen05Layout(
+                meta, phy_col_bounds->max_value - phy_col_bounds->min_value + 1,
+                num_useful_threads, row_dom, col_dom);
+        if (!is_success) {
+          continue;
+        }
+        Fragment logical_coord2frag =
+            Fragment(logical_coords, tmem_coord2frag->Forward(phy_indices),
+                     tmem_coord2frag->ForwardThread(phy_indices, std::nullopt),
+                     make_itervar("rep", 1));
+        results.Set(dst, logical_coord2frag->BindThreadRange(T.thread_bounds));
+        break;
+      }
+    }
+    return results;
+  }
+
+  if (copy_inst == CopyInst::kBulkLoad || copy_inst == CopyInst::kBulkStore) {
+    // if can apply swizzling, we skip layout inference
+    // for bulk load/store, we can directly apply the layout of normal copy
+    // This must be a global/shared layout, so we can skip the parallel op
+    // layout inference (parallel layout inference only annotate the loop layout
+    // and the register layout).
+    bool is_load = copy_inst == CopyInst::kBulkLoad;
+    Buffer global_tensor = is_load ? src : dst;
+    Buffer shared_tensor = is_load ? dst : src;
+    // check shared layout is non-swizzle
+    // skip layout inference if shared layout is already annotated
+    if (level == InferLevel::kFree && !T.layout_map.count(shared_tensor)) {
+      // create a new layout map for tma linear layout
+      Layout linear_layout = ComputeLinearLayout(shared_tensor);
+      return Map<Buffer, Layout>({{shared_tensor, linear_layout}});
+    }
+  }
+  // for LDSM/STSM, the layout was deduced from register layout
+  // so we can directly apply the layout of normal copy
+  // Use parallel op to infer the layout
+  if (!par_op_.defined()) {
+    arith::Analyzer analyzer;
+    par_op_ = ParallelOp((MakeSIMTLoop(&analyzer)));
+  }
+  return par_op_->InferLayout(T, level);
+}
+/**
+ * @brief Determine whether this CopyNode can be lowered to a Bulk Load (TMA)
+ * instruction.
+ *
+ * The function returns true when all of the following hold:
+ * - the target architecture advertises bulk-copy/TMA support;
+ * - the source buffer resides in global memory;
+ * - the destination buffer resides in shared memory (either "shared" or
+ * "shared.dyn");
+ * - the source and destination have the same element data type.
+ *
+ * If the source and destination dtypes differ, a warning is logged and the
+ * function returns false (the caller is expected to fall back to a normal
+ * copy).
+ *
+ * @param target The compilation target to query for bulk-copy support.
+ * @return true if the copy can be implemented as a Bulk Load (TMA); false
+ * otherwise.
+ */
+bool CopyNode::CheckBulkLoad(Target target, arith::Analyzer *analyzer,
+                             bool check_last_dim) const {
+  // 1. arch must have bulk copy support
+  if (!TargetHasBulkCopy(target))
+    return false;
+  // 2. src and dst must be global and shared
+  if (src.scope() != "global" ||
+      (dst.scope() != "shared.dyn" && dst.scope() != "shared"))
+    return false;
+  // 3. check shape.
+  // last dim of src * dtype.bits() must be a multiple of 16
+  // https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html#group__CUDA__TENSOR__MEMORY_1ga7c7d2aaac9e49294304e755e6f341d7
+  // now we check src (gmem) as tma box dim is deduced from src
+  if (check_last_dim &&
+      analyzer->CanProve(
+          FloorMod(src_range[src_range.size() - 1]->extent * src->dtype.bytes(),
+                   16) != 0,
+          arith::ProofStrength::kSymbolicBound)) {
+    LOG(WARNING)
+        << "src range must have last dim multiple of 16 for tma bulk load "
+        << src->name << " range " << src_range[src_range.size() - 1]->extent
+        << " * " << src->dtype.bytes() << " % 16 != 0";
+    return false;
+  }
+
+  // 4. src and dst must have the same dtype
+  if (src->dtype != dst->dtype) {
+    LOG(WARNING) << "src and dst must have the same dtype for tma load "
+                 << src->name << " vs. " << dst->name << " dtype " << src->dtype
+                 << " vs. " << dst->dtype << " will be fallback to normal copy";
+    return false;
+  }
+  return true;
+}
+
+bool CopyNode::CheckBulkCopy1D(const Buffer &global_tensor,
+                               const Buffer &shared_tensor,
+                               const Array<Range> &global_range,
+                               const Array<Range> &shared_range,
+                               const LayoutMap &layout_map,
+                               arith::Analyzer *analyzer) const {
+
+  // Step 1: check shared is contiguous
+  bool shared_is_contiguous = true;
+  if (layout_map.count(shared_tensor)) {
+    shared_is_contiguous = false;
+  }
+  // Step 2: check global is contiguous
+  bool global_is_contiguous = true;
+  bool global_not_full_dim_encounter = false;
+  for (int i = global_range.size() - 1; i >= 0; i--) {
+    if (!global_not_full_dim_encounter) {
+      if (!analyzer->CanProve(global_range[i]->extent ==
+                                      global_tensor->shape[i] &&
+                                  global_range[i]->min == 0,
+                              arith::ProofStrength::kSymbolicBound)) {
+        global_not_full_dim_encounter = true;
+      }
+    } else {
+      if (!analyzer->CanProve(global_range[i]->extent == 1,
+                              arith::ProofStrength::kSymbolicBound)) {
+        global_is_contiguous = false;
+        break;
+      }
+    }
+  }
+
+  // Step 3: check element match and no OOB
+  PrimExpr shared_elements = 1;
+  for (size_t i = 0; i < shared_range.size(); i++) {
+    shared_elements *= shared_range[i]->extent;
+  }
+  PrimExpr global_elements = 1;
+  for (size_t i = 0; i < global_range.size(); i++) {
+    global_elements *= global_range[i]->extent;
+  }
+  bool element_match =
+      analyzer->CanProveEqual(shared_elements, global_elements);
+
+  return (shared_is_contiguous && global_is_contiguous && element_match);
+}
+
+bool CopyNode::CheckBulkLoad1D(Target target, const LayoutMap &layout_map,
+                               arith::Analyzer *analyzer) const {
+  if (!CheckBulkLoad(target, analyzer, false))
+    return false;
+  auto global_tensor = src;
+  auto shared_tensor = dst;
+  auto global_range = src_range;
+  auto shared_range = dst_range;
+  return CheckBulkCopy1D(global_tensor, shared_tensor, global_range,
+                         shared_range, layout_map, analyzer);
+}
+
+bool CopyNode::CheckBulkStore1D(Target target, const LayoutMap &layout_map,
+                                arith::Analyzer *analyzer) const {
+  if (!CheckBulkStore(target, analyzer, false))
+    return false;
+  auto shared_tensor = src;
+  auto global_tensor = dst;
+  auto shared_range = src_range;
+  auto global_range = dst_range;
+  return CheckBulkCopy1D(global_tensor, shared_tensor, global_range,
+                         shared_range, layout_map, analyzer);
+}
+
+/**
+ * @brief Determine if this CopyNode can be lowered to a CUDA BulkStore (TMA
+ * store).
+ *
+ * Checks whether the target supports bulk copy, the source buffer is in shared
+ * memory (shared or shared.dyn), the destination buffer is in global memory,
+ * and both buffers have the same element data type. If the data types differ,
+ * a warning is logged and false is returned.
+ *
+ * @param target Target device/architecture to check for bulk-copy support.
+ * @return true if all conditions for a BulkStore are met; false otherwise.
+ */
+bool CopyNode::CheckBulkStore(Target target, arith::Analyzer *analyzer,
+                              bool check_last_dim) const {
+  // 1. arch must have bulk copy support
+  if (!TargetHasBulkCopy(target))
+    return false;
+  // 2. src and dst must be shared.dyn and local.fragment
+  if ((src.scope() != "shared.dyn" && src.scope() != "shared") ||
+      dst.scope() != "global")
+    return false;
+  // 3. check shape.
+  // last dim of dst * dtype.bits() must be a multiple of 16
+  // https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html#group__CUDA__TENSOR__MEMORY_1ga7c7d2aaac9e49294304e755e6f341d7
+  // now we check dst (gmem) as tma box dim is deduced from dst
+  if (check_last_dim &&
+      analyzer->CanProve(
+          FloorMod(dst_range[dst_range.size() - 1]->extent * dst->dtype.bytes(),
+                   16) != 0,
+          arith::ProofStrength::kSymbolicBound)) {
+    LOG(WARNING)
+        << "dst range must have last dim multiple of 16 for tma bulk store "
+        << dst->name << " range " << dst_range[dst_range.size() - 1]->extent
+        << " * " << dst->dtype.bytes() << " % 16 != 0";
+    return false;
+  }
+  // 4. src and dst must have the same dtype
+  if (src->dtype != dst->dtype) {
+    LOG(WARNING) << "src and dst must have the same dtype for tma store "
+                 << src->name << " vs. " << dst->name << " dtype " << src->dtype
+                 << " vs. " << dst->dtype << " will be fallback to normal copy";
+    return false;
+  }
+  return true;
+}
+
+/*!
+ * \brief Check if the copy operation is a LDSM copy.
+ * This function verifies if the copy operation can be implemented using CUDA's
+ * Load Matrix (LDSM) instruction. Requirements include: target supports
+ * LDMATRIX, source is shared.dyn, destination is local.fragment. \param target
+ * Target device. \return True if the copy operation is a LDSM copy, false
+ * otherwise.
+ */
+bool CopyNode::CheckLDSMCopy(Target target) const {
+  return TargetHasLdmatrix(target) &&
+         (src.scope() == "shared.dyn" || src.scope() == "shared") &&
+         dst.scope() == "local.fragment";
+}
+
+/**
+ * @brief Determine whether this copy can use the STMATRIX store (STSM) path.
+ *
+ * Returns true when the target supports STMATRIX and the source buffer is in
+ * the `local.fragment` scope while the destination buffer is in shared memory
+ * (`shared` or `shared.dyn`).
+ *
+ * @param target The compilation target to query for STMATRIX support.
+ * @return true if the copy may be lowered to an STSM instruction; false
+ * otherwise.
+ */
+bool CopyNode::CheckSTSMCopy(Target target) const {
+  return TargetHasStmatrix(target) && src.scope() == "local.fragment" &&
+         (dst.scope() == "shared.dyn" || dst.scope() == "shared");
+}
+
+/**
+ * @brief Determine whether this copy can use tensor memory load (tcgen05.ld).
+ *
+ * Returns true when the target supports tensor memory and the source buffer is
+ * in `shared.tmem` scope while the destination buffer is in `local.fragment`.
+ *
+ * @param target The compilation target to query for tensor memory support.
+ * @return true if the copy may be lowered to a tcgen05.ld instruction; false
+ * otherwise.
+ */
+bool CopyNode::CheckTMemLoad(Target target) const {
+  return TargetHasTmem(target) && src.scope() == "shared.tmem" &&
+         dst.scope() == "local.fragment";
+}
+
+/**
+ * @brief Determine whether this copy can use tensor memory store (tcgen05.st).
+ *
+ * Returns true when the target supports tensor memory and the source buffer is
+ * in `local.fragment` scope while the destination buffer is in `shared.tmem`.
+ *
+ * @param target The compilation target to query for tensor memory support.
+ * @return true if the copy may be lowered to a tcgen05.st instruction; false
+ * otherwise.
+ */
+bool CopyNode::CheckTMemStore(Target target) const {
+  return TargetHasTmem(target) && src.scope() == "local.fragment" &&
+         dst.scope() == "shared.tmem";
+}
+
+/**
+ * @brief Selects the most specific copy instruction supported for the given
+ * target and buffers.
+ *
+ * Determines which specialized copy lowering to use (TMA bulk load/store, LDSM,
+ * STSM, TMem load/store) based on target capabilities and the memory scopes of
+ * the source/destination buffers. If TMA lowering is disabled via the flag,
+ * BulkLoad/BulkStore are not selected. The selection priority is: TMemLoad,
+ * TMemStore, BulkLoad1D, BulkStore1D, BulkLoad, BulkStore, LDSM, STSM, then
+ * Normal (fallback).
+ *
+ * @param target The compilation target used to query hardware capabilities.
+ * @param disable_tma_lower If true, prevents selecting TMA-based bulk
+ * load/store instructions.
+ * @return CopyInst The chosen copy instruction enum value.
+ */
+CopyInst CopyNode::GetCopyInst(Target target, bool disable_tma_lower,
+                               const LayoutMap &layout_map,
+                               arith::Analyzer *analyzer,
+                               bool buffer_oob = false) const {
+  // disable_tma_lower is from pass_configs
+  // when tilelang.PassConfigKey.TL_DISABLE_TMA_LOWER is True,
+  // we will not use tma for bulk load/store
+
+  // Check tensor memory operations first (highest priority for SM100/Blackwell)
+  // 1d tma access can not support out of bound access
+  if (!disable_tma_lower && !buffer_oob &&
+      CheckBulkLoad1D(target, layout_map, analyzer)) {
+    return CopyInst::kBulkLoad1D;
+  } else if (!disable_tma_lower && !buffer_oob &&
+             CheckBulkStore1D(target, layout_map, analyzer)) {
+    return CopyInst::kBulkStore1D;
+  } else if (!disable_tma_lower && CheckBulkLoad(target, analyzer)) {
+    return CopyInst::kBulkLoad;
+  } else if (!disable_tma_lower && CheckBulkStore(target, analyzer)) {
+    return CopyInst::kBulkStore;
+  } else if (CheckLDSMCopy(target)) {
+    return CopyInst::kLDSM;
+  } else if (CheckSTSMCopy(target)) {
+    return CopyInst::kSTSM;
+  } else if (CheckTMemLoad(target)) {
+    return CopyInst::kTMemLoad;
+  } else if (CheckTMemStore(target)) {
+    return CopyInst::kTMemStore;
+  } else {
+    return CopyInst::kNormal;
+  }
+}
+
+/*!
+ * \brief Lower the copy operation to PTX code.
+ * This function converts the high-level copy operation into low-level PTX
+ * instructions. It dispatches to specialized lowering functions based on the
+ * determined copy instruction type:
+ * - Bulk Load/Store: Uses Tensor Memory Accelerator (TMA) instructions
+ * - LDSM/STSM: Uses matrix load/store instructions for tensor cores
+ * - Normal: Uses standard load/store operations with loop transformations
+ * \param T LowerArgs containing target and layout map.
+ * \param analyzer Arithmetic analyzer for simplification.
+ * \return Stmt representing the PTX code for the copy operation.
+ */
+Stmt CopyNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  Target target = T.target;
+
+  using namespace tvm::transform;
+  PassContext pass_ctx = PassContext::Current();
+  bool disable_tma_lower =
+      pass_ctx->GetConfig<bool>(kDisableTMALower, false).value();
+  auto copy_inst = GetCopyInst(target, disable_tma_lower || disable_tma,
+                               T.layout_map, analyzer);
+  if (copy_inst == CopyInst::kTMemLoad || copy_inst == CopyInst::kTMemStore) {
+    auto tmem_copy = LowerTmemCopy(T, analyzer);
+    ICHECK(tmem_copy.defined()) << "Failed to lower tensor memory copy";
+    return tmem_copy;
+  } else if (copy_inst == CopyInst::kBulkLoad1D ||
+             copy_inst == CopyInst::kBulkStore1D) {
+    auto bulk_copy = LowerBulkCopy1D(T, analyzer, copy_inst);
+    ICHECK(bulk_copy.defined()) << "Failed to lower bulk load 1d";
+    return bulk_copy;
+  } else if (copy_inst == CopyInst::kBulkLoad ||
+             copy_inst == CopyInst::kBulkStore) {
+    auto bulk_copy = LowerBulkCopy(T, analyzer, copy_inst);
+    ICHECK(bulk_copy.defined()) << "Failed to lower bulk load/store";
+    return bulk_copy;
+  } else if (copy_inst == CopyInst::kLDSM || copy_inst == CopyInst::kSTSM) {
+    auto ldsm_copy = LowerLDSMCopy(T, analyzer, copy_inst);
+    ICHECK(ldsm_copy.defined()) << "Failed to lower ptx matrix copy";
+    return ldsm_copy;
+  } else if (copy_inst == CopyInst::kNormal) {
+    return LowerNormalCopy(T, analyzer);
+  } else {
+    LOG(FATAL) << "Unsupported copy inst " << static_cast<int>(copy_inst);
+  }
+}
+
+/**
+ * @brief Lower the copy operator using the generic (non-specialized) path.
+ *
+ * Generates standard load/store code paths for targets that cannot or should
+ * not use specialized copy instructions (TMA, LDSM/STSM). Builds a SIMT loop,
+ * fuses and transforms parallel loops, infers and applies loop layouts on GPU
+ * targets, partitions by thread, and applies vectorization appropriate to the
+ * device (CPU or GPU). If a thread-level predicate is required, the resulting
+ * body is guarded with an IfThenElse.
+ *
+ * @param T Lowering context including the target, thread bounds, thread var,
+ *          layout map, and buffer remapping used during layout inference and
+ *          loop partitioning.
+ * @param analyzer Arithmetic analyzer used to simplify and reason about bounds
+ *                 during loop partitioning and predicate construction.
+ * @return Stmt Lowered statement representing the transformed, vectorized
+ *              normal-copy loop (possibly wrapped in a predicate).
+ */
+Stmt CopyNode::LowerNormalCopy(const LowerArgs &T,
+                               arith::Analyzer *analyzer) const {
+  bool is_cpu_target = T.target->GetTargetDeviceType() == kDLCPU;
+  auto simt_loop = MakeSIMTLoop(analyzer);
+  auto fused_loop = Downcast<For>(ParallelLoopFuser::Fuse(simt_loop));
+
+  auto transformed_loop =
+      Downcast<For>(ParallelLoopTransformer::Substitute(fused_loop));
+
+  For vectorized_thread_loop;
+  auto par_op = ParallelOp(transformed_loop);
+
+  if (is_cpu_target) {
+    vectorized_thread_loop = VectorizeLoop(transformed_loop);
+  } else {
+    std::vector<InferLevel> levels = {InferLevel::kCommon, InferLevel::kStrict,
+                                      InferLevel::kFree};
+    for (auto level : levels) {
+      par_op->InferLayout({T.target, T.thread_bounds, T.layout_map, analyzer,
+                           false, T.buffer_remap},
+                          level);
+    }
+    auto loop_layout = par_op->GetLoopLayout();
+    auto thread_var = T.thread_var;
+    auto thread_loop =
+        PartitionLoop(par_op->GetRoot(), T.thread_var, analyzer, loop_layout);
+    vectorized_thread_loop = VectorizeLoop(thread_loop);
+  }
+
+  if (par_op->GetPredicate(T.thread_var).defined()) {
+    return IfThenElse(par_op->GetPredicate(T.thread_var).value(),
+                      vectorized_thread_loop);
+  }
+  return vectorized_thread_loop;
+}
+
+/**
+ * @brief Lower a Copy operator to LDSM/STSM (warp-level 8x8 matrix)
+ * instructions.
+ *
+ * Lowers a CopyNode into PTX matrix load/store (LDSM/STSM) sequences when the
+ * access/layouts meet the hardware constraints required by warp-level 8x8
+ * fragment transfers (thread-mapped 8x8 fragment layout, 16-byte contiguous
+ * shared memory accesses, full-range local tiles, matching dtypes for loads,
+ * and no access predicates). If these conditions are not met the function
+ * falls back to lowering via LowerNormalCopy().
+ *
+ * The routine validates layout/thread-mapping compatibility (including support
+ * for transposed fragment layouts), determines vectorization factor (4/2/1)
+ * based on extent alignment, computes shared/local addresses, emits the
+ * appropriate ptx_ldmatrix/ptx_stmatrix call(s), and wraps them in a small
+ * loop that may be unrolled and adjusted for thread-bounds offsets.
+ *
+ * @param T Lowering context (target, layout/ buffer remaps, thread/ bounds).
+ * @param analyzer Arithmetic analyzer used to simplify and prove bounds.
+ * @param copy_inst Must be either CopyInst::kLDSM or CopyInst::kSTSM to select
+ *                  matrix-load vs matrix-store lowering.
+ * @return Stmt A statement implementing the LDSM/STSM lowering, or the result
+ *              of LowerNormalCopy(...) when constraints require fallback.
+ */
+Stmt CopyNode::LowerLDSMCopy(const LowerArgs &T, arith::Analyzer *analyzer,
+                             CopyInst copy_inst) const {
+  ICHECK(copy_inst == CopyInst::kLDSM || copy_inst == CopyInst::kSTSM)
+      << "Invalid copy inst " << static_cast<int>(copy_inst);
+  bool is_ldmatrix = copy_inst == CopyInst::kLDSM;
+
+  // Check no predicates
+  Array<IterVar> loop_vars = MakeIterVars();
+  if (loop_vars.size() < 2) {
+    // cannot support 1-d case
+    return LowerNormalCopy(T, analyzer);
+  }
+  for (const auto &iv : loop_vars)
+    analyzer->Bind(iv->var, iv->dom);
+  PrimExpr src_predicate = MakePredicate(analyzer, loop_vars, src->shape, 0);
+  PrimExpr dst_predicate = MakePredicate(analyzer, loop_vars, dst->shape, 1);
+  if (src_predicate.defined() || dst_predicate.defined()) {
+    // stmatrix and ldmatrix can only support no predicate
+    return LowerNormalCopy(T, analyzer);
+  }
+
+  Buffer shared_tensor = is_ldmatrix ? src : dst;
+  Buffer local_tensor = is_ldmatrix ? dst : src;
+
+  Array<PrimExpr> local_indices = MakeIndices(loop_vars, is_ldmatrix ? 1 : 0);
+  Fragment local_layout = Downcast<Fragment>(T.layout_map[local_tensor]);
+  Array<PrimExpr> local_indices_transformed =
+      local_layout->Forward(local_indices);
+  local_tensor = T.buffer_remap[local_tensor];
+  // currently only support 1-d case
+  if (local_layout->OutputDim() != 1) {
+    // TMA ldmatrix/stmatrix cannot support non-1-d layout, will be fallback to
+    // normal copy
+    return LowerNormalCopy(T, analyzer);
+  }
+
+  Array<PrimExpr> shared_indices = MakeIndices(loop_vars, is_ldmatrix ? 0 : 1);
+  Array<PrimExpr> shared_indices_transformed = shared_indices;
+  Layout shared_layout;
+  if (T.buffer_remap.count(shared_tensor)) {
+    shared_layout = T.layout_map[shared_tensor];
+    shared_tensor = T.buffer_remap[shared_tensor];
+    shared_indices_transformed = shared_layout->Forward(shared_indices);
+  }
+
+  // Check local_layout follows 8x8 layout
+  // LDSM/STSM instructions require 8x8 matrix fragment layout
+  // This matches the warp-level matrix multiplication pattern used in tensor
+  // cores We check both normal and transposed layouts to support different
+  // access patterns
+  bool is_transposed;
+  IterVar col_var = loop_vars[loop_vars.size() - 1];
+  IterVar row_var = loop_vars[loop_vars.size() - 2];
+  PrimExpr local_layout_thread_map =
+      FloorMod(local_layout->ForwardThread(local_indices, std::nullopt), 32);
+  PrimExpr matrix_8x8_thread_map = makeGemmFragment8x8()->ForwardThread(
+      {FloorMod(row_var, 8), FloorMod(col_var, 8)}, std::nullopt);
+  PrimExpr matrix_8x8_thread_map_trans =
+      makeGemmFragment8x8Transposed()->ForwardThread(
+          {FloorMod(row_var, 8), FloorMod(col_var, 8)}, std::nullopt);
+  PrimExpr local_indices_flattened =
+      local_tensor.OffsetOf(local_indices_transformed).back();
+  if (analyzer->CanProveEqual(matrix_8x8_thread_map, local_layout_thread_map) &&
+      IndiceCanVectorize(local_indices_flattened, col_var->var,
+                         col_var->dom->extent, 2, analyzer)) {
+    is_transposed = false;
+  } else if (analyzer->CanProveEqual(matrix_8x8_thread_map_trans,
+                                     local_layout_thread_map) &&
+             IndiceCanVectorize(local_indices_flattened, row_var->var,
+                                row_var->dom->extent, 2, analyzer)) {
+    is_transposed = true;
+  } else {
+    // TMA ldmatrix/stmatrix cannot support non-8x8 layout, will be fallback to
+    // normal copy
+    return LowerNormalCopy(T, analyzer);
+  }
+  // Check shared_layout is 16 bytes continuous
+  // LDSM/STSM instructions require 16-byte aligned data (half-precision floats)
+  // This is a hardware constraint for matrix load/store operations
+  if (shared_tensor->dtype.bytes() != 2) {
+    // TMA ldmatrix/stmatrix cannot support non-16 bytes continuous layout, will
+    // be fallback to normal copy
+    return LowerNormalCopy(T, analyzer);
+  }
+  PrimExpr flattened_indice =
+      shared_tensor.OffsetOf(shared_indices_transformed).back();
+  if (!IndiceCanVectorize(flattened_indice, loop_vars.back()->var,
+                          loop_vars.back()->dom->extent, 8, analyzer)) {
+    // TMA ldmatrix/stmatrix cannot support non-16 bytes continuous layout, will
+    // be fallback to normal copy
+    return LowerNormalCopy(T, analyzer);
+  }
+
+  // Can only support local_range to be a full range
+  for (size_t i = 0; i < dst_range.size(); i++) {
+    if (!is_zero(dst_range[i]->min) ||
+        !analyzer->CanProveEqual(dst_range[i]->extent, dst->shape[i]))
+      // TMA ldmatrix/stmatrix cannot support non-full range, will be fallback
+      // to normal copy
+      return LowerNormalCopy(T, analyzer);
+  }
+
+  // Do the lowering here, try vectorized ldmatrix/stmatrix by 4/2/1
+  PrimExpr extent = local_tensor->shape[0];
+  int num = 1;
+  if (analyzer->CanProveEqual(FloorMod(extent, 8), 0))
+    num = 4;
+  else if (analyzer->CanProveEqual(FloorMod(extent, 4), 0))
+    num = 2;
+
+  Array<PrimExpr> args;
+  const Op &op = is_ldmatrix ? tl::ptx_ldmatrix() : tl::ptx_stmatrix();
+  args.push_back(static_cast<int>(is_transposed));
+  args.push_back(num);
+
+  // Create shared address with regard to local address
+  // if not transpose
+  // coords = Inverse(base + 2 * (thread / 8) % num, warp + (thread % 8) * 4))
+  // if transpose
+  // coords = Inverse(base + 2 * (thread / 8) % num + thread % 2, warp + thread
+  // % 8 / 2)
+  Var local_iter("i");
+  Layout inv = local_layout->Inverse();
+  Array<PrimExpr> shared_coords;
+  PrimExpr warp = FloorDiv(T.thread_var, 32) * 32;
+  if (!is_transposed)
+    shared_coords = inv->Forward(
+        {local_iter * 2 * num + 2 * FloorMod(FloorDiv(T.thread_var, 8), num),
+         warp + FloorMod(T.thread_var, 8) * 4});
+  else
+    shared_coords = inv->Forward(
+        {local_iter * 2 * num + 2 * FloorMod(FloorDiv(T.thread_var, 8), num) +
+             FloorMod(T.thread_var, 2),
+         warp + FloorDiv(FloorMod(T.thread_var, 8), 2)});
+  shared_coords.pop_back(); // remove rep
+  if (shared_layout.defined())
+    shared_coords = shared_layout->Forward(shared_coords);
+  PrimExpr shared_addr = shared_tensor.access_ptr(
+      is_ldmatrix ? 1 : 2, DataType::Handle(), 1,
+      shared_tensor.OffsetOf(shared_coords).back(), PrimExpr(2 * num));
+  args.push_back(shared_addr);
+
+  if (is_ldmatrix) {
+    // Can only support same dtype for ldmatrx
+    if (local_tensor->dtype != shared_tensor->dtype) {
+      // TMA ldmatrix cannot support different dtype, will be fallback to normal
+      // copy
+      return LowerNormalCopy(T, analyzer);
+    }
+    PrimExpr local_addr = local_tensor.access_ptr(
+        2, DataType::Handle(), 1, local_iter * 2 * num, PrimExpr(2 * num));
+    args.push_back(local_addr);
+  } else {
+    for (int i = 0; i < num; i++) {
+      PrimExpr value0 =
+          BufferLoad(local_tensor, {local_iter * 2 * num + 2 * i});
+      PrimExpr value1 =
+          BufferLoad(local_tensor, {local_iter * 2 * num + 2 * i + 1});
+      if (local_tensor->dtype != shared_tensor->dtype) {
+        value0 = Cast(shared_tensor->dtype, value0);
+        value1 = Cast(shared_tensor->dtype, value1);
+      }
+      PrimExpr value_packed =
+          Call(DataType::Int(32), pack_b16(), {value0, value1});
+      args.push_back(value_packed);
+    }
+  }
+
+  auto body = Evaluate(Call(DataType::Handle(), op, args));
+  For for_node =
+      For(local_iter, 0, FloorDiv(extent, 2 * num), ForKind::kSerial, body);
+  for_node = LoopPragmaUnroll(for_node);
+  auto range = T.thread_bounds;
+  if (range.defined()) {
+    auto thread_var = T.thread_var;
+    auto thread_var_with_offset = thread_var - range->min;
+    for_node.CopyOnWrite()->body =
+        Substitute(for_node->body, {{thread_var, thread_var_with_offset}});
+  }
+  return for_node;
+}
+
+/**
+ * @brief Lower tensor memory copy operations (tcgen05.ld/st/cp).
+ *
+ * Handles copy operations involving shared.tmem buffers (tensor memory on
+ * SM100/Blackwell). Supports three types of tensor memory copies:
+ * - tcgen05.ld: tensor memory -> register (local.fragment)
+ * - tcgen05.st: register (local.fragment) -> tensor memory
+ * - tcgen05.cp: shared memory -> tensor memory
+ *
+ * The function validates buffer scopes, extracts 2D loop structure, performs
+ * layout compatibility checks, selects an appropriate TCGEN05 instruction
+ * variant based on data width and thread count, and emits the corresponding PTX
+ * intrinsic call.
+ *
+ * Currently only tcgen05.ld is fully supported; st/cp will trigger an ICHECK
+ * failure.
+ *
+ * @param T Lowering context (target, thread bounds, layout maps, buffer
+ * remaps).
+ * @param analyzer Arithmetic analyzer for proving bounds and simplifying
+ * expressions.
+ * @return Stmt The lowered tensor memory copy statement, or an empty Stmt if
+ * this copy does not involve tensor memory.
+ */
+Stmt CopyNode::LowerTmemCopy(const LowerArgs &T,
+                             arith::Analyzer *analyzer) const {
+  if (src.scope() != "shared.tmem" && dst.scope() != "shared.tmem") {
+    return Stmt();
+  }
+  ICHECK(TargetHasTmem(T.target)) << "Target " << T.target->ToDebugString()
+                                  << " does not support tensor memory copy";
+
+  // Decide copy type
+  bool is_ld = false; // tcgen05.ld (tensor memory -> register)
+  bool is_st = false; // tcgen05.st (register -> tensor memory)
+  bool is_cp = false; // tcgen05.cp (shared memory -> tensor memory)
+  if (src.scope() == "shared.tmem" && dst.scope() == "local.fragment") {
+    is_ld = true;
+  } else if (src.scope() == "local.fragment" && dst.scope() == "shared.tmem") {
+    is_st = true;
+  } else if (src.scope() == "shared.dyn" && dst.scope() == "shared.tmem") {
+    is_cp = true;
+  } else {
+    ICHECK(0) << "Unsupported tensor memory copy: "
+              << "src scope = " << src.scope()
+              << ", dst scope = " << dst.scope();
+  }
+  // Currently tcgen05.cp is not supported
+  // TODO (mzw) Support tcgen05.cp
+  ICHECK(!is_cp)
+      << "Copy from shared memory to tensor memory is not supported yet";
+  // Currently tcgen05.st is not supported
+  // TODO (mzw) Support tcgen05.st
+  ICHECK(!is_st) << "Copy from register to tensor memory is not supported yet";
+
+  // Extract loop variables and ranges
+  Array<IterVar> loop_vars = MakeIterVars();
+  ICHECK(loop_vars.size() == 2) << "Only support 2D tensor memory copy, got "
+                                << loop_vars.size() << " dimensions";
+  for (const auto &iv : loop_vars)
+    analyzer->Bind(iv->var, iv->dom);
+  PrimExpr src_predicate = MakePredicate(analyzer, loop_vars, src->shape, 0);
+  PrimExpr dst_predicate = MakePredicate(analyzer, loop_vars, dst->shape, 1);
+  ICHECK(!src_predicate.defined() && !dst_predicate.defined())
+      << "Tensor memory copy does not support predicates, got " << src_predicate
+      << " and " << dst_predicate;
+  ICHECK(is_const_int(loop_vars[0]->dom->min) &&
+         is_const_int(loop_vars[0]->dom->extent) &&
+         is_const_int(loop_vars[1]->dom->min) &&
+         is_const_int(loop_vars[1]->dom->extent))
+      << "Tensor memory copy requires loop bounds to be constant integers";
+  int64_t logical_row_min = *as_const_int(loop_vars[0]->dom->min);
+  int64_t logical_row_extent = *as_const_int(loop_vars[0]->dom->extent);
+  int64_t logical_col_min = *as_const_int(loop_vars[1]->dom->min);
+  int64_t logical_col_extent = *as_const_int(loop_vars[1]->dom->extent);
+
+  // Extract thread bounds
+  constexpr int WARP_SIZE = 32; // Set to 32 since only sm100 is supported
+  constexpr int WARPGROUP_SIZE = 4 * WARP_SIZE;
+  ICHECK(is_const_int(T.thread_bounds->extent))
+      << "Tensor memory copy requires thread_bounds->extent (num_threads) to "
+         "be constant integers";
+  int num_threads = *as_const_int(T.thread_bounds->extent);
+  ICHECK(analyzer->CanProveEqual(FloorMod(T.thread_bounds->min, WARPGROUP_SIZE),
+                                 0) &&
+         num_threads % WARPGROUP_SIZE == 0)
+      << "Tensor memory copy requires thread bounds to be aligned to "
+         "warpgroups, but found "
+      << "thread range = " << T.thread_bounds;
+
+  // TODO (mzw) Buffer remap for shared.dyn when is_cp is true?
+
+  // Retrieve layout
+  ICHECK(T.layout_map.count(src))
+      << "Source buffer " << src->name << " does not have a layout specified";
+  ICHECK(T.layout_map.count(dst)) << "Destination buffer " << dst->name
+                                  << " does not have a layout specified";
+  Layout src_layout = T.layout_map[src];
+  Fragment dst_layout = Downcast<Fragment>(T.layout_map[dst]);
+
+  // Check layout
+  Array<PrimExpr> logical_indices = MakeIndices(loop_vars, 0);
+  Array<PrimExpr> phy_indices =
+      src_layout->Forward(logical_indices); // "phy" for "physical"
+
+  // Analyse the range of tmem_phy_row and tmem_phy_col
+  arith::ConstIntBound phy_row_bounds =
+      analyzer->const_int_bound(phy_indices[0]);
+  arith::ConstIntBound phy_col_bounds =
+      analyzer->const_int_bound(phy_indices[1]);
+  int tmem_phy_row_min = phy_row_bounds->min_value;
+  int tmem_phy_row_max = phy_row_bounds->max_value;
+  int tmem_phy_col_min = phy_col_bounds->min_value;
+  int tmem_phy_col_max = phy_col_bounds->max_value;
+  int tmem_phy_row_extent = tmem_phy_row_max - tmem_phy_row_min + 1;
+  int tmem_phy_col_extent = tmem_phy_col_max - tmem_phy_col_min + 1;
+  Range row_dom = Range(tmem_phy_row_min, tmem_phy_row_max + 1);
+  Range col_dom = Range(tmem_phy_col_min, tmem_phy_col_max + 1);
+
+  bool have_succeeded = false;
+  Stmt body;
+
+  auto try_tcgen05_instruction = [&](Tcgen05Meta meta) {
+    if (have_succeeded) {
+      return;
+    }
+    if (tmem_phy_row_min != 0 || tmem_phy_row_max != 127) {
+      return;
+    }
+    if (tmem_phy_col_min % meta.width != 0 ||
+        (tmem_phy_col_max + 1) % meta.width != 0) {
+      return;
+    }
+
+    for (int num_useful_wgs = num_threads / WARPGROUP_SIZE; num_useful_wgs >= 1;
+         num_useful_wgs--) {
+      int num_useful_threads = num_useful_wgs * WARPGROUP_SIZE;
+      auto [is_success, target_frag, num_chunks_each_wg] = expandTcgen05Layout(
+          meta, tmem_phy_col_extent, num_useful_threads, row_dom, col_dom);
+      if (!is_success) {
+        continue;
+      }
+
+      PrimExpr target_thread =
+          target_frag->ForwardThread(phy_indices, std::nullopt);
+      PrimExpr dst_thread =
+          dst_layout->ForwardThread(logical_indices, std::nullopt);
+      if (!analyzer->CanProveEqual(target_thread, dst_thread)) {
+        continue;
+      }
+      PrimExpr target_reg = target_frag->Forward(phy_indices)[0];
+      PrimExpr dst_reg = dst_layout->Forward(logical_indices)[0];
+      if (!analyzer->CanProveEqual(target_reg, dst_reg)) {
+        continue;
+      }
+
+      // All checks passed, we can use this instruction
+      PrimExpr relative_wg_idx =
+          FloorDiv(Sub(T.thread_var, T.thread_bounds->min), WARPGROUP_SIZE);
+      PrimExpr col_offset =
+          num_useful_threads == WARPGROUP_SIZE
+              ? PrimExpr(0)
+              : relative_wg_idx * (num_chunks_each_wg * meta.width);
+      have_succeeded = true;
+      Array<PrimExpr> args;
+      args.push_back(StringImm(meta.intrinsics_name + "<" +
+                               std::to_string(num_chunks_each_wg) + ">"));
+      args.push_back(
+          BufferLoad(src, {(int)logical_row_min,
+                           (int)logical_col_min})); // Will be translated later
+                                                    // in lower_shared_tmem pass
+      args.push_back(col_offset);
+      args.push_back(dst.access_ptr(2, DataType::Handle(), 1, 0,
+                                    PrimExpr(tmem_phy_col_extent)));
+
+      Stmt call =
+          Evaluate(Call(DataType::Handle(), builtin::call_extern(), args));
+      if (num_useful_threads != num_threads) {
+        body =
+            IfThenElse(T.thread_var < T.thread_bounds->min + num_useful_threads,
+                       call, // No-op for unused threads
+                       Stmt());
+      } else {
+        body = call;
+      }
+      break;
+    }
+  };
+
+  try_tcgen05_instruction(getTcgen05Meta_32dp32b());
+  try_tcgen05_instruction(getTcgen05Meta_32dp64b());
+  try_tcgen05_instruction(getTcgen05Meta_32dp128b());
+  try_tcgen05_instruction(getTcgen05Meta_32dp256b());
+
+  ICHECK(have_succeeded) << "Failed to find a suitable instruction for "
+                            "tcgen05.ld. Check your layout.";
+
+  return body;
+}
+
+/**
+ * @brief Lower a Copy operator to a bulk TMA (Tensor Memory Accelerator)
+ * transfer.
+ *
+ * Lowers the copy to an optimized TMA load or store when the target and buffer
+ * layouts permit. Constructs a TMADesc, detects shared-memory
+ * swizzle/interleave patterns, encodes global shape/stride/SMEM parameters, and
+ * emits either a 1D TMA transfer (when global/shared are contiguous and element
+ * counts match, currently only for loads) or a full multi-dimensional TMA call.
+ * The emitted statement is guarded so only the thread with min thread id
+ * executes the TMA.
+ *
+ * If preconditions are not satisfied (unsupported swizzle, stride/size limits,
+ * mismatched element counts, OOB risks, or other hardware constraints), this
+ * function falls back to LowerNormalCopy.
+ *
+ * @param T LowerArgs containing target information, thread/bounds variables,
+ *          and layout/ buffer remap information used for descriptor
+ * construction.
+ * @param analyzer Analyzer used to prove shapes/contiguity/equality
+ * constraints.
+ * @param copy_inst Indicates whether to emit a BulkLoad (TMA load) or BulkStore
+ *                  (TMA store). Must be CopyInst::kBulkLoad or kBulkStore.
+ * @return Stmt A TIR statement performing the bulk TMA copy (or the result of
+ *         LowerNormalCopy when falling back).
+ */
+Stmt CopyNode::LowerBulkCopy(const LowerArgs &T, arith::Analyzer *analyzer,
+                             CopyInst copy_inst) const {
+  ICHECK(copy_inst == CopyInst::kBulkLoad || copy_inst == CopyInst::kBulkStore)
+      << "Invalid copy inst " << static_cast<int>(copy_inst);
+  bool is_load = copy_inst == CopyInst::kBulkLoad;
+  Buffer global_tensor = is_load ? src : dst;
+  Buffer shared_tensor = is_load ? dst : src;
+  Array<Range> global_range = is_load ? src_range : dst_range;
+  Array<Range> shared_range = is_load ? dst_range : src_range;
+  // TMA bulk copy cannot support a non-swizzled global layout, will be fallback
+  // to normal copy
+  if (T.layout_map.count(global_tensor)) {
+    LOG(WARNING) << "TMA bulk copy cannot support a non-swizzled global "
+                    "layout, fallback to normal copy.";
+    return LowerNormalCopy(T, analyzer);
+  }
+
+  // linear layout must be computed before remapping
+  auto linear_layout = ComputeLinearLayout(shared_tensor);
+
+  Array<PrimExpr> shared_indices;
+  for (auto r : shared_range)
+    shared_indices.push_back(r->min);
+  std::vector<PrimExpr> shared_strides;
+  PrimExpr shared_stride = 1;
+  for (size_t i = 0; i < shared_tensor->shape.size(); i++) {
+    auto s = shared_tensor->shape[shared_tensor->shape.size() - i - 1];
+    shared_strides.insert(shared_strides.begin(), shared_stride);
+    shared_stride *= s;
+  }
+
+  Array<PrimExpr> global_indices;
+  for (auto r : global_range) {
+    global_indices.push_back(r->min);
+  }
+  std::vector<PrimExpr> global_strides;
+  PrimExpr global_stride = 1;
+  for (size_t i = 0; i < global_tensor->shape.size(); i++) {
+    auto s = global_tensor->shape[global_tensor->shape.size() - i - 1];
+    global_strides.insert(global_strides.begin(), global_stride);
+    global_stride *= s;
+  }
+
+  ICHECK(shared_strides.size() == shared_indices.size())
+      << "shared_strides.size() != shared_indices.size()"
+      << shared_strides.size() << " " << shared_indices.size();
+  PrimExpr shared_offset = 0;
+  for (size_t i = 0; i < shared_indices.size(); i++) {
+    shared_offset += shared_indices[i] * shared_strides[i];
+  }
+  PrimExpr global_offset = 0;
+  for (size_t i = 0; i < global_indices.size(); i++) {
+    global_offset += global_indices[i] * global_strides[i];
+  }
+
+  TMADesc desc;
+  // Verify copy rank
+  desc.rank = global_tensor->shape.size();
+  ICHECK(desc.rank >= 1 && desc.rank <= 5) << desc.rank;
+
+  // Verify datatype
+  ICHECK(global_tensor->dtype == shared_tensor->dtype)
+      << "Copy between buffer " << global_tensor->name << " and "
+      << shared_tensor->name << " with different data type "
+      << global_tensor->dtype << " and " << shared_tensor->dtype;
+
+  desc.data_type = to_CUtensorMapDataType(global_tensor->dtype);
+
+  // Global Tensor Shape and Stride
+  desc.global_addr = global_tensor->data;
+  desc.global_shape = ReverseArray(global_tensor->shape);
+  Array<PrimExpr> global_coords =
+      ReverseArray(global_range.Map([](Range r) { return r->min; }));
+  if (!global_tensor->strides.empty()) {
+    desc.global_stride = ReverseArray(global_tensor->strides);
+  } else {
+    // Create stride from shape
+    PrimExpr stride = 1;
+    desc.global_stride.reserve(desc.rank);
+    for (size_t i = 0; i < desc.rank; i++) {
+      desc.global_stride.push_back(stride);
+      stride *= desc.global_shape[i];
+    }
+  }
+  // The first stride element should be 1
+  ICHECK(is_one(desc.global_stride[0])) << desc.global_stride;
+  // Make global stride in bytes
+  desc.global_stride = desc.global_stride.Map([&](PrimExpr e) {
+    return cast(DataType::Int(64), e) * global_tensor->dtype.bytes();
+  });
+  for (size_t i{1}; i < desc.global_stride.size(); i++) {
+    auto stride = desc.global_stride[i].as<IntImmNode>();
+    if (stride != nullptr) {
+      // otherwise, the stride is symbolic, we need to check in future with
+      // assumptions
+      if (stride->value % 16 != 0 || stride->value >= (1ULL << 40)) {
+        LOG(WARNING) << "TMA bulk copy cannot support a global stride of "
+                     << desc.global_stride[i] << ", fallback to normal copy.";
+        return LowerNormalCopy(T, analyzer);
+      }
+    }
+  }
+
+  // Smem Box
+  // check smem range and global range is legal
+  auto s_range_idx = 0;
+  for (size_t i = 0; i < global_range.size(); i++) {
+    auto g_range = global_range[i];
+    if (is_one(g_range->extent)) {
+      continue;
+    }
+    // skip one range if it is 1
+    // in case of global range is [128, 64], while shared range is [1, 128, 64]
+    // A_shared[0, :, :].
+    while (is_one(shared_range[s_range_idx]->extent) &&
+           s_range_idx < shared_range.size()) {
+      s_range_idx++;
+    }
+    if (s_range_idx >= shared_range.size()) {
+      LOG(FATAL) << "TMA bulk copy cannot support a global range of "
+                 << global_range << ", shared_range " << shared_range;
+    }
+    auto s_range = shared_range[s_range_idx];
+    s_range_idx++;
+
+    ICHECK(StructuralEqual()(g_range->extent, s_range->extent))
+        << global_tensor->name << "[" << i << "] is illegal, "
+        << global_tensor->name << "[" << i << "] = " << g_range->extent << ", "
+        << shared_tensor->name << "[" << s_range_idx
+        << "] = " << s_range->extent;
+  }
+  // TODO(lei): find a much smarter way to deduce smem box dim
+  // instead of using global_range
+  desc.smem_box =
+      ReverseArray(global_range.Map([](Range r) { return r->extent; }));
+
+  desc.smem_stride = Array<PrimExpr>(desc.rank, PrimExpr(1));
+  // L2 & OOB
+  desc.l2_promotion = static_cast<int>(CU_TENSOR_MAP_L2_PROMOTION_L2_128B);
+  desc.oob_fill = static_cast<int>(CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE);
+
+  // Detect smem layout
+  // Shared memory swizzling is crucial for TMA performance
+  // It determines how data is arranged in shared memory banks to minimize bank
+  // conflicts Different swizzle patterns (32B, 64B, 128B) offer different
+  // trade-offs between access efficiency and memory usage
+  desc.interleave = static_cast<int>(CU_TENSOR_MAP_INTERLEAVE_NONE);
+  Layout shared_layout;
+  if (T.layout_map.count(shared_tensor)) {
+    shared_layout = T.layout_map.at(shared_tensor);
+    ICHECK(T.buffer_remap.count(shared_tensor))
+        << "shared_tensor: " << shared_tensor->name
+        << " not found in buffer_remap";
+    shared_tensor = T.buffer_remap.at(shared_tensor);
+  }
+  if (!shared_layout.defined()) {
+    desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_NONE);
+  } else if (StructuralEqual()(shared_layout, linear_layout)) {
+    desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_NONE);
+  } else {
+    ICHECK(shared_layout->InputDim() == 2) << "Cannot detect TMA layout.";
+    auto stride = as_const_int(shared_layout->InputShape()[0]);
+    auto continuous = as_const_int(shared_layout->InputShape()[1]);
+    ICHECK(stride != nullptr && continuous != nullptr);
+    // We also need to check if the shape satisfies the following doc:
+    // https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html#group__CUDA__TENSOR__MEMORY_1ga7c7d2aaac9e49294304e755e6f341d7
+    if (StructuralEqual()(shared_layout, makeQuarterBankSwizzleLayout(
+                                             *stride, *continuous,
+                                             shared_tensor->dtype.bits()))) {
+      desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_32B);
+    } else if (StructuralEqual()(
+                   shared_layout,
+                   makeHalfBankSwizzleLayout(*stride, *continuous,
+                                             shared_tensor->dtype.bits()))) {
+      desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_64B);
+    } else if (StructuralEqual()(
+                   shared_layout,
+                   makeFullBankSwizzleLayout(*stride, *continuous,
+                                             shared_tensor->dtype.bits()))) {
+      desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_128B);
+    } else if (StructuralEqual()(
+                   shared_layout,
+                   makeGemmABLayoutPadded(*stride, *continuous,
+                                          shared_tensor->dtype.bits()))) {
+      LOG(WARNING) << "Bulk copy cannot support a padded layout for src: "
+                   << src->name << ", dst: " << dst->name
+                   << ", fallback to normal copy";
+      return LowerNormalCopy(T, analyzer);
+    } else {
+      LOG(WARNING) << "Came across unsupported swizzle layout for src: "
+                   << src->name << ", dst: " << dst->name
+                   << ", fallback to normal copy";
+      return LowerNormalCopy(T, analyzer);
+    }
+  }
+
+  auto inner_box_dim = as_const_int(desc.smem_box[0]);
+  ICHECK(inner_box_dim != nullptr);
+  int instruction_dim = *inner_box_dim;
+  if (desc.swizzle == static_cast<int>(CU_TENSOR_MAP_SWIZZLE_64B)) {
+    instruction_dim = 64 / src->dtype.bytes();
+  } else if (desc.swizzle == static_cast<int>(CU_TENSOR_MAP_SWIZZLE_128B)) {
+    instruction_dim = 128 / src->dtype.bytes();
+  }
+  if (instruction_dim > 256) {
+    // smem_box dim must be in [0, 256]
+    // if is 512, we need to split the copy into two parts
+    ICHECK((*inner_box_dim) % 256 == 0)
+        << "inner_box_dim: " << *inner_box_dim << " is not divisible by 256";
+    instruction_dim = 256;
+  }
+  ICHECK((*inner_box_dim) % instruction_dim == 0)
+      << "inner_box_dim: " << *inner_box_dim
+      << " is not divisible by instruction_dim: " << instruction_dim;
+  desc.smem_box.Set(0, PrimExpr(instruction_dim));
+
+  int inner_box_dim_ = instruction_dim * shared_tensor->dtype.bytes();
+
+  // Check inner_box_dim_ for each swizzle type in a cleaner way
+  struct SwizzleCheck {
+    int swizzle;
+    int max_dim;
+  };
+  static const std::vector<SwizzleCheck> swizzle_checks = {
+      {static_cast<int>(CU_TENSOR_MAP_SWIZZLE_32B), 32},
+      {static_cast<int>(CU_TENSOR_MAP_SWIZZLE_64B), 64},
+      {static_cast<int>(CU_TENSOR_MAP_SWIZZLE_128B), 128},
+  };
+  for (const auto &check : swizzle_checks) {
+    if (desc.swizzle == check.swizzle && inner_box_dim_ > check.max_dim) {
+      LOG(WARNING) << "TMA bulk copy cannot support a swizzled global layout "
+                      "with inner_box_dim_ > "
+                   << check.max_dim << ", will be fallback to normal copy";
+      return LowerNormalCopy(T, analyzer);
+    }
+  }
+
+  Call create_descriptor =
+      Call(DataType::Handle(), create_tma_descriptor(), desc.EncodeCallArgs());
+
+  Array<PrimExpr> args;
+  args.reserve(desc.rank + 4);
+  args.push_back(create_descriptor);
+  if (is_load)
+    args.push_back(0); // mbarrier id placeholder
+  auto op = is_load ? tma_load() : tma_store();
+
+  Stmt tma_copy;
+  PrimExpr total_elements = 1;
+  for (auto e : desc.smem_box)
+    total_elements *= e;
+
+  if ((*inner_box_dim) != instruction_dim) {
+    Var loop_var("i");
+    int loop_extent = (*inner_box_dim) / instruction_dim;
+
+    PrimExpr shared_addr = shared_tensor.access_ptr(
+        is_load ? 2 : 1, DataType::Handle(), 1,
+        shared_offset + total_elements * loop_var, total_elements);
+    args.push_back(shared_addr);
+    global_coords.Set(0, global_coords[0] + instruction_dim * loop_var);
+    for (auto coord : global_coords)
+      args.push_back(coord);
+    int need_reduce = 0;
+    if (!is_load)
+      args.push_back(need_reduce);
+    args.push_back(this->eviction_policy);
+    tma_copy = For(loop_var, 0, loop_extent, ForKind::kUnrolled,
+                   Evaluate(Call(DataType::Handle(), op, args)));
+  } else {
+    PrimExpr shared_addr = shared_tensor.access_ptr(
+        is_load ? 2 : 1, DataType::Handle(), 1, shared_offset, total_elements);
+    args.push_back(shared_addr);
+    for (auto coord : global_coords)
+      args.push_back(coord);
+    int need_reduce = 0;
+    if (!is_load)
+      args.push_back(need_reduce);
+    args.push_back(this->eviction_policy);
+    tma_copy = Evaluate(Call(DataType::Handle(), op, args));
+  }
+  tma_copy = IfThenElse(EQ(T.thread_var, T.thread_bounds->min), tma_copy);
+
+  return tma_copy;
+}
+
+Stmt CopyNode::LowerBulkCopy1D(const LowerArgs &T, arith::Analyzer *analyzer,
+                               CopyInst copy_inst) const {
+  ICHECK(copy_inst == CopyInst::kBulkLoad1D ||
+         copy_inst == CopyInst::kBulkStore1D);
+
+  // Add 1D TMA copy when the global and shared memory is contiguous
+  // Check if shared_tensor->name is present in T.buffer_var_gemm
+  // (Array<PrimExpr>) to avoid use 1D TMA copy for swizzled layout
+  bool is_load = copy_inst == CopyInst::kBulkLoad1D;
+  auto shared_range = is_load ? dst_range : src_range;
+  auto global_range = is_load ? src_range : dst_range;
+  auto shared_tensor = is_load ? dst : src;
+  auto global_tensor = is_load ? src : dst;
+
+  PrimExpr shared_elements = 1;
+  for (size_t i = 0; i < shared_range.size(); i++) {
+    shared_elements *= shared_range[i]->extent;
+  }
+
+  std::vector<PrimExpr> shared_strides;
+  PrimExpr shared_stride = 1;
+  for (size_t i = 0; i < shared_tensor->shape.size(); i++) {
+    auto s = shared_tensor->shape[shared_tensor->shape.size() - i - 1];
+    shared_strides.insert(shared_strides.begin(), shared_stride);
+    shared_stride *= s;
+  }
+
+  Array<PrimExpr> shared_indices;
+  for (auto r : shared_range)
+    shared_indices.push_back(r->min);
+
+  Array<PrimExpr> global_indices;
+  for (auto r : global_range) {
+    global_indices.push_back(r->min);
+  }
+  std::vector<PrimExpr> global_strides;
+  PrimExpr global_stride = 1;
+  for (size_t i = 0; i < global_tensor->shape.size(); i++) {
+    auto s = global_tensor->shape[global_tensor->shape.size() - i - 1];
+    global_strides.insert(global_strides.begin(), global_stride);
+    global_stride *= s;
+  }
+
+  PrimExpr global_offset = 0;
+  for (size_t i = 0; i < global_indices.size(); i++) {
+    global_offset += global_indices[i] * global_strides[i];
+  }
+
+  PrimExpr shared_offset = 0;
+  for (size_t i = 0; i < shared_indices.size(); i++) {
+    shared_offset += shared_indices[i] * shared_strides[i];
+  }
+
+  PrimExpr elements = analyzer->Simplify(shared_elements);
+  PrimExpr shared_addr = shared_tensor.access_ptr(
+      is_load ? 2 : 1, DataType::Handle(), 1, shared_offset, elements);
+  PrimExpr global_addr = global_tensor.access_ptr(
+      is_load ? 1 : 2, DataType::Handle(), 1, global_offset, elements);
+  Stmt tma_copy;
+  if (is_load) {
+    // the zero is a placeholder for mbarrier ids
+    tma_copy = Evaluate(
+        Call(DataType::Handle(), tma_load(),
+             {shared_addr, global_addr, 0,
+              elements * shared_tensor->dtype.bytes(), this->eviction_policy}));
+  } else {
+    int need_reduce = 0;
+    tma_copy = Evaluate(
+        Call(DataType::Handle(), tma_store(),
+             {global_addr, shared_addr, elements * shared_tensor->dtype.bytes(),
+              need_reduce, this->eviction_policy}));
+  }
+  tma_copy = IfThenElse(EQ(T.thread_var, T.thread_bounds->min), tma_copy);
+  return tma_copy;
+}
+/*!
+ * \brief Encode the TMA descriptor into an array of PrimExpr.
+ * This function serializes the TMA descriptor fields into a format suitable for
+ * passing to the create_tma_descriptor() builtin function. The encoding follows
+ * the expected argument order for the TMA descriptor creation.
+ * \return Array of PrimExpr representing the encoded TMA descriptor.
+ */
+Array<PrimExpr> TMADesc::EncodeCallArgs() const {
+  Array<PrimExpr> args;
+  args.reserve(rank * 4 + 7);
+
+  args.push_back(data_type);
+  args.push_back(static_cast<int>(rank));
+  args.push_back(global_addr);
+  for (auto e : global_shape)
+    args.push_back(e);
+  for (auto e : global_stride)
+    args.push_back(e);
+  for (auto e : smem_box)
+    args.push_back(e);
+  for (auto e : smem_stride)
+    args.push_back(e);
+  args.push_back(interleave);
+  args.push_back(swizzle);
+  args.push_back(l2_promotion);
+  args.push_back(oob_fill);
+
+  return args;
+}
+
+/**
+ * @brief Construct a Conv2DIm2ColOp node.
+ *
+ * Initializes a Conv2DIm2ColOpNode from raw TL-call arguments and a buffer map.
+ * The constructor extracts source and destination Buffers from vmap and reads
+ * convolution parameters encoded in args:
+ * - args[0]: source tensor access pointer
+ * - args[1]: destination tensor access pointer
+ * - args[2]: nhw_step (PrimExpr)
+ * - args[3]: c_step (PrimExpr)
+ * - args[4]: kernel (IntImm)
+ * - args[5]: stride (IntImm)
+ * - args[6]: dilation (IntImm)
+ * - args[7]: padding (IntImm)
+ * - args[8]: eviction_policy (IntImm)
+ *
+ * The created node stores these values (src, dst, nhw_step, c_step, kernel,
+ * stride, dilation, padding, eviction_policy) for later lowering to TMA-based
+ * GPU intrinsics.
+ *
+ * @param args Array of PrimExpr TL-call arguments (see list above).
+ * @param vmap Mapping from original buffer variables to actual Buffer objects.
+ */
+Conv2DIm2ColOp::Conv2DIm2ColOp(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<Conv2DIm2ColOpNode> node = make_object<Conv2DIm2ColOpNode>();
+  node->src = vmap[GetVarFromAccessPtr(args[0])];
+  node->dst = vmap[GetVarFromAccessPtr(args[1])];
+  node->nhw_step = args[2];
+  node->c_step = args[3];
+  node->kernel = args[4].as<IntImm>().value()->value;
+  node->stride = args[5].as<IntImm>().value()->value;
+  node->dilation = args[6].as<IntImm>().value()->value;
+  node->padding = args[7].as<IntImm>().value()->value;
+  node->eviction_policy = args[8].as<IntImm>().value()->value;
+  data_ = std::move(node);
+}
+
+/**
+ * @brief Create a shallow copy of this Conv2DIm2ColOpNode wrapped as a
+ * TileOperator.
+ *
+ * Produces a new Conv2DIm2ColOp that owns a freshly allocated
+ * Conv2DIm2ColOpNode initialized from this node (member-wise copy). This is
+ * used to duplicate the operator node for compiler passes that require
+ * independent operator instances.
+ *
+ * @return TileOperator A TileOperator containing the cloned Conv2DIm2ColOpNode.
+ */
+TileOperator Conv2DIm2ColOpNode::Clone() const {
+  auto op = make_object<Conv2DIm2ColOpNode>(*this);
+  return Conv2DIm2ColOp(op);
+}
+
+/**
+ * @brief Lower Conv2D im2col into a TMA-backed PTX sequence for Hopper.
+ *
+ * Constructs a TMA im2col descriptor from the Conv2DIm2ColOp parameters
+ * (kernel, stride, dilation, padding, channel/image tiling, dtype and shapes),
+ * emits a call to create the im2col descriptor, and returns a statement that
+ * invokes the corresponding tma_load_im2col builtin guarded to a single
+ * thread. The lowering assumes the destination resides in shared memory and the
+ * source in global memory and uses the provided layout information (when
+ * available) to select the appropriate shared-memory swizzle.
+ *
+ * Preconditions (checked with ICHECK):
+ * - Target is Hopper.
+ * - src.scope() == "global" and dst.scope() is "shared.dyn" or "shared".
+ * - src->shape has rank 4 and dst->shape has rank 2.
+ * - src and dst have the same dtype.
+ * - When a shared layout is supplied it must match a recognized TMA swizzle
+ *   pattern (32B/64B/128B) or an ICHECK will fail.
+ *
+ * @param T Lowering context (target, layout map, thread_var, thread_bounds,
+ *          buffer remapping, etc.). Used to fetch target/layout and to emit a
+ *          thread-guarded TMA call.
+ * @param analyzer Arithmetic analyzer used to prove divisibility and simplify
+ *                 expressions required by descriptor construction.
+ * @return Stmt A TIR statement that performs a tma_load_im2col call wrapped in
+ *              a thread-min guard (IfThenElse). The returned statement is ready
+ *              to be inserted into the lowered TIR.
+ */
+Stmt Conv2DIm2ColOpNode::Lower(const LowerArgs &T,
+                               arith::Analyzer *analyzer) const {
+  ICHECK(TargetIsHopper(T.target));
+  ICHECK(src.scope() == "global" &&
+         (dst.scope() == "shared.dyn" || dst.scope() == "shared"));
+  ICHECK(src->shape.size() == 4);
+  ICHECK(dst->shape.size() == 2);
+  ICHECK(src->dtype == dst->dtype);
+  Layout shared_layout;
+  if (T.layout_map.count(dst)) {
+    shared_layout = T.layout_map[dst];
+  }
+
+  TMAIm2ColDesc desc;
+  desc.rank = src->shape.size();
+  desc.data_type = to_CUtensorMapDataType(src->dtype);
+  desc.global_addr = src->data;
+  desc.global_shape = ReverseArray(src->shape);
+
+  if (!src->strides.empty()) {
+    desc.global_stride = ReverseArray(src->strides);
+  } else {
+    // Create stride from shape
+    PrimExpr stride = 1;
+    desc.global_stride.reserve(desc.rank);
+    for (size_t i = 0; i < desc.rank; i++) {
+      desc.global_stride.push_back(stride);
+      stride *= desc.global_shape[i];
+    }
+  }
+  // The first stride element should be 1
+  ICHECK(is_one(desc.global_stride[0])) << desc.global_stride;
+  // Make global stride in bytes
+  desc.global_stride = desc.global_stride.Map([&](PrimExpr e) {
+    return cast(DataType::Int(64), e) * src->dtype.bytes();
+  });
+  desc.elem_stride = {1, stride, stride, 1};
+  desc.lower_corner = {-padding, -padding};
+  desc.upper_corner = {-padding, -padding};
+  desc.smem_box_pixel = Downcast<IntImm>(dst->shape[0])->value;
+  desc.smem_box_channel = Downcast<IntImm>(dst->shape[1])->value;
+  desc.l2_promotion = static_cast<int>(CU_TENSOR_MAP_L2_PROMOTION_L2_128B);
+  desc.oob_fill = static_cast<int>(CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE);
+  desc.interleave = static_cast<int>(CU_TENSOR_MAP_INTERLEAVE_NONE);
+  if (!shared_layout.defined()) {
+    desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_NONE);
+  } else {
+    ICHECK(shared_layout->InputDim() == 2) << "Cannot detect TMA layout.";
+    auto stride = as_const_int(shared_layout->InputShape()[0]);
+    auto continuous = as_const_int(shared_layout->InputShape()[1]);
+    ICHECK(stride != nullptr && continuous != nullptr);
+
+    if (StructuralEqual()(shared_layout,
+                          makeQuarterBankSwizzleLayout(*stride, *continuous,
+                                                       dst->dtype.bits()))) {
+      desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_32B);
+    } else if (StructuralEqual()(shared_layout, makeHalfBankSwizzleLayout(
+                                                    *stride, *continuous,
+                                                    dst->dtype.bits()))) {
+      desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_64B);
+    } else if (StructuralEqual()(shared_layout, makeFullBankSwizzleLayout(
+                                                    *stride, *continuous,
+                                                    dst->dtype.bits()))) {
+      desc.swizzle = static_cast<int>(CU_TENSOR_MAP_SWIZZLE_128B);
+    } else {
+      ICHECK(0) << "Cannot detect TMA layout.";
+    }
+  }
+
+  Call create_desc = Call(DataType::Handle(), create_tma_im2col_descriptor(),
+                          desc.EncodeCallArgs());
+
+  Array<PrimExpr> global_coords; // c, w, h, n
+  Array<PrimExpr> image_offset;  // w, h
+  global_coords.reserve(desc.rank);
+
+  ICHECK(analyzer->CanProveEqual(
+      FloorMod(desc.global_shape[0], desc.smem_box_channel), 0))
+      << "Currently can only support divisible channel case";
+
+  global_coords.push_back(
+      FloorMod(c_step * desc.smem_box_channel, desc.global_shape[0]));
+  image_offset.push_back(
+      dilation *
+      FloorMod(FloorDiv(c_step * desc.smem_box_channel, desc.global_shape[0]),
+               kernel));
+  image_offset.push_back(dilation * FloorDiv(c_step * desc.smem_box_channel,
+                                             desc.global_shape[0] * kernel));
+
+  PrimExpr h_dim =
+      FloorDiv(src->shape[1] + 2 * padding - (kernel - 1) * dilation - 1,
+               stride) +
+      1;
+  PrimExpr w_dim =
+      FloorDiv(src->shape[2] + 2 * padding - (kernel - 1) * dilation - 1,
+               stride) +
+      1;
+  global_coords.push_back(
+      stride * FloorMod(nhw_step * desc.smem_box_pixel, w_dim) - padding);
+  global_coords.push_back(
+      stride *
+          FloorMod(FloorDiv(nhw_step * desc.smem_box_pixel, w_dim), h_dim) -
+      padding);
+  global_coords.push_back(
+      FloorDiv(nhw_step * desc.smem_box_pixel, w_dim * h_dim));
+
+  Array<PrimExpr> args;
+  args.reserve(desc.rank * 2 + 2);
+  args.push_back(create_desc);
+  args.push_back(0); // mbar placeholder
+  auto dst_buffer = T.buffer_remap.count(dst) ? T.buffer_remap[dst] : dst;
+  auto shared_addr = dst_buffer.access_ptr(2);
+  args.push_back(shared_addr);
+  for (auto coord : global_coords)
+    args.push_back(coord);
+  for (auto offset : image_offset)
+    args.push_back(offset);
+  args.push_back(this->eviction_policy);
+  Stmt tma_copy =
+      IfThenElse(EQ(T.thread_var, T.thread_bounds->min),
+                 Evaluate(Call(DataType::Handle(), tma_load_im2col(), args)));
+  return tma_copy;
+}
+
+/*!
+ * \brief Encode the TMA im2col descriptor into an array of PrimExpr.
+ * This function serializes the TMA im2col descriptor fields for passing to the
+ * create_tma_im2col_descriptor() builtin function. It includes
+ * convolution-specific parameters like kernel size, stride, padding, and
+ * dilation in addition to standard tensor descriptor fields. \return Array of
+ * PrimExpr representing the encoded TMA im2col descriptor.
+ */
+Array<PrimExpr> TMAIm2ColDesc::EncodeCallArgs() const {
+  Array<PrimExpr> args;
+  args.reserve(rank * 5 + 5);
+
+  args.push_back(data_type);
+  args.push_back(static_cast<int>(rank));
+  args.push_back(global_addr);
+  for (auto e : global_shape)
+    args.push_back(e);
+  for (auto e : global_stride)
+    args.push_back(e);
+  for (auto e : elem_stride)
+    args.push_back(e);
+  for (auto e : lower_corner)
+    args.push_back(e);
+  for (auto e : upper_corner)
+    args.push_back(e);
+  args.push_back(smem_box_pixel);
+  args.push_back(smem_box_channel);
+  args.push_back(interleave);
+  args.push_back(swizzle);
+  args.push_back(l2_promotion);
+  args.push_back(oob_fill);
+
+  return args;
+}
+
+// Register the Copy operation with TVM's TIR system
+// This makes the copy operation available for use in TVM programs
+// - Takes 5 inputs: src_buffer, dst_buffer, coalesced_width, disable_tma,
+// eviction_policy
+// - Marked as opaque since it has side effects (memory writes)
+TIR_REGISTER_TL_OP(Copy, copy)
+    .set_num_inputs(5)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+/**
+ * @brief Layout inference hook for Conv2DIm2ColOpNode.
+ *
+ * This operator does not provide any layout inference; the function
+ * intentionally returns an empty LayoutMap to indicate no layout suggestions.
+ *
+ * @param T Context for layout inference (ignored).
+ * @param level Inference level (ignored).
+ * @return LayoutMap An empty map.
+ */
+LayoutMap Conv2DIm2ColOpNode::InferLayout(const LayoutInferArgs &T,
+                                          InferLevel level) const {
+  return {};
+}
+
+// Register the Conv2DIm2Col operation with TVM's TIR system
+// This operation performs im2col transformation for 2D convolutions using TMA
+// - Takes 9 inputs: src_buffer, dst_buffer, nhw_step, c_step, kernel, stride,
+// dilation, padding, eviction_policy
+// - Marked as opaque since it has side effects (memory writes)
+TIR_REGISTER_TL_OP(Conv2DIm2ColOp, c2d_im2col)
+    .set_num_inputs(9)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TVM_FFI_STATIC_INIT_BLOCK({
+  CopyNode::RegisterReflection();
+  Conv2DIm2ColOpNode::RegisterReflection();
+});
+} // namespace tl
+} // namespace tvm
diff --git a/src/op/copy.h b/src/op/copy.h
new file mode 100644
index 00000000..00d07f16
--- /dev/null
+++ b/src/op/copy.h
@@ -0,0 +1,390 @@
+/*!
+ * \file tl/op/copy.h
+ * \brief Copy operations and Tensor Memory Access (TMA) descriptors
+ */
+
+#ifndef TVM_TL_OP_COPY_H_
+#define TVM_TL_OP_COPY_H_
+
+#include "operator.h"
+#include "parallel.h"
+
+namespace tvm {
+namespace tl {
+using namespace tir;
+
+/// Copy instruction types for different memory access patterns
+enum class CopyInst : uint8_t {
+  kNormal = 0,    // utilize ldg/stg or cpasync or any buffer copy
+  kLDSM = 1,      // ldmatrix memory copy
+  kSTSM = 2,      // stmatrix memory copy
+  kBulkLoad = 3,  // utilize tma load
+  kBulkStore = 4, // utilize tma store
+  // we should separate the bulk load and store for 1d and multi-dim
+  // as they have different memory access patterns
+  kBulkLoad1D = 5,  // utilize tma load 1d
+  kBulkStore1D = 6, // utilize tma store 1d
+  kTMemLoad = 7,    // tcgen05.ld (tensor memory -> register)
+  kTMemStore = 8,   // tcgen05.st (register -> tensor memory)
+};
+
+/// Descriptor for Tensor Memory Access (TMA) copy operations
+struct TMADesc {
+  size_t rank;                   ///< Tensor rank (number of dimensions)
+  int data_type;                 ///< Data type identifier
+  Array<PrimExpr> global_shape;  ///< Shape in global memory
+  Array<PrimExpr> global_stride; ///< Strides in global memory
+  Array<PrimExpr> smem_box;      ///< Block shape in shared memory
+  Array<PrimExpr> smem_stride;   ///< Strides in shared memory
+  PrimExpr global_addr;          ///< Base address in global memory
+  int swizzle;                   ///< Memory layout swizzle parameter
+  int interleave;                ///< Memory interleave parameter
+  int oob_fill;                  ///< Out-of-bound fill policy
+  int l2_promotion;              ///< L2 cache promotion flag
+
+  /// Encode descriptor fields into runtime call arguments
+  Array<PrimExpr> EncodeCallArgs() const;
+};
+
+/*!
+ * \brief Descriptor for TMA-based im2col transformation used in Conv2D.
+ *
+ * This supports extracting patches from the input image (im2col)
+ * for convolution lowering, storing them in shared memory.
+ */
+struct TMAIm2ColDesc {
+  size_t rank;                   // Rank of the tensor
+  int data_type;                 // Data type identifier
+  Array<PrimExpr> global_shape;  // Shape of input tensor in global memory
+  Array<PrimExpr> global_stride; // Stride in global memory
+  Array<PrimExpr> elem_stride;   // Stride at element level (per axis)
+  Array<PrimExpr> lower_corner; // Lower bound offsets for the extraction window
+                                // (rank - 2 dims)
+  Array<PrimExpr> upper_corner; // Upper bound offsets for the extraction window
+                                // (rank - 2 dims)
+  PrimExpr global_addr;         // Base address in global memory
+  int smem_box_pixel;           // Pixel dimension of shared memory box
+  int smem_box_channel;         // Channel dimension of shared memory box
+  int swizzle;                  // Memory swizzle setting
+  int interleave;               // Memory interleaving setting
+  int oob_fill;                 // Out-of-bound fill policy
+  int l2_promotion;             // Whether to enable L2 cache promotion
+
+  /*!
+   * \brief Encode descriptor fields into runtime arguments.
+   */
+  Array<PrimExpr> EncodeCallArgs() const;
+};
+
+/*!
+ * \brief Get TVM Op handle for Conv2DIm2Col.
+ */
+
+/*!
+ * \brief Clone this Conv2DIm2Col operator.
+ *
+ * Returns a TileOperator reference that is a shallow clone of this operator.
+ */
+class CopyNode : public TileOperatorNode {
+public:
+  Buffer src, dst;                   // Source and destination buffers
+  Array<Range> src_range, dst_range; // Ranges for each dimension in src and dst
+  IntImm coalesced_width; // Width (in elements) for coalesced memory access
+  Bool disable_tma = Bool(false); // Whether to disable TMA acceleration
+
+  mutable ParallelOp par_op_; // Optional associated parallelization operator
+
+  enum class EvictionPolicy : uint8_t {
+    kEvictNormal = 0,
+    kEvictFirst = 1,
+    kEvictLast = 2,
+  };
+
+  uint8_t eviction_policy; // Policy for cache eviction
+  static constexpr const char *_type_key = "tl.Copy";
+  TVM_DECLARE_FINAL_OBJECT_INFO(CopyNode, TileOperatorNode);
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<CopyNode>()
+        .def_ro("src", &CopyNode::src)
+        .def_ro("dst", &CopyNode::dst)
+        .def_ro("src_range", &CopyNode::src_range)
+        .def_ro("dst_range", &CopyNode::dst_range)
+        .def_ro("coalesced_width", &CopyNode::coalesced_width);
+  }
+
+  bool SEqualReduce(const CopyNode *other, SEqualReducer equal) const {
+    return equal(src, other->src) && equal(dst, other->dst) &&
+           equal(src_range, other->src_range) &&
+           equal(dst_range, other->dst_range) &&
+           equal(coalesced_width, other->coalesced_width);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(src);
+    hash_reduce(dst);
+    hash_reduce(src_range);
+    hash_reduce(dst_range);
+    hash_reduce(coalesced_width);
+  }
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  /*!
+   * \brief Lower the copy operator to a TIR statement.
+   * \param T        Arguments for lowering.
+   * \param analyzer Analyzer for simplification and bounds checks.
+   */
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+
+  /*!
+   * \brief Infer buffer layouts after applying this operator.
+   * \param T     Arguments for layout inference.
+   * \param level Level of inference (basic or detailed).
+   */
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+
+  /*!
+   * \brief Check if bulk copy is supported.
+   */
+  bool CheckBulkLoad(Target target, arith::Analyzer *analyzer,
+                     bool check_last_dim = true) const;
+
+  /*!
+   * \brief Check if bulk store is supported.
+   */
+  bool CheckBulkStore(Target target, arith::Analyzer *analyzer,
+                      bool check_last_dim = true) const;
+
+  /*!
+   * \brief Check if bulk copy 1d load is supported.
+   */
+  bool CheckBulkLoad1D(Target target, const LayoutMap &layout_map,
+                       arith::Analyzer *analyzer) const;
+
+  /*!
+   * \brief Check if bulk copy 1d store is supported.
+   */
+  bool CheckBulkStore1D(Target target, const LayoutMap &layout_map,
+                        arith::Analyzer *analyzer) const;
+
+  /*!
+   * \brief Check if bulk copy 1d is supported.
+   */
+  bool CheckBulkCopy1D(const Buffer &global_tensor, const Buffer &shared_tensor,
+                       const Array<Range> &global_range,
+                       const Array<Range> &shared_range,
+                       const LayoutMap &layout_map,
+                       arith::Analyzer *analyzer) const;
+
+  /*!
+   * \brief Check if lds memory copy is supported.
+   */
+  bool CheckLDSMCopy(Target target) const;
+
+  /*!
+   * \brief Check if stsm memory copy is supported.
+   */
+  bool CheckSTSMCopy(Target target) const;
+
+  /*!
+   * \brief Check if tensor memory load is supported.
+   */
+  bool CheckTMemLoad(Target target) const;
+
+  /*!
+   * \brief Check if tensor memory store is supported.
+   */
+  bool CheckTMemStore(Target target) const;
+
+  /*!
+   * \brief Get the copy instruction type.
+   */
+  CopyInst GetCopyInst(Target target, bool disable_tma_lower,
+                       const LayoutMap &layout_map, arith::Analyzer *analyzer,
+                       bool buffer_oob) const;
+
+protected:
+  /*!
+   * \brief Generate lowering for bulk/global-to-shared copy.
+   */
+  Stmt LowerBulkCopy(const LowerArgs &T, arith::Analyzer *analyzer,
+                     CopyInst copy_inst) const;
+
+  /*!
+   * \brief Generate lowering for bulk copy 1d.
+   */
+  Stmt LowerBulkCopy1D(const LowerArgs &T, arith::Analyzer *analyzer,
+                       CopyInst copy_inst) const;
+
+  /*!
+   * \brief Generate lowering for LDS Memory Copy (shared memory to shared
+   * memory or smem usage).
+   */
+  Stmt LowerLDSMCopy(const LowerArgs &T, arith::Analyzer *analyzer,
+                     CopyInst copy_inst) const;
+
+  /*!
+   * \brief Generate lowering for tensor memory copy (tcgen05.ld/st/cp).
+   */
+  Stmt LowerTmemCopy(const LowerArgs &T, arith::Analyzer *analyzer) const;
+
+  /*!
+   * \brief Generate lowering for normal copy.
+   */
+  Stmt LowerNormalCopy(const LowerArgs &T, arith::Analyzer *analyzer) const;
+
+  /*!
+   * \brief Generate SIMT (thread-level) loop for copying.
+   */
+  For MakeSIMTLoop(arith::Analyzer *analyzer) const;
+
+  /*!
+   * \brief Compute linear layout for tma copy.
+   */
+  Layout ComputeLinearLayout(const Buffer &shared_tensor) const;
+
+  /*!
+   * \brief Create iterator variables for multi-dimensional copy loops.
+   */
+  Array<IterVar> MakeIterVars() const;
+
+  /*!
+   * \brief Calculate source or destination indices from iteration vars.
+   * \param ivs      Iterator variables from MakeIterVars().
+   * \param src_dst  0 = make source indices, 1 = make destination indices.
+   */
+  Array<PrimExpr> MakeIndices(const Array<IterVar> &ivs, int src_dst) const;
+
+  /*!
+   * \brief Construct the boundary predicate for valid copy (to avoid OOB).
+   * \param analyzer  Arithmetic analyser for simplification.
+   * \param ivs       Iterator variables.
+   * \param extents   Extent expressions for the relevant buffer.
+   * \param src_dst   0 = predicate for source, 1 = predicate for destination.
+   */
+  PrimExpr MakePredicate(arith::Analyzer *analyzer, const Array<IterVar> &ivs,
+                         Array<PrimExpr> extents, int src_dst) const;
+
+  /**
+   * \brief Create a deep copy of this operator.
+   *
+   * Returns a TileOperator that is a copy of the current node, preserving all
+   * configuration (buffers, parameters, and layout-related fields).
+   * @return A TileOperator owning the cloned operator node.
+   */
+
+  /**
+   * \brief Constructor.
+   * \param args Expression arguments for the Conv2D im2col operator.
+   * \param vmap Buffer variable mapping.
+   */
+
+  /**
+   * \brief Get the TVM Op handle corresponding to this Conv2DIm2Col operator.
+   * @return Reference to the singleton TVM Op representing this operator.
+   */
+  TileOperator Clone() const;
+};
+
+class Copy : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(Copy, TileOperator, CopyNode);
+
+  /*!
+   * \brief Constructor.
+   * \param args  Expression arguments for the copy.
+   * \param vmap  Buffer variable mapping.
+   */
+  TVM_DLL Copy(Array<PrimExpr> args, BufferMap vmap);
+
+  /*!
+   * \brief Get the TVM Op handle corresponding to this Copy op.
+   */
+  static const Op &Get();
+};
+
+/*!
+ * \brief Special operator for Conv2D im2col transformation.
+ *
+ * This operator converts input image layout into columnar format suitable
+ * for matrix multiplication-based convolution lowering.
+ */
+class Conv2DIm2ColOpNode : public TileOperatorNode {
+public:
+  Buffer src, dst; // Source (input feature map) and destination (im2col matrix)
+  int stride;      // Stride for convolution
+  int padding;     // Padding amount
+  int dilation;    // Dilation factor
+  int kernel;      // Kernel size
+  int eviction_policy; // Cache eviction policy
+  PrimExpr nhw_step;   // Step size in NHW dimensions
+  PrimExpr c_step;     // Step size in channel dimension
+
+  static constexpr const char *_type_key = "tl.Conv2DIm2Col";
+  TVM_DECLARE_FINAL_OBJECT_INFO(Conv2DIm2ColOpNode, TileOperatorNode);
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<Conv2DIm2ColOpNode>()
+        .def_ro("src", &Conv2DIm2ColOpNode::src)
+        .def_ro("dst", &Conv2DIm2ColOpNode::dst)
+        .def_ro("stride", &Conv2DIm2ColOpNode::stride)
+        .def_ro("padding", &Conv2DIm2ColOpNode::padding)
+        .def_ro("dilation", &Conv2DIm2ColOpNode::dilation)
+        .def_ro("kernel", &Conv2DIm2ColOpNode::kernel)
+        .def_ro("eviction_policy", &Conv2DIm2ColOpNode::eviction_policy);
+  }
+
+  bool SEqualReduce(const Conv2DIm2ColOpNode *other,
+                    SEqualReducer equal) const {
+    return equal(src, other->src) && equal(dst, other->dst) &&
+           equal(stride, other->stride) && equal(padding, other->padding) &&
+           equal(dilation, other->dilation) && equal(kernel, other->kernel) &&
+           equal(eviction_policy, other->eviction_policy);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(src);
+    hash_reduce(dst);
+    hash_reduce(stride);
+    hash_reduce(padding);
+    hash_reduce(dilation);
+    hash_reduce(kernel);
+    hash_reduce(eviction_policy);
+  }
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  /*!
+   * \brief Lower to TIR statement.
+   */
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+
+  /*!
+   * \brief Infer layout for this operator.
+   */
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+
+  /*!
+   * \brief Get TVM Op handle.
+   */
+  static const Op &Get();
+  TileOperator Clone() const;
+};
+
+class Conv2DIm2ColOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(Conv2DIm2ColOp, TileOperator,
+                                Conv2DIm2ColOpNode);
+  TVM_DLL Conv2DIm2ColOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+} // namespace tl
+} // namespace tvm
+
+#endif // TVM_TL_OP_COPY_H_
\ No newline at end of file
diff --git a/src/op/distributed.cc b/src/op/distributed.cc
new file mode 100644
index 00000000..84a23afa
--- /dev/null
+++ b/src/op/distributed.cc
@@ -0,0 +1,216 @@
+/*!
+ * \file tl/op/distributed.cc
+ * \brief Distributed intrinsics.
+ *
+ */
+
+#include "distributed.h"
+
+#include <tvm/tir/builtin.h>
+#include <tvm/tir/op.h>
+#include <tvm/tir/op_attr_types.h>
+
+#include "../target/cuda.h"
+#include "../target/utils.h"
+
+namespace tvm {
+namespace tl {
+
+#define TIR_DEFINE_TL_BUILTIN(OpName)                                          \
+  const Op &OpName() {                                                         \
+    static const Op &op = Op::Get("tl." #OpName);                              \
+    return op;                                                                 \
+  }                                                                            \
+  TVM_REGISTER_OP("tl." #OpName)                                               \
+      .set_attr<TScriptPrinterName>("TScriptPrinterName", #OpName)
+
+// TODO: check the effect kind and num_inputs
+TIR_DEFINE_TL_BUILTIN(GetPE).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(GetPENum).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(IntPE).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(BarrierAll)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(BarrierAllBlock)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(BarrierAllWarp)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(SyncAll).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(SyncAllBlock)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(SyncAllWarp)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(Quiet).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(Fence).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(GetmemNbiBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(GetmemBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(GetmemNbiWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(GetmemWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(GetmemNbi).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(Getmem).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemNbiBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemNbiWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(Putmem).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemNbi).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemSignal)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemSignalNbi)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemSignalBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemSignalNbiBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemSignalWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(PutmemSignalNbiWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(SignalOp).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(SignalWaitUntil)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(Broadcast).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(BroadcastWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(BroadcastBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(BroadcastmemBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(Fcollect).set_num_inputs(-1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(FcollectWarp)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(FcollectBlock)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(CpengineCpAsync)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(get_rank).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(get_num_ranks)
+    .set_num_inputs(0)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(get_remote_base_ptr)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(get_uintptr_t)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+} // namespace tl
+} // namespace tvm
\ No newline at end of file
diff --git a/src/op/distributed.h b/src/op/distributed.h
new file mode 100644
index 00000000..5170cc7e
--- /dev/null
+++ b/src/op/distributed.h
@@ -0,0 +1,234 @@
+/*!
+ * \file tl/op/distributed.h
+ * \brief Distributed intrinsics.
+ *
+ */
+
+#include "operator.h"
+#include <tvm/ir/transform.h>
+
+namespace tvm {
+namespace tl {
+
+/*!
+ * \brief tvm intrinsics for getting the PE id
+ *
+ * int GetPE()
+ *
+ */
+const Op &GetPE();
+
+/*!
+ * \brief tvm intrinsics for getting the total number of PEs
+ */
+const Op &GetPENum();
+
+/*!
+ * \brief tvm intrinsics for getting the PE id
+ *
+ * int IntPE()
+ *
+ */
+const Op &IntPE();
+
+/*!
+ * \brief tvm intrinsics for global barrier synchronization
+ */
+const Op &BarrierAll();
+
+/*!
+ * \brief tvm intrinsics for block-level barrier synchronization
+ */
+const Op &BarrierAllBlock();
+
+/*!
+ * \brief tvm intrinsics for warp-level barrier synchronization
+ */
+const Op &BarrierAllWarp();
+
+/*!
+ * \brief tvm intrinsics for global synchronization
+ */
+const Op &SyncAll();
+
+/*!
+ * \brief tvm intrinsics for block-level synchronization
+ */
+const Op &SyncAllBlock();
+
+/*!
+ * \brief tvm intrinsics for warp-level synchronization
+ */
+const Op &SyncAllWarp();
+
+/*!
+ * \brief tvm intrinsics for quiet operation
+ */
+const Op &Quiet();
+
+/*!
+ * \brief tvm intrinsics for memory fence operation
+ */
+const Op &Fence();
+
+/*!
+ * \brief tvm intrinsics for non-blocking block-level memory get
+ */
+const Op &GetmemNbiBlock();
+
+/*!
+ * \brief tvm intrinsics for blocking block-level memory get
+ */
+const Op &GetmemBlock();
+
+/*!
+ * \brief tvm intrinsics for non-blocking warp-level memory get
+ */
+const Op &GetmemNbiWarp();
+
+/*!
+ * \brief tvm intrinsics for blocking warp-level memory get
+ */
+const Op &GetmemWarp();
+
+/*!
+ * \brief tvm intrinsics for non-blocking memory get
+ */
+const Op &GetmemNbi();
+
+/*!
+ * \brief tvm intrinsics for blocking memory get
+ */
+const Op &Getmem();
+
+/*!
+ * \brief tvm intrinsics for block-level memory put
+ */
+const Op &PutmemBlock();
+
+/*!
+ * \brief tvm intrinsics for non-blocking block-level memory put
+ */
+const Op &PutmemNbiBlock();
+
+/*!
+ * \brief tvm intrinsics for warp-level memory put
+ */
+const Op &PutmemWarp();
+
+/*!
+ * \brief tvm intrinsics for non-blocking warp-level memory put
+ */
+const Op &PutmemNbiWarp();
+
+/*!
+ * \brief tvm intrinsics for memory put
+ */
+const Op &Putmem();
+
+/*!
+ * \brief tvm intrinsics for non-blocking memory put
+ */
+const Op &PutmemNbi();
+
+/*!
+ * \brief tvm intrinsics for signaled memory put
+ */
+const Op &PutmemSignal();
+
+/*!
+ * \brief tvm intrinsics for non-blocking signaled memory put
+ */
+const Op &PutmemSignalNbi();
+
+/*!
+ * \brief tvm intrinsics for block-level signaled memory put
+ */
+const Op &PutmemSignalBlock();
+
+/*!
+ * \brief tvm intrinsics for non-blocking block-level signaled memory put
+ */
+const Op &PutmemSignalNbiBlock();
+
+/*!
+ * \brief tvm intrinsics for warp-level signaled memory put
+ */
+const Op &PutmemSignalWarp();
+
+/*!
+ * \brief tvm intrinsics for non-blocking warp-level signaled memory put
+ */
+const Op &PutmemSignalNbiWarp();
+
+/*!
+ * \brief tvm intrinsics for signal operation
+ */
+const Op &SignalOp();
+
+/*!
+ * \brief tvm intrinsics for waiting on signal
+ */
+const Op &SignalWaitUntil();
+
+/*!
+ * \brief tvm intrinsics for broadcast operation
+ */
+const Op &Broadcast();
+
+/*!
+ * \brief tvm intrinsics for warp-level broadcast
+ */
+const Op &BroadcastWarp();
+
+/*!
+ * \brief tvm intrinsics for block-level broadcast
+ */
+const Op &BroadcastBlock();
+
+/*!
+ * \brief tvm intrinsics for block-level memory broadcast
+ */
+const Op &BroadcastmemBlock();
+
+/*!
+ * \brief tvm intrinsics for collective gather operation
+ */
+const Op &Fcollect();
+
+/*!
+ * \brief tvm intrinsics for warp-level collective gather
+ */
+const Op &FcollectWarp();
+
+/*!
+ * \brief tvm intrinsics for block-level collective gather
+ */
+const Op &FcollectBlock();
+
+/*!
+ * \brief tvm intrinsics for collective gather operation
+ */
+const Op &CpengineCpAsync();
+
+/*!
+ * \brief tvm intrinsics for getting the rank of the current process
+ */
+const Op &get_rank();
+
+/*!
+ * \brief tvm intrinsics for getting the number of processes
+ */
+const Op &get_num_ranks();
+
+/*!
+ * \brief tvm intrinsics for getting the remote base pointer
+ */
+const Op &get_remote_base_ptr();
+
+/*!
+ * \brief tvm intrinsics for getting the uintptr_t of a pointer
+ */
+const Op &get_uintptr_t();
+} // namespace tl
+} // namespace tvm
diff --git a/src/op/elem.cc b/src/op/elem.cc
deleted file mode 100644
index e31cb5f5..00000000
--- a/src/op/elem.cc
+++ /dev/null
@@ -1,504 +0,0 @@
-/*!
- * \file tl/op/elem.cc
- *
- * Define elment-wise operators.
- */
-
-#include "elem.h"
-
-#include <tvm/tir/builtin.h>
-#include <tvm/tir/op.h>
-#include <tvm/tir/op_attr_types.h>
-
-#include "../target/utils.h"
-#include "../transform/common/loop_fusion_utils.h"
-#include "../transform/common/loop_parallel_transform_utils.h"
-#include "../transform/loop_partition.h"
-#include "../transform/loop_vectorize.h"
-#include "builtin.h"
-
-namespace tvm {
-namespace tl {
-
-using namespace tir;
-
-Copy::Copy(Array<PrimExpr> args, BufferMap vmap) : args_(args) {
-  Array<Range> rgs[2];
-  Buffer bf[2];
-  for (int i = 0; i < 2; i++) {
-    auto expr = args[i];
-    auto call = expr.as<CallNode>();
-    ICHECK(call);
-    auto region = RegionOp(call->args, vmap);
-    rgs[i] = region.GetRanges();
-    bf[i] = region.GetBuffer();
-  }
-  std::tie(this->src, this->dst) = std::tie(bf[0], bf[1]);
-  std::tie(this->src_range, this->dst_range) = std::tie(rgs[0], rgs[1]);
-  if (args.size() >= 3) {
-    auto coalesced_width = Downcast<IntImm>(args[2]);
-    if (coalesced_width->value > 0) {
-      this->coalesced_width = coalesced_width;
-    }
-  }
-  if (args.size() >= 4) {
-    auto disable_tma = Downcast<Bool>(args[3]);
-    this->disable_tma = disable_tma;
-  }
-}
-
-Array<IterVar> Copy::MakeIterVars() const {
-  Array<IterVar> loop_vars;
-  size_t idx = 0;
-  for (size_t i = 0; i < src_range.size(); i++) {
-    if (is_one(src_range[i]->extent))
-      continue;
-    Var var = Var(std::string{char('i' + idx)}, src_range[i]->extent->dtype);
-    idx++;
-    loop_vars.push_back(
-        {Range(0, src_range[i]->extent), var, IterVarType::kDataPar});
-  }
-  return loop_vars;
-}
-
-// ivs: itervars returned by MakeIterVars()
-// src_dst: 0 for src_indices, 1 for dst_indices
-Array<PrimExpr> Copy::MakeIndices(const Array<IterVar> &ivs,
-                                  int src_dst) const {
-  Array<PrimExpr> indices;
-  Array<Range> ranges = src_dst == 0 ? src_range : dst_range;
-  size_t idx = 0;
-  for (size_t i = 0; i < ranges.size(); i++) {
-    if (is_one(ranges[i]->extent))
-      indices.push_back(ranges[i]->min);
-    else {
-      indices.push_back(ranges[i]->min + ivs[idx]->var);
-      idx++;
-    }
-  }
-  ICHECK(idx == ivs.size())
-      << "idx = " << idx << ", ivs.size() = " << ivs.size()
-      << "src name = " << src->name << ", dst name = " << dst->name;
-  return indices;
-}
-
-PrimExpr Copy::MakePredicate(arith::Analyzer *analyzer,
-                             const Array<IterVar> &ivs, Array<PrimExpr> extents,
-                             int src_dst) const {
-  Array<Range> ranges = src_dst == 0 ? src_range : dst_range;
-  Array<PrimExpr> cond_list;
-  ICHECK(extents.size() == ranges.size()) << extents << " " << ranges;
-  size_t idx = 0;
-  for (size_t i = 0; i < ranges.size(); i++) {
-    if (is_one(ranges[i]->extent))
-      continue;
-    PrimExpr cond = ranges[i]->min + ivs[idx]->var < extents[i];
-    if (!analyzer->CanProve(cond, arith::ProofStrength::kSymbolicBound)) {
-      cond_list.push_back(cond);
-    }
-    cond = ranges[i]->min + ivs[idx]->var >= 0;
-    if (!analyzer->CanProve(cond, arith::ProofStrength::kSymbolicBound)) {
-      cond_list.push_back(cond);
-    }
-    idx++;
-  }
-  if (cond_list.empty())
-    return {};
-  else {
-    PrimExpr cond = cond_list[0];
-    for (size_t i = 1; i < cond_list.size(); i++)
-      cond = And(cond, cond_list[i]);
-    return cond;
-  }
-}
-
-For Copy::MakeSIMTLoop(arith::Analyzer *analyzer) const {
-  Array<IterVar> loop_vars = MakeIterVars();
-  bool is_scalar = loop_vars.size() == 0;
-  if (is_scalar) {
-    return For(Var("i"), 0, 1, ForKind::kSerial,
-               BufferStore(dst, BufferLoad(src, {0}), {0}));
-  }
-
-  for (const auto &iv : loop_vars)
-    analyzer->Bind(iv->var, iv->dom);
-
-  ICHECK(loop_vars.size() <= src_range.size())
-      << "loop_vars.size() = " << loop_vars.size()
-      << ", src_range.size() = " << src_range.size() << ", src = " << src->name
-      << ", dst = " << dst->name;
-
-  ICHECK(loop_vars.size() <= dst_range.size())
-      << "loop_vars.size() = " << loop_vars.size()
-      << ", dst_range.size() = " << dst_range.size() << ", src = " << src->name
-      << ", dst = " << dst->name;
-
-  Array<PrimExpr> src_indices = MakeIndices(loop_vars, 0);
-  Array<PrimExpr> dst_indices = MakeIndices(loop_vars, 1);
-
-  PrimExpr src_predicate = MakePredicate(analyzer, loop_vars, src->shape, 0);
-  PrimExpr dst_predicate = MakePredicate(analyzer, loop_vars, dst->shape, 1);
-
-  PrimExpr value = BufferLoad(src, src_indices);
-  if (src->dtype != dst->dtype)
-    value = Cast(dst->dtype, value);
-  if (src_predicate.defined())
-    value = if_then_else(src_predicate, value, make_zero(dst->dtype));
-
-  Stmt body = BufferStore(dst, value, dst_indices);
-  if (dst_predicate.defined())
-    body = IfThenElse(dst_predicate, body);
-  for (int i = loop_vars.size() - 1; i >= 0; i--) {
-    Map<String, ObjectRef> annotations = {};
-    if (coalesced_width.defined()) {
-      annotations.Set("coalesced_width", coalesced_width);
-    }
-    body = For(loop_vars[i]->var, 0, loop_vars[i]->dom->extent,
-               ForKind::kParallel, body, NullOpt, annotations);
-  }
-  return Downcast<For>(body);
-}
-
-Stmt Copy::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
-  Target target = T.target;
-  bool is_cpu_target = target->GetTargetDeviceType() == kDLCPU;
-  Stmt ldsm_stmt = LowerLDSMCopy(T, analyzer);
-  if (ldsm_stmt.defined())
-    return ldsm_stmt;
-
-  if (!disable_tma) {
-    Stmt bulk_copy_stmt = LowerBulkCopy(T, analyzer);
-    if (bulk_copy_stmt.defined())
-      return bulk_copy_stmt;
-  }
-
-  auto simt_loop = MakeSIMTLoop(analyzer);
-  auto fused_loop = Downcast<For>(ParallelLoopFuser::Fuse(simt_loop));
-
-  auto transformed_loop =
-      Downcast<For>(ParallelLoopTransformer::Substitute(fused_loop));
-
-  For vectorized_thread_loop;
-  auto par_op = std::make_unique<ParallelOp>(transformed_loop);
-
-  if (is_cpu_target) {
-    vectorized_thread_loop = VectorizeLoop(transformed_loop);
-  } else {
-    std::vector<InferLevel> levels = {InferLevel::kCommon, InferLevel::kStrict,
-                                      InferLevel::kFree};
-    for (auto level : levels) {
-      par_op->InferLayout(
-          {T.target, T.thread_bounds, T.layout_map, T.buffer_remap}, level);
-    }
-    auto loop_layout = par_op->GetLoopLayout();
-    auto thread_var = T.thread_var;
-    auto thread_loop =
-        PartitionLoop(par_op->GetRoot(), T.thread_var, analyzer, loop_layout);
-    vectorized_thread_loop = VectorizeLoop(thread_loop);
-  }
-
-  if (par_op->GetPredicate(T.thread_var).defined()) {
-    return IfThenElse(par_op->GetPredicate(T.thread_var).value(),
-                      vectorized_thread_loop);
-  }
-  return vectorized_thread_loop;
-}
-
-Stmt Copy::LowerLDSMCopy(const LowerArgs &T, arith::Analyzer *analyzer) const {
-  // Check buffer scope
-  bool is_ldmatrix;
-  if (TargetHasLdmatrix(T.target) && src.scope() == "shared.dyn" &&
-      dst.scope() == "local.fragment") {
-    is_ldmatrix = true;
-  } else if (TargetHasStmatrix(T.target) && dst.scope() == "shared.dyn" &&
-             src.scope() == "local.fragment") {
-    is_ldmatrix = false;
-  } else {
-    return Stmt();
-  }
-
-  // Check no predicates
-  Array<IterVar> loop_vars = MakeIterVars();
-  if (loop_vars.size() < 2)
-    return Stmt();
-  for (const auto &iv : loop_vars)
-    analyzer->Bind(iv->var, iv->dom);
-  PrimExpr src_predicate = MakePredicate(analyzer, loop_vars, src->shape, 0);
-  PrimExpr dst_predicate = MakePredicate(analyzer, loop_vars, dst->shape, 1);
-  if (src_predicate.defined() || dst_predicate.defined())
-    return Stmt();
-
-  Buffer shared_tensor = is_ldmatrix ? src : dst;
-  Buffer local_tensor = is_ldmatrix ? dst : src;
-
-  Array<PrimExpr> local_indices = MakeIndices(loop_vars, is_ldmatrix ? 1 : 0);
-  Fragment local_layout = Downcast<Fragment>(T.layout_map[local_tensor]);
-  Array<PrimExpr> local_indices_transformed =
-      local_layout->Forward(local_indices);
-  local_tensor = T.buffer_remap[local_tensor];
-  // currently only support 1-d case
-  if (local_layout->OutputDim() != 1)
-    return Stmt();
-
-  Array<PrimExpr> shared_indices = MakeIndices(loop_vars, is_ldmatrix ? 0 : 1);
-  Array<PrimExpr> shared_indices_transformed = shared_indices;
-  Layout shared_layout;
-  if (T.buffer_remap.count(shared_tensor)) {
-    shared_layout = T.layout_map[shared_tensor];
-    shared_tensor = T.buffer_remap[shared_tensor];
-    shared_indices_transformed = shared_layout->Forward(shared_indices);
-  }
-
-  // Check local_layout follows 8x8 layout
-  bool is_transposed;
-  IterVar col_var = loop_vars[loop_vars.size() - 1];
-  IterVar row_var = loop_vars[loop_vars.size() - 2];
-  PrimExpr local_layout_thread_map =
-      FloorMod(local_layout->ForwardThread(local_indices, NullOpt), 32);
-  PrimExpr matrix_8x8_thread_map = makeGemmFragment8x8()->ForwardThread(
-      {FloorMod(row_var, 8), FloorMod(col_var, 8)}, NullOpt);
-  PrimExpr matrix_8x8_thread_map_trans =
-      makeGemmFragment8x8Transposed()->ForwardThread(
-          {FloorMod(row_var, 8), FloorMod(col_var, 8)}, NullOpt);
-  PrimExpr local_indices_flattened =
-      local_tensor.OffsetOf(local_indices_transformed).back();
-  if (analyzer->CanProveEqual(matrix_8x8_thread_map, local_layout_thread_map) &&
-      IndiceCanVectorize(local_indices_flattened, col_var->var,
-                         col_var->dom->extent, 2, analyzer)) {
-    is_transposed = false;
-  } else if (analyzer->CanProveEqual(matrix_8x8_thread_map_trans,
-                                     local_layout_thread_map) &&
-             IndiceCanVectorize(local_indices_flattened, row_var->var,
-                                row_var->dom->extent, 2, analyzer)) {
-    is_transposed = true;
-  } else {
-    return Stmt();
-  }
-  // Check shared_layout is 16 bytes continuous
-  if (shared_tensor->dtype.bytes() != 2)
-    return Stmt();
-  PrimExpr flattened_indice =
-      shared_tensor.OffsetOf(shared_indices_transformed).back();
-  if (!IndiceCanVectorize(flattened_indice, loop_vars.back()->var,
-                          loop_vars.back()->dom->extent, 8, analyzer))
-    return Stmt();
-
-  // Can only support local_range to be a full range
-  for (size_t i = 0; i < dst_range.size(); i++) {
-    if (!is_zero(dst_range[i]->min) ||
-        !analyzer->CanProveEqual(dst_range[i]->extent, dst->shape[i]))
-      return Stmt();
-  }
-
-  // Do the lowering here, try vectorized ldmatrix/stmatrix by 4/2/1
-  PrimExpr extent = local_tensor->shape[0];
-  int num = 1;
-  if (analyzer->CanProveEqual(FloorMod(extent, 8), 0))
-    num = 4;
-  else if (analyzer->CanProveEqual(FloorMod(extent, 4), 0))
-    num = 2;
-
-  Array<PrimExpr> args;
-  const Op &op = is_ldmatrix ? tl::ptx_ldmatirx() : tl::ptx_stmatirx();
-  args.push_back(static_cast<int>(is_transposed));
-  args.push_back(num);
-
-  // Create shared address with regard to local address
-  // if not transpose
-  // coords = Inverse(base + 2 * (thread / 8) % num, warp + (thread % 8) * 4))
-  // if transpose
-  // coords = Inverse(base + 2 * (thread / 8) % num + thread % 2, warp + thread
-  // % 8 / 2)
-  Var local_iter("i");
-  Layout inv = local_layout->Inverse();
-  Array<PrimExpr> shared_coords;
-  PrimExpr warp = FloorDiv(T.thread_var, 32) * 32;
-  if (!is_transposed)
-    shared_coords = inv->Forward(
-        {local_iter * 2 * num + 2 * FloorMod(FloorDiv(T.thread_var, 8), num),
-         warp + FloorMod(T.thread_var, 8) * 4});
-  else
-    shared_coords = inv->Forward(
-        {local_iter * 2 * num + 2 * FloorMod(FloorDiv(T.thread_var, 8), num) +
-             FloorMod(T.thread_var, 2),
-         warp + FloorDiv(FloorMod(T.thread_var, 8), 2)});
-  shared_coords.pop_back(); // remove rep
-  if (shared_layout.defined())
-    shared_coords = shared_layout->Forward(shared_coords);
-  PrimExpr shared_addr = shared_tensor.access_ptr(
-      is_ldmatrix ? 1 : 2, DataType::Handle(), 1,
-      shared_tensor.OffsetOf(shared_coords).back(), PrimExpr(2 * num));
-  args.push_back(shared_addr);
-
-  if (is_ldmatrix) {
-    // Can only support same dtype for ldmatrx
-    if (local_tensor->dtype != shared_tensor->dtype)
-      return Stmt();
-    PrimExpr local_addr = local_tensor.access_ptr(
-        2, DataType::Handle(), 1, local_iter * 2 * num, PrimExpr(2 * num));
-    args.push_back(local_addr);
-  } else {
-    for (int i = 0; i < num; i++) {
-      PrimExpr value0 =
-          BufferLoad(local_tensor, {local_iter * 2 * num + 2 * i});
-      PrimExpr value1 =
-          BufferLoad(local_tensor, {local_iter * 2 * num + 2 * i + 1});
-      if (local_tensor->dtype != shared_tensor->dtype) {
-        value0 = Cast(shared_tensor->dtype, value0);
-        value1 = Cast(shared_tensor->dtype, value1);
-      }
-      PrimExpr value_packed =
-          Call(DataType::Int(32), pack_b16(), {value0, value1});
-      args.push_back(value_packed);
-    }
-  }
-
-  auto body = Evaluate(Call(DataType::Handle(), op, args));
-  For for_node =
-      For(local_iter, 0, FloorDiv(extent, 2 * num), ForKind::kSerial, body);
-  for_node = LoopPragmaUnroll(for_node);
-  auto range = T.thread_bounds;
-  if (range.defined()) {
-    auto thread_var = T.thread_var;
-    auto thread_var_with_offset = thread_var - range->min;
-    for_node.CopyOnWrite()->body =
-        Substitute(for_node->body, {{thread_var, thread_var_with_offset}});
-  }
-  return for_node;
-}
-
-LayoutMap Copy::InferLayout(const LayoutInferArgs &T, InferLevel level) {
-  // Use parallel op to infer the layout
-  if (par_op_ == nullptr) {
-    arith::Analyzer analyzer;
-    par_op_ = std::make_unique<ParallelOp>(MakeSIMTLoop(&analyzer));
-  }
-  if (T.layout_map.count(src) && T.layout_map.count(dst)) {
-    // Only compare fragment layout
-    if (src.scope() == "local.fragment" && dst.scope() == "local.fragment") {
-      const FragmentNode *src_layout = T.layout_map[src].as<Fragment>().get();
-      const FragmentNode *dst_layout = T.layout_map[dst].as<Fragment>().get();
-      if (src_layout && dst_layout) {
-        ICHECK(src_layout->IsEqual(dst_layout, true))
-            << "Get different layout for " << src << " and " << dst
-            << "\nLHS = " << src_layout->DebugOutput()
-            << "\nRHS = " << dst_layout->DebugOutput()
-            << "\nYou may need to use a shared memory to transform the layout";
-      }
-    }
-  }
-  return par_op_->InferLayout(T, level);
-}
-
-Fill::Fill(Array<PrimExpr> args, BufferMap vmap) {
-
-  if (args[0]->IsInstance<BufferLoadNode>()) {
-    auto buffer_load = Downcast<BufferLoad>(args[0]);
-    for (const auto &index : buffer_load->indices) {
-      if (const auto *ramp = index.as<RampNode>()) {
-        CHECK(ramp->stride.as<IntImmNode>()->value == 1)
-            << "Only stride 1 ramps are supported";
-        const auto *lanes = ramp->lanes.as<IntImmNode>();
-        CHECK(lanes)
-            << "Scalable vectors not supported in BufferRegion conversion";
-        region.push_back(Range::FromMinExtent(ramp->base, ramp->lanes));
-      } else {
-        region.push_back(Range::FromMinExtent(index, 1));
-      }
-    }
-    dst = buffer_load->buffer;
-  } else {
-    dst = vmap[GetVarFromAccessPtr(args[0])];
-    for (int i = 0; i < dst->shape.size(); i++) {
-      region.push_back(Range(0, dst->shape[i]));
-    }
-  }
-
-  if (args[1]->dtype != dst->dtype) {
-    value = Cast(dst->dtype, args[1]);
-  } else {
-    value = args[1];
-  }
-
-  ICHECK(region.size() == dst->shape.size())
-      << "region size = " << region.size() << " != " << dst->shape.size();
-  for (int i = 0; i < region.size(); i++) {
-    // bound check if region is static
-    if (region[i]->min.as<IntImm>()) {
-      int64_t min = Downcast<IntImm>(region[i]->min)->value;
-      ICHECK_GE(min, 0) << "region[" << i << "] = " << min << " < 0";
-    }
-    if (region[i]->extent.as<IntImm>()) {
-      int64_t extent = Downcast<IntImm>(region[i]->extent)->value;
-      ICHECK_LE(extent, Downcast<IntImm>(dst->shape[i])->value)
-          << "region[" << i << "] = " << extent << " > " << dst->shape[i];
-    }
-  }
-}
-
-For Fill::MakeSIMTLoop(arith::Analyzer *analyzer) const {
-  int ndim = dst->shape.size();
-  Array<IterVar> loop_vars;
-  Array<PrimExpr> dst_indices;
-  for (int i = 0; i < ndim; i++) {
-    Var var = Var(std::string{char('i' + i)}, region[i]->extent->dtype);
-    loop_vars.push_back({region[i], var, IterVarType::kDataPar});
-    dst_indices.push_back(var);
-  }
-  Stmt body = BufferStore(dst, value, dst_indices);
-  for (int i = ndim - 1; i >= 0; i--) {
-    body = For(loop_vars[i]->var, 0, loop_vars[i]->dom->extent,
-               ForKind::kParallel, body);
-  }
-  return Downcast<For>(body);
-}
-
-Stmt Fill::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
-
-  if (dst.scope() == "local.fragment") {
-    auto par_op = std::make_unique<ParallelOp>(MakeSIMTLoop(analyzer));
-    par_op->InferLayout({T.target, T.thread_bounds, T.layout_map},
-                        InferLevel::kFree);
-    par_op->InferLayout({T.target, T.thread_bounds, T.layout_map},
-                        InferLevel::kFree);
-    auto thread_loop = PartitionLoop(par_op->GetRoot(), T.thread_var, analyzer,
-                                     par_op->GetLoopLayout());
-    auto vectorized_thread_loop = VectorizeLoop(thread_loop);
-    if (par_op->GetPredicate(T.thread_var).defined()) {
-      return IfThenElse(par_op->GetPredicate(T.thread_var).value(),
-                        vectorized_thread_loop);
-    }
-    return vectorized_thread_loop;
-  } else if (dst.scope() == "local") {
-    auto init_loop = MakeSIMTLoop(analyzer);
-    auto vectorized_thread_loop = VectorizeLoop(init_loop);
-    return vectorized_thread_loop;
-  } else if (dst.scope() == "shared.dyn" || dst.scope() == "shared") {
-    auto par_op = std::make_unique<ParallelOp>(MakeSIMTLoop(analyzer));
-    par_op->InferLayout({T.target, T.thread_bounds, T.layout_map},
-                        InferLevel::kFree);
-    auto thread_loop = PartitionLoop(par_op->GetRoot(), T.thread_var, analyzer,
-                                     par_op->GetLoopLayout());
-    auto vectorized_thread_loop = VectorizeLoop(thread_loop);
-    if (par_op->GetPredicate(T.thread_var).defined()) {
-      return IfThenElse(par_op->GetPredicate(T.thread_var).value(),
-                        vectorized_thread_loop);
-    }
-    return vectorized_thread_loop;
-  } else {
-    LOG(FATAL) << "Unsupported scope " << dst.scope();
-  }
-}
-
-TIR_REGISTER_TL_OP(Copy, copy)
-    .set_num_inputs(3)
-    .set_attr<TCallEffectKind>("TCallEffectKind",
-                               Integer(CallEffectKind::kOpaque));
-
-TIR_REGISTER_TL_OP(Fill, fill)
-    .set_num_inputs(2)
-    .set_attr<TCallEffectKind>("TCallEffectKind",
-                               Integer(CallEffectKind::kOpaque));
-
-} // namespace tl
-} // namespace tvm
\ No newline at end of file
diff --git a/src/op/elem.h b/src/op/elem.h
deleted file mode 100644
index a3d42291..00000000
--- a/src/op/elem.h
+++ /dev/null
@@ -1,66 +0,0 @@
-/*!
- * \file tl/op/elem.h
- * \brief Define elment-wise operators.
- *
- */
-
-#ifndef TVM_TL_OP_ELEM_H_
-#define TVM_TL_OP_ELEM_H_
-
-#include "op.h"
-#include "parallel.h"
-
-namespace tvm {
-namespace tl {
-
-using namespace tir;
-
-class Copy : public Operator {
-public:
-  Copy(Array<PrimExpr> args, BufferMap vmap);
-  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const final;
-  LayoutMap InferLayout(const LayoutInferArgs &T, InferLevel level) final;
-
-  static const Op &Get();
-
-protected:
-  Stmt LowerBulkCopy(const LowerArgs &T, arith::Analyzer *analyzer) const;
-  Stmt LowerLDSMCopy(const LowerArgs &T, arith::Analyzer *analyzer) const;
-
-  For MakeSIMTLoop(arith::Analyzer *analyzer) const;
-  Array<IterVar> MakeIterVars() const;
-
-  // ivs: itervars returned by MakeIterVars()
-  // src_dst: 0 for src_indices, 1 for dst_indices
-  Array<PrimExpr> MakeIndices(const Array<IterVar> &ivs, int src_dst) const;
-
-  PrimExpr MakePredicate(arith::Analyzer *analyzer, const Array<IterVar> &ivs,
-                         Array<PrimExpr> extents, int src_dst) const;
-
-  Array<PrimExpr> args_;
-
-  Buffer src, dst;
-  Array<Range> src_range, dst_range;
-  IntImm coalesced_width;
-  Bool disable_tma = Bool(false);
-
-  std::unique_ptr<ParallelOp> par_op_;
-};
-
-class Fill : public Operator {
-public:
-  Fill(Array<PrimExpr> args, BufferMap vmap);
-  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const final;
-  static const Op &Get();
-
-private:
-  For MakeSIMTLoop(arith::Analyzer *analyzer) const;
-  tir::Buffer dst;
-  PrimExpr value;
-  Array<Range> region;
-};
-
-} // namespace tl
-} // namespace tvm
-
-#endif //  TVM_TL_OP_ELEM_H_
\ No newline at end of file
diff --git a/src/op/fill.cc b/src/op/fill.cc
new file mode 100644
index 00000000..8f0dec63
--- /dev/null
+++ b/src/op/fill.cc
@@ -0,0 +1,232 @@
+/*!
+ * \file tl/op/fill.cc
+ *
+ * Define elment-wise operators.
+ */
+
+#include "fill.h"
+
+#include <tvm/tir/builtin.h>
+#include <tvm/tir/op.h>
+#include <tvm/tir/op_attr_types.h>
+
+#include "../layout/tcgen05_layout.h"
+#include "../target/utils.h"
+#include "../transform/common/loop_fusion_utils.h"
+#include "../transform/common/loop_parallel_transform_utils.h"
+#include "../transform/loop_partition.h"
+#include "../transform/loop_vectorize.h"
+#include "builtin.h"
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+/**
+ * @brief Construct a Fill operator node from call arguments and a buffer map.
+ *
+ * This constructor builds a FillNode describing an element-wise fill of a
+ * destination buffer region with a scalar/vector value and stores it in
+ * `data_`.
+ *
+ * Detailed behavior:
+ * - If `args[0]` is a `BufferLoad`, the loaded buffer becomes the destination
+ * and the load indices are converted to per-dimension ranges:
+ *   - `Ramp(base, lanes, stride)` is converted to `Range(base, lanes)`. Only
+ * stride == 1 and constant `lanes` are supported.
+ *   - Non-ramp indices become `Range(index, 1)`.
+ * - Otherwise `args[0]` is treated as an access pointer; the destination buffer
+ * is resolved via `vmap[GetVarFromAccessPtr(args[0])]` and the region is the
+ * full buffer shape for each dimension.
+ * - `args[1]` is used as the fill value; it is cast to the destination buffer's
+ * dtype if necessary.
+ * - Performs validation:
+ *   - Region dimensionality must match destination rank.
+ *   - For statically-known region mins and extents, checks that mins >= 0 and
+ * extents do not exceed the corresponding destination shape extents.
+ *
+ * Parameters:
+ * @param args Call arguments: expected layout is [dst_access_or_bufferload,
+ * value].
+ *             - args[0]: destination access (BufferLoad or pointer expression).
+ *             - args[1]: value to fill (scalar or vector).
+ * @param vmap Mapping from buffer variables to Buffer objects; used to resolve
+ * the destination when args[0] is not a BufferLoad.
+ *
+ * Notes:
+ * - The constructor enforces constraints (e.g., stride == 1 ramps, constant
+ * lanes) and will terminate (via CHECK/ICHECK) if inputs are unsupported or out
+ * of bounds.
+ */
+Fill::Fill(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<FillNode> node = make_object<FillNode>();
+
+  if (args[0]->IsInstance<BufferLoadNode>()) {
+    auto buffer_load = Downcast<BufferLoad>(args[0]);
+    for (const auto &index : buffer_load->indices) {
+      if (const auto *ramp = index.as<RampNode>()) {
+        CHECK(ramp->stride.as<IntImmNode>()->value == 1)
+            << "Only stride 1 ramps are supported";
+        const auto *lanes = ramp->lanes.as<IntImmNode>();
+        CHECK(lanes)
+            << "Scalable vectors not supported in BufferRegion conversion";
+        node->region.push_back(Range::FromMinExtent(ramp->base, ramp->lanes));
+      } else {
+        node->region.push_back(Range::FromMinExtent(index, 1));
+      }
+    }
+    node->dst = buffer_load->buffer;
+  } else {
+    node->dst = vmap[GetVarFromAccessPtr(args[0])];
+    for (int i = 0; i < node->dst->shape.size(); i++) {
+      node->region.push_back(Range(0, node->dst->shape[i]));
+    }
+  }
+
+  if (args[1]->dtype != node->dst->dtype) {
+    node->value = Cast(node->dst->dtype, args[1]);
+  } else {
+    node->value = args[1];
+  }
+
+  ICHECK(node->region.size() == node->dst->shape.size())
+      << "region size = " << node->region.size()
+      << " != " << node->dst->shape.size();
+  for (int i = 0; i < node->region.size(); i++) {
+    // bound check if region is static
+    if (node->region[i]->min.as<IntImm>()) {
+      int64_t min = Downcast<IntImm>(node->region[i]->min)->value;
+      ICHECK_GE(min, 0) << "region[" << i << "] = " << min << " < 0";
+    }
+    if (node->region[i]->extent.as<IntImm>()) {
+      int64_t extent = Downcast<IntImm>(node->region[i]->extent)->value;
+      ICHECK_LE(extent, Downcast<IntImm>(node->dst->shape[i])->value)
+          << "region[" << i << "] = " << extent << " > " << node->dst->shape[i];
+    }
+  }
+  data_ = std::move(node);
+}
+
+/**
+ * @brief Create a copy of this FillNode and return it as a TileOperator.
+ *
+ * Constructs a new FillNode by copying the current node and wraps the copy in a
+ * Fill TileOperator.
+ *
+ * @return TileOperator A TileOperator that owns the copied FillNode.
+ */
+TileOperator FillNode::Clone() const {
+  auto op = make_object<FillNode>(*this);
+  return Fill(op);
+}
+
+/**
+ * @brief Build a SIMT-style nested parallel loop that fills the destination
+ * buffer.
+ *
+ * Constructs per-dimension data-parallel loop iterators matching this node's
+ * region extents, emits a BufferStore that writes the node's `value` into `dst`
+ * at the loop indices, and nests the loops (innermost to outermost) as parallel
+ * `For` nodes. Returns the outermost `For` loop representing the complete
+ * multi-dimensional fill kernel.
+ *
+ * @return For Outermost parallel `For` loop of the generated nested SIMT loop.
+ */
+For FillNode::MakeSIMTLoop(arith::Analyzer *analyzer) const {
+  int ndim = dst->shape.size();
+  Array<IterVar> loop_vars;
+  Array<PrimExpr> dst_indices;
+  for (int i = 0; i < ndim; i++) {
+    Var var = Var(std::string{char('i' + i)}, region[i]->extent->dtype);
+    loop_vars.push_back({region[i], var, IterVarType::kDataPar});
+    dst_indices.push_back(var);
+  }
+  Stmt body = BufferStore(dst, value, dst_indices);
+  for (int i = ndim - 1; i >= 0; i--) {
+    body = For(loop_vars[i]->var, 0, loop_vars[i]->dom->extent,
+               ForKind::kParallel, body);
+  }
+  return Downcast<For>(body);
+}
+
+/**
+ * @brief Lower this Fill operator to a TIR statement for the target.
+ *
+ * Lowers the FillNode into a Stmt according to the destination buffer scope:
+ * - "local.fragment" and shared ("shared", "shared.dyn"): create a parallel
+ *   operation from a SIMT loop, infer its layout, partition the root loop by
+ *   the thread variable, vectorize the resulting thread loop, and, if a
+ *   per-thread predicate exists, guard the vectorized loop with that
+ *   predicate.
+ * - "local": build a SIMT loop and return its vectorized form.
+ * - other scopes: fatal error.
+ *
+ * The lowering may query layout and thread information from @p T and uses the
+ * provided analyzer for any required arithmetic/layout analysis.
+ *
+ * @param T Lowering arguments (target, thread bounds, thread var, layout map).
+ * @return Stmt The lowered TIR statement implementing the fill.
+ */
+Stmt FillNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  if (dst.scope() == "local.fragment") {
+    auto par_op = ParallelOp(MakeSIMTLoop(analyzer));
+    par_op->InferLayout({T.target, T.thread_bounds, T.layout_map, analyzer,
+                         false, T.buffer_remap},
+                        InferLevel::kFree);
+    auto thread_loop = PartitionLoop(par_op->GetRoot(), T.thread_var, analyzer,
+                                     par_op->GetLoopLayout());
+    auto vectorized_thread_loop = VectorizeLoop(thread_loop);
+    if (par_op->GetPredicate(T.thread_var).defined()) {
+      return IfThenElse(par_op->GetPredicate(T.thread_var).value(),
+                        vectorized_thread_loop);
+    }
+    return vectorized_thread_loop;
+  } else if (dst.scope() == "local") {
+    auto init_loop = MakeSIMTLoop(analyzer);
+    auto vectorized_thread_loop = VectorizeLoop(init_loop);
+    return vectorized_thread_loop;
+  } else if (dst.scope() == "shared.dyn" || dst.scope() == "shared" ||
+             dst.scope() == "global") {
+    auto par_op = ParallelOp(MakeSIMTLoop(analyzer));
+    par_op->InferLayout({T.target, T.thread_bounds, T.layout_map, analyzer,
+                         false, T.buffer_remap},
+                        InferLevel::kFree);
+    auto thread_loop = PartitionLoop(par_op->GetRoot(), T.thread_var, analyzer,
+                                     par_op->GetLoopLayout());
+    auto vectorized_thread_loop = VectorizeLoop(thread_loop);
+    if (par_op->GetPredicate(T.thread_var).defined()) {
+      return IfThenElse(par_op->GetPredicate(T.thread_var).value(),
+                        vectorized_thread_loop);
+    }
+    return vectorized_thread_loop;
+  } else {
+    LOG(FATAL) << "Unsupported scope " << dst.scope();
+  }
+}
+
+/**
+ * @brief Infer memory/layout mapping for the Fill operator.
+ *
+ * Returns the layout mapping produced by layout inference for this FillNode.
+ * Currently no layout inference is performed for Fill and the function returns
+ * an empty LayoutMap.
+ *
+ * @param T Context required for layout inference (unused).
+ * @param level The inference level requested (unused).
+ * @return LayoutMap Empty map indicating no inferred layouts for this operator.
+ */
+LayoutMap FillNode::InferLayout(const LayoutInferArgs &T,
+                                InferLevel level) const {
+  return {};
+}
+
+TIR_REGISTER_TL_OP(Fill, fill)
+    .set_num_inputs(2)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TVM_FFI_STATIC_INIT_BLOCK({ FillNode::RegisterReflection(); });
+
+} // namespace tl
+} // namespace tvm
\ No newline at end of file
diff --git a/src/op/fill.h b/src/op/fill.h
new file mode 100644
index 00000000..6d384076
--- /dev/null
+++ b/src/op/fill.h
@@ -0,0 +1,69 @@
+/*!
+ * \file tl/op/fill.h
+ * \brief Fill operations for tensor initialization
+ */
+
+#ifndef TVM_TL_OP_FILL_H_
+#define TVM_TL_OP_FILL_H_
+
+#include "operator.h"
+#include "parallel.h"
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+/// Node class for fill operations
+class FillNode : public TileOperatorNode {
+public:
+  tir::Buffer dst;     ///< Destination buffer to fill
+  PrimExpr value;      ///< Value to fill with
+  Array<Range> region; ///< Region to fill within the buffer
+  static constexpr const char *_type_key = "tl.Fill";
+  TVM_DECLARE_FINAL_OBJECT_INFO(FillNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const;
+  LayoutMap InferLayout(const LayoutInferArgs &T, InferLevel level) const;
+  static const Op &Get();
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<FillNode>()
+        .def_ro("dst", &FillNode::dst)
+        .def_ro("value", &FillNode::value)
+        .def_ro("region", &FillNode::region);
+  }
+
+  bool SEqualReduce(const FillNode *other, SEqualReducer equal) const {
+    return equal(dst, other->dst) && equal(value, other->value) &&
+           equal(region, other->region);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(dst);
+    hash_reduce(value);
+    hash_reduce(region);
+  }
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  TileOperator Clone() const;
+
+private:
+  /// Create SIMT-style parallel loop for filling
+  For MakeSIMTLoop(arith::Analyzer *analyzer) const;
+};
+
+/// Wrapper class for fill operations
+class Fill : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(Fill, TileOperator, FillNode);
+  TVM_DLL Fill(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+} // namespace tl
+} // namespace tvm
+
+#endif // TVM_TL_OP_FILL_H_
\ No newline at end of file
diff --git a/src/op/finalize_reducer.cc b/src/op/finalize_reducer.cc
new file mode 100644
index 00000000..def940b4
--- /dev/null
+++ b/src/op/finalize_reducer.cc
@@ -0,0 +1,166 @@
+/*!
+ * \file src/op/finalize_reducer.cc
+ *
+ * Define finalize_reducer operator.
+ */
+
+#include "finalize_reducer.h"
+
+#include <tvm/arith/iter_affine_map.h>
+#include <tvm/tir/builtin.h>
+#include <tvm/tir/op.h>
+#include <tvm/tir/op_attr_types.h>
+
+#include "../target/utils.h"
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+/**
+ * @brief Construct a FinalizeReducerOp from TL operator arguments and a buffer
+ * map.
+ *
+ * Extracts the reducer Buffer from `vmap` using the variable referenced by
+ * `args[0]` and sets the reduction operation type from the integer code in
+ * `args[1]`.
+ *
+ * @param args TL operator arguments: expects at least two elements where
+ *             `args[0]` is an access pointer identifying the reducer variable
+ * and `args[1]` is an integer encoding a `ReducerOpType` (e.g., Sum/Max/Min).
+ * @param vmap Mapping from variables to Buffers used to look up the reducer
+ * Buffer.
+ */
+FinalizeReducerOp::FinalizeReducerOp(Array<PrimExpr> args, BufferMap vmap) {
+  auto node = make_object<FinalizeReducerOpNode>();
+  node->reducer = vmap[GetVarFromAccessPtr(args[0])];
+  node->op = (ReducerOpType)*as_const_int(args[1]);
+  data_ = std::move(node);
+}
+
+/**
+ * @brief Lower the finalize_reducer TL operator to a TIR statement.
+ *
+ * Lowers the operator that finalizes a reducer by performing a thread-wide
+ * AllReduce across the reducer's output elements and writing the reduced value
+ * back into the reducer buffer. The function:
+ * - Fetches the reducer buffer and expects its layout to be a Fragment.
+ * - Builds index Vars for each output dimension.
+ * - Reads the layout's ReplicateExtent and:
+ *   - if extent == 1, emits a no-op Evaluate(0);
+ *   - otherwise constructs an AllReduce extern call (uses `run_hopper` when the
+ *     compilation target is Hopper) with an optional workspace (allocated via
+ *     T.AddWorkspace when reducing_threads >= 32) and stores the result via
+ *     BufferStore.
+ * - Wraps the store in parallel outer For loops over each output dimension.
+ *
+ * @param T Lowering context containing buffer remapping, layout map, thread
+ * bounds, target, and helper methods (e.g., AddWorkspace).
+ * @param analyzer Arithmetic analyzer (unused by this implementation but
+ * provided for consistency with lowering API).
+ * @return Stmt The lowered TIR statement representing the AllReduce and
+ * surrounding loops.
+ *
+ * @note The function ICHECKs that the reducer layout is present and a Fragment,
+ *       and that ReplicateExtent is either 1 or equal to the thread block
+ * extent; violations cause a fatal check failure.
+ */
+Stmt FinalizeReducerOpNode::Lower(const LowerArgs &T,
+                                  arith::Analyzer *analyzer) const {
+  auto buffer = T.buffer_remap[reducer];
+  auto opt_layout = T.layout_map.Get(reducer);
+  ICHECK(opt_layout);
+  ICHECK(opt_layout->as<Fragment>());
+  auto layout = opt_layout->as<Fragment>().value();
+  Array<PrimExpr> indices_0;
+  indices_0.reserve(layout->OutputDim());
+  for (int i = 0; i < layout->OutputDim(); ++i)
+    indices_0.push_back(Var("__finred_" + std::to_string(i)));
+
+  const int64_t *p_extent = as_const_int(layout->ReplicateExtent());
+  ICHECK(p_extent);
+  int extent = *p_extent, scale = 1;
+  ICHECK(extent == 1 || extent == *as_const_int(T.thread_bounds->extent))
+      << "Illegal finalize_reducer: extent=" << extent
+      << "; T.thread_bounds=" << T.thread_bounds;
+
+  if (extent == 1)
+    return Evaluate(0);
+
+  std::array op_names{"tl::SumOp", "tl::MaxOp", "tl::MinOp"};
+  auto op_str = op_names[(int)op];
+
+  // adopted from ReduceOp
+  int reducing_threads = extent;
+  std::stringstream ss;
+  auto thread_offset = T.thread_bounds->min;
+  if (TargetIsHopper(T.target) || TargetIsSm100(T.target)) {
+    auto all_threads = T.thread_bounds->extent;
+    ss << "tl::AllReduce<" << op_str << ", " << reducing_threads << ", " << 1
+       << ", " << thread_offset << ", " << all_threads << ">::run_hopper";
+  } else {
+    ss << "tl::AllReduce<" << op_str << ", " << reducing_threads << ", " << 1
+       << ", " << thread_offset << ">::run";
+  }
+  Array<PrimExpr> thread_reduce_args = {StringImm(ss.str()),
+                                        BufferLoad(buffer, indices_0)};
+  if (reducing_threads >= 32) {
+    PrimExpr workspace =
+        T.AddWorkspace(*as_const_int(T.thread_bounds->extent), buffer->dtype);
+    thread_reduce_args.push_back(workspace);
+  }
+  auto call = Call(buffer->dtype, builtin::call_extern(), thread_reduce_args);
+  Stmt body = BufferStore(buffer, call, indices_0);
+
+  // make the outer spatial loop
+  for (int i = layout->OutputDim() - 1; i >= 0; i--) {
+    body = For(indices_0[i].as<Var>().value(), 0, layout->OutputShape()[i],
+               ForKind::kParallel, body);
+  }
+
+  return body;
+}
+
+/**
+ * @brief Infer and return the layout mapping for the reducer buffer.
+ *
+ * Copies the existing layout for the reducer from the provided LayoutInferArgs
+ * into a new LayoutMap and returns it. The inference does not modify the
+ * layout; it preserves the reducer's current layout.
+ *
+ * @param T Provides the input layout map from which the reducer's layout is
+ * copied.
+ * @param level Unused by this operator; present for API compatibility.
+ * @return LayoutMap A map that contains the reducer buffer mapped to its
+ * original layout.
+ */
+LayoutMap FinalizeReducerOpNode::InferLayout(const LayoutInferArgs &T,
+                                             InferLevel level) const {
+  LayoutMap layout_map;
+  layout_map.Set(reducer, T.layout_map.Get(reducer).value());
+  return layout_map;
+}
+
+/**
+ * @brief Create a deep copy of this FinalizeReducerOpNode and wrap it as a
+ * TileOperator.
+ *
+ * Constructs a new FinalizeReducerOpNode by copying the current node state and
+ * returns a TileOperator that owns the copied node.
+ *
+ * @return TileOperator A TileOperator that contains a deep copy of this node.
+ */
+TileOperator FinalizeReducerOpNode::Clone() const {
+  auto node = make_object<FinalizeReducerOpNode>(*this);
+  return TileOperator(node);
+}
+
+TIR_REGISTER_TL_OP(FinalizeReducerOp, finalize_reducer)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TVM_FFI_STATIC_INIT_BLOCK({ FinalizeReducerOpNode::RegisterReflection(); });
+} // namespace tl
+} // namespace tvm
diff --git a/src/op/finalize_reducer.h b/src/op/finalize_reducer.h
new file mode 100644
index 00000000..d9a66d1b
--- /dev/null
+++ b/src/op/finalize_reducer.h
@@ -0,0 +1,70 @@
+// Copyright (c) Tile-AI Corporation.
+// Licensed under the MIT License.
+
+/*!
+ * \file src/op/finalize_reducer.h
+ * \brief Define finalize_reducer operator.
+ */
+
+#ifndef TVM_TL_OP_FINALIZE_REDUCER_H_
+#define TVM_TL_OP_FINALIZE_REDUCER_H_
+
+#include "../transform/layout_reducer.h"
+#include "./operator.h"
+
+/**
+ * Get the Op singleton for the public FinalizeReducerOp handle.
+ *
+ * @return A reference to the Op describing FinalizeReducer.
+ */
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+class FinalizeReducerOpNode : public TileOperatorNode {
+public:
+  tir::Buffer reducer;
+  ReducerOpType op;
+
+  static constexpr const char *_type_key = "tl.FinalizeReducerOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(FinalizeReducerOpNode, TileOperatorNode);
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<FinalizeReducerOpNode>()
+        .def_ro("reducer", &FinalizeReducerOpNode::reducer)
+        .def_ro("op", &FinalizeReducerOpNode::op);
+  }
+
+  bool SEqualReduce(const FinalizeReducerOpNode *other,
+                    SEqualReducer equal) const {
+    return equal(reducer, other->reducer) && equal(op, other->op);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(reducer);
+    hash_reduce(op);
+  }
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+  static const Op &Get();
+  TileOperator Clone() const;
+};
+
+class FinalizeReducerOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(FinalizeReducerOp, TileOperator,
+                                FinalizeReducerOpNode);
+  TVM_DLL FinalizeReducerOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+} // namespace tl
+} // namespace tvm
+
+#endif //  TVM_TL_OP_FINALIZE_REDUCER_H_
\ No newline at end of file
diff --git a/src/op/gemm.cc b/src/op/gemm.cc
index 639f3a18..8912a7a3 100644
--- a/src/op/gemm.cc
+++ b/src/op/gemm.cc
@@ -1,7 +1,6 @@
 /*!
  * \file tl/op/gemm.cc
- *
- * Define gemm operator.
+ * \brief Implementation of General Matrix Multiplication (GEMM) operators
  */
 
 #include "gemm.h"
@@ -19,57 +18,203 @@ namespace tl {
 
 using namespace tir;
 
-static std::vector<int> toPrimeFactors(int x) {
-  int i = 2;
-  std::vector<int> result;
-  while (x > 1) {
-    if (x % i == 0) {
-      x /= i;
-      result.push_back(i);
+struct TCGEN5MMAMeta {
+  int atom_m, atom_n, atom_k;
+};
+
+// Return {is_success, meta}
+static inline std::pair<bool, TCGEN5MMAMeta>
+GetTCGEN5MMAMeta(int M, int N, int K, DataType ab_dtype, DataType c_dtype) {
+// TODO (lei) Currently not all shapes / dtypes are supported for TCGEN5MMA.
+#define FAIL                                                                   \
+  return { false, TCGEN5MMAMeta{0, 0, 0} }
+#define SUCCESS(atom_m, atom_n, atom_k)                                        \
+  return {                                                                     \
+    true, TCGEN5MMAMeta { atom_m, atom_n, atom_k }                             \
+  }
+  std::vector<int> ws_valid_atom_ns = {256, 128, 64};
+  if ((ab_dtype.is_bfloat16() || ab_dtype.is_float16()) &&
+      (c_dtype.is_float() && c_dtype.bits() == 32)) {
+    if (K % 16 != 0)
+      FAIL;
+    if (M % 128 == 0) {
+      for (int atom_n = 256; atom_n >= 16; atom_n -= 16)
+        if (N % atom_n == 0)
+          SUCCESS(128, atom_n, 16);
+      FAIL;
+    } else if (M % 64 == 0) {
+      for (int atom_n : ws_valid_atom_ns)
+        if (N % atom_n == 0)
+          SUCCESS(64, atom_n, 16);
+      FAIL;
+    } else if (M % 32 == 0) {
+      for (int atom_n : ws_valid_atom_ns)
+        if (N % atom_n == 0)
+          SUCCESS(32, atom_n, 16);
+      FAIL;
+    } else {
+      FAIL;
+    }
+  } else if ((ab_dtype.is_float8_e4m3fn() || ab_dtype.is_float8_e5m2()) &&
+             (c_dtype.is_float() && c_dtype.bits() == 32)) {
+    if (K % 32 != 0)
+      FAIL;
+    if (M % 128 == 0) {
+      for (int atom_n = 256; atom_n >= 16; atom_n -= 16)
+        if (N % atom_n == 0)
+          SUCCESS(128, atom_n, 32);
+      FAIL;
+    } else if (M % 64 == 0) {
+      for (int atom_n : ws_valid_atom_ns)
+        if (N % atom_n == 0)
+          SUCCESS(64, atom_n, 32);
+      FAIL;
+    } else if (M % 32 == 0) {
+      for (int atom_n : ws_valid_atom_ns)
+        if (N % atom_n == 0)
+          SUCCESS(32, atom_n, 32);
+      FAIL;
     } else {
-      i++;
+      FAIL;
     }
   }
-  return result;
+  FAIL;
+#undef FAIL
+#undef SUCCESS
 }
 
+/**
+ * @brief Construct a Gemm operator from serialized TL arguments and a buffer
+ * map.
+ *
+ * This constructor deserializes operator parameters from `args` and resolves
+ * buffer references via `vmap`, populating an internal GemmNode with:
+ * - device pointers for A, B, C and their corresponding Buffer objects,
+ * - transpose flags for A and B,
+ * - matrix dimensions M, N, K,
+ * - warp allocation policy and clear_accum flag,
+ * - strides and memory offsets for A and B,
+ * - optional kPack (must be 1 or 2) and optional wg_wait.
+ *
+ * The populated GemmNode is stored into the wrapper's internal `data_`.
+ *
+ * @param args Positional serialized arguments produced by the TL frontend:
+ *   expected layout is:
+ *     [Aptr, Bptr, Cptr, trans_A (Bool), trans_B (Bool),
+ *      M (Int), N (Int), K (Int), policy (Int), clear_accum (Bool),
+ *      stride_A (Int), stride_B (Int), offset_A (Int), offset_B (Int),
+ *      (optional) kPack (Int), (optional) wg_wait (Int)]
+ * @param vmap Mapping from access pointer vars to Buffer objects used to
+ *   resolve the Buffer corresponding to each pointer argument.
+ *
+ * @note If `kPack` is provided it must be 1; otherwise the constructor
+ *       fails with an ICHECK (runtime assertion). No other validation is
+ *       performed here.
+ */
 Gemm::Gemm(Array<PrimExpr> args, BufferMap vmap) {
-  Aptr = args[0];
-  Bptr = args[1];
-  Cptr = args[2];
-  A = vmap[GetVarFromAccessPtr(Aptr)];
-  B = vmap[GetVarFromAccessPtr(Bptr)];
-  C = vmap[GetVarFromAccessPtr(Cptr)];
-  trans_A = args[3].as<Bool>().value();
-  trans_B = args[4].as<Bool>().value();
-  M = args[5].as<IntImm>().value()->value;
-  N = args[6].as<IntImm>().value()->value;
-  K = args[7].as<IntImm>().value()->value;
-  policy = static_cast<GemmWarpPolicy>(args[8].as<IntImm>().value()->value);
-  clear_accum = args[9].as<Bool>().value();
-  if (args.size() > 10) {
-    kPack = args[10].as<IntImm>().value()->value;
-    if (kPack != 1 && kPack != 2) {
+  ObjectPtr<GemmNode> node = make_object<GemmNode>();
+
+  node->Aptr = args[0];
+  node->Bptr = args[1];
+  node->Cptr = args[2];
+  node->A = vmap[GetVarFromAccessPtr(node->Aptr)];
+  node->B = vmap[GetVarFromAccessPtr(node->Bptr)];
+  node->C = vmap[GetVarFromAccessPtr(node->Cptr)];
+  node->trans_A = args[3].as<Bool>().value();
+  node->trans_B = args[4].as<Bool>().value();
+  node->M = args[5].as<IntImm>().value()->value;
+  node->N = args[6].as<IntImm>().value()->value;
+  node->K = args[7].as<IntImm>().value()->value;
+  node->policy = GemmWarpPolicy(args[8].as<IntImm>().value()->value);
+  node->clear_accum = args[9].as<PrimExpr>().value();
+  node->stride_A = args[10].as<IntImm>().value()->value;
+  node->stride_B = args[11].as<IntImm>().value()->value;
+  node->offset_A = args[12].as<IntImm>().value()->value;
+  node->offset_B = args[13].as<IntImm>().value()->value;
+  if (args.size() > 14) {
+    node->kPack = args[14].as<IntImm>().value()->value;
+    if (node->kPack != 1 && node->kPack != 2) {
       ICHECK(false) << "kPack must be 1 or 2";
     }
   }
-  if (args.size() > 11) {
-    wg_wait = args[11].as<IntImm>().value()->value;
+  if (args.size() > 15) {
+    node->wg_wait = args[15].as<IntImm>().value()->value;
+  }
+  node->mbarptr = args[16];
+  if (node->mbarptr.as<CallNode>()) {
+    node->mbar = vmap[GetVarFromAccessPtr(node->mbarptr)];
+  } else {
+    node->mbar = std::nullopt;
+  }
+  node->C_coords = Array<PrimExpr>(
+      {args[17].as<PrimExpr>().value(), args[18].as<PrimExpr>().value()});
+  data_ = std::move(node);
+}
+
+/**
+ * @brief Create a copy of this GemmNode as a TileOperator.
+ *
+ * Constructs a new GemmNode by copying the current node state and returns it
+ * wrapped in a Gemm TileOperator.
+ *
+ * @return TileOperator A Gemm operator that owns a copy of this node.
+ */
+TileOperator GemmNode::Clone() const {
+  auto op = make_object<GemmNode>(*this);
+  return Gemm(op);
+}
+
+bool GemmNode::AllowTCGEN5MMA(Target target) const {
+  return TargetIsSm100(target) &&
+         ((A.scope() == "shared.dyn" || A.scope() == "shared" ||
+           A.scope() == "shared.tmem") &&
+          (B.scope() == "shared.dyn" || B.scope() == "shared") &&
+          C.scope() == "shared.tmem") &&
+         GetTCGEN5MMAMeta(M, N, K, A->dtype, C->dtype).first;
+}
+
+bool GemmNode::AllowWGMMA(int block_size, Target target) const {
+  tvm::transform::PassContext ctxt = tvm::transform::PassContext::Current();
+
+  int warp_size = TargetGetWarpSize(target);
+  int num_warps = block_size / warp_size;
+  return !ctxt->GetConfig(kDisableWGMMA, Optional<Bool>()).value_or(false) &&
+         TargetIsHopper(target) && (this->M >= 64) && (num_warps % 4 == 0) &&
+         CheckWGMMA();
+}
+
+GemmInst GemmNode::GetGemmInst(int block_size, Target target) const {
+  bool allow_tcgen5mma = AllowTCGEN5MMA(target);
+  bool allow_wgmma = AllowWGMMA(block_size, target);
+  if (allow_tcgen5mma) {
+    return GemmInst::kTCGEN5MMA;
+  } else if (allow_wgmma) {
+    return GemmInst::kWGMMA;
+  } else if (TargetIsCDNA(target)) {
+    return GemmInst::kMFMA;
+  } else if (TargetIsCuda(target)) {
+    return GemmInst::kMMA;
+  } else {
+    ICHECK(0) << "Unsupported target for gemm: " << target->str();
   }
 }
 
-std::pair<int, int> Gemm::ComputeWarpPartition(int num_warps, Target target,
-                                               bool maybe_hopper_wgmma) const {
+std::pair<int, int> GemmWarpPolicyNode::ComputeWarpPartition(
+    int M, int N, int block_size, Target target, GemmInst gemm_inst) const {
+  int num_warps = block_size / TargetGetWarpSize(target);
+  if (gemm_inst == GemmInst::kTCGEN5MMA) {
+    return {1, num_warps}; // TCGEN5MMA doesn't care about warp partitioning
+  }
+
   int m_warp = 1, n_warp = 1;
   constexpr int kMPerWarp = 16; // Rows processed by a single warp
   constexpr int kNPerWarp = 8;  // Columns processed by a single warp
-  bool allow_wgmma = TargetIsHopper(target) && maybe_hopper_wgmma &&
-                     (this->M >= 64) && (num_warps % 4 == 0);
-  ICHECK(this->M % kMPerWarp == 0)
-      << "M must be divisible by " << kMPerWarp << ", but got " << this->M;
-  ICHECK(this->N % kNPerWarp == 0)
-      << "N must be divisible by " << kNPerWarp << ", but got " << this->N;
-  if (allow_wgmma) {
+  ICHECK(M % kMPerWarp == 0)
+      << "M must be divisible by " << kMPerWarp << ", but got " << M;
+  ICHECK(N % kNPerWarp == 0)
+      << "N must be divisible by " << kNPerWarp << ", but got " << N;
+
+  if (gemm_inst == GemmInst::kWGMMA) {
     ICHECK(num_warps % 4 == 0) << "Warp-Group MMA requires 128k threads.";
 
     constexpr int kGroup = 4; // Number of warps in a warp-group
@@ -77,22 +222,22 @@ std::pair<int, int> Gemm::ComputeWarpPartition(int num_warps, Target target,
     m_warp = kGroup; // Initially, only one warp-group on M dimension
     n_warp = num_warps / m_warp; // Rest all on N dimension
 
-    if (this->policy == GemmWarpPolicy::kFullRow) {
+    if (this->isFullRow()) {
       // Try to put as many warp-groups as possible on M dimension
       // (decreasing multiples of 4, ensuring divisibility by M)
       for (int cand = num_warps; cand >= kGroup; cand -= kGroup) {
-        if (this->M % (cand * kMPerWarp) == 0) {
+        if (M % (cand * kMPerWarp) == 0) {
           m_warp = cand;
           n_warp = num_warps / m_warp;
           break;
         }
       }
-    } else if (this->policy == GemmWarpPolicy::kFullCol) {
+    } else if (this->isFullCol()) {
       // Try to use warps on N dimension; if N is not divisible, split excess
       // groups to M
-      int cand_n = n_warp;                       // Initially assume all on N
-      if (this->N % (cand_n * kNPerWarp) != 0) { // N direction division fails
-        int max_n = this->N / kNPerWarp;
+      int cand_n = n_warp;                 // Initially assume all on N
+      if (N % (cand_n * kNPerWarp) != 0) { // N direction division fails
+        int max_n = N / kNPerWarp;
         // Find a feasible n_warp from max possible downwards, ensuring
         // num_warps/n_warp is multiple of 4
         for (int n = std::min(cand_n, max_n); n >= 1; --n) {
@@ -103,12 +248,12 @@ std::pair<int, int> Gemm::ComputeWarpPartition(int num_warps, Target target,
           }
         }
       }
-    } else if (this->policy == GemmWarpPolicy::kSquare) {
+    } else if (this->isSquare()) {
       // Exhaustive search, but m must be multiple of 4
-      int max_m = this->M / kMPerWarp;
-      int max_n = this->N / kNPerWarp;
+      int max_m = M / kMPerWarp;
+      int max_n = N / kNPerWarp;
 
-      float ideal = this->N > 0 ? static_cast<float>(this->M) / this->N : 1.f;
+      float ideal = N > 0 ? static_cast<float>(M) / N : 1.f;
 
       float best_score = std::numeric_limits<float>::max();
       int best_m = kGroup, best_n = n_warp;
@@ -120,8 +265,8 @@ std::pair<int, int> Gemm::ComputeWarpPartition(int num_warps, Target target,
         if (n > max_n)
           continue;
 
-        float m_per_warp = static_cast<float>(this->M) / (m * kMPerWarp);
-        float n_per_warp = static_cast<float>(this->N) / (n * kNPerWarp);
+        float m_per_warp = static_cast<float>(M) / (m * kMPerWarp);
+        float n_per_warp = static_cast<float>(N) / (n * kNPerWarp);
         float score = std::abs(m_per_warp / n_per_warp - ideal);
 
         if (score < best_score) {
@@ -137,71 +282,77 @@ std::pair<int, int> Gemm::ComputeWarpPartition(int num_warps, Target target,
     }
 
     ICHECK(m_warp * n_warp == num_warps)
-        << "m_warp * n_warp must equal num_warps";
+        << "m_warp * n_warp must equal num_warps, m_warp: " << m_warp
+        << ", n_warp: " << n_warp << ", num_warps: " << num_warps;
+
+    // Store the computed values in the object's member variables
+    this->m_warp = m_warp;
+    this->n_warp = n_warp;
+
     return {m_warp, n_warp};
   }
 
-  if (this->policy == GemmWarpPolicy::kFullRow) {
+  if (this->isFullRow()) {
     // Try to partition M first
     m_warp = num_warps;
     n_warp = 1;
 
     // If M cannot be evenly divided by m_warp*16, try to split remaining warps
     // to N
-    if (this->M % (m_warp * kMPerWarp) != 0) {
+    if (M % (m_warp * kMPerWarp) != 0) {
       // Calculate how many warps we can use for M
-      int max_m_warps = this->M / kMPerWarp;
+      int max_m_warps = M / kMPerWarp;
       m_warp = max_m_warps;
       // Use remaining warps for N
       n_warp = num_warps / m_warp;
       if (n_warp == 0)
         n_warp = 1;
     }
-  } else if (this->policy == GemmWarpPolicy::kFullCol) {
+  } else if (this->isFullCol()) {
     // Try to partition N first
     m_warp = 1;
     n_warp = num_warps;
 
     // If N cannot be evenly divided by n_warp*8, try to split remaining warps
     // to M
-    if (this->N % (n_warp * kNPerWarp) != 0) {
+    if (N % (n_warp * kNPerWarp) != 0) {
       // Calculate how many warps we can use for N
-      int max_n_warps = this->N / kNPerWarp;
+      int max_n_warps = N / kNPerWarp;
       n_warp = max_n_warps;
       // Use remaining warps for M
       m_warp = num_warps / n_warp;
       if (m_warp == 0)
         m_warp = 1;
     }
-  } else if (this->policy == GemmWarpPolicy::kSquare) {
+  } else if (this->isSquare()) {
     // First calculate the maximum possible warps for each dimension
     int max_m_warps =
-        this->M / kMPerWarp; // Each warp needs at least 16 elements in M
-    int max_n_warps =
-        this->N / kNPerWarp; // Each warp needs at least 8 elements in N
+        M / kMPerWarp; // Each warp needs at least 16 elements in M
 
     // Calculate the ideal ratio of M/N warps based on the matrix dimensions
     float ideal_ratio = 1.0f;
-    if (this->N > 0) {
-      ideal_ratio = static_cast<float>(this->M) / this->N;
+    if (N > 0) {
+      ideal_ratio = static_cast<float>(M) / N;
     }
 
-    // Start with a balanced initial guess
-    m_warp = 1;
-    n_warp = 1;
-
     // Try to find the best balanced partition
     int best_m = 1;
     int best_n = 1;
     float best_balance = std::numeric_limits<float>::max();
-
     // Try all possible combinations that satisfy the constraints
     for (int m = 1; m <= max_m_warps && m <= num_warps; m++) {
       int n = num_warps / m;
 
       // Calculate how balanced this partition is
-      float m_per_warp = static_cast<float>(this->M) / (m * kMPerWarp);
-      float n_per_warp = static_cast<float>(this->N) / (n * kNPerWarp);
+      float m_per_warp = static_cast<float>(M) / (m * kMPerWarp);
+      float n_per_warp = static_cast<float>(N) / (n * kNPerWarp);
+      // m_per_warp and n_per_warp must be greater than 1
+      if (m_per_warp < 1 || n_per_warp < 1)
+        continue;
+      // m * n must equal num_warps
+      if (m * n != num_warps)
+        continue;
+
       float balance = std::abs(m_per_warp / n_per_warp - ideal_ratio);
 
       if (balance < best_balance) {
@@ -216,67 +367,298 @@ std::pair<int, int> Gemm::ComputeWarpPartition(int num_warps, Target target,
   } else {
     ICHECK(0) << "Unknown GemmWarpPolicy";
   }
+  ICHECK(m_warp * n_warp == num_warps)
+      << "m_warp * n_warp must equal num_warps, m_warp: " << m_warp
+      << ", n_warp: " << n_warp << ", num_warps: " << num_warps;
+
+  // Store the computed values in the object's member variables
+  this->m_warp = m_warp;
+  this->n_warp = n_warp;
+
   return {m_warp, n_warp};
 }
 
-Stmt Gemm::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
-  int warp_size = 32;
-  if (TargetIsCDNA(T.target)) {
-    warp_size = 64;
+/**
+ * @brief Checks whether WGMMA (warp-group MMA) can be used for this GEMM.
+ *
+ * Evaluates device-memory placement, data-type combinations, transpose flags,
+ * and K divisibility constraints required for the Hopper WGMMA code path.
+ *
+ * The check returns true only when:
+ * - B resides in shared memory ("shared" or "shared.dyn"); and
+ * - (C, A, B) dtypes match one of the supported combinations below and K
+ *   satisfies the required alignment; and
+ * - for combinations that require specific orientations, A is not transposed
+ *   and B is transposed.
+ *
+ * Supported combinations and constraints:
+ * - C=float16:
+ *   - A=float16, B=float16: K % 16 == 0
+ *   - Various float8 mixes (e4m3/e5m2): require (!trans_A && trans_B) and K %
+ * 32 == 0
+ * - C=float32:
+ *   - A=float16, B=float16: K % 16 == 0
+ *   - A=bfloat16, B=bfloat16: K % 16 == 0
+ *   - A=float32, B=float32: require (!trans_A && trans_B) and K % 8 == 0
+ *   - Various float8 mixes: require (!trans_A && trans_B) and K % 32 == 0
+ * - C=int32:
+ *   - 8-bit integer combinations (Int8/UInt8): require (!trans_A && trans_B)
+ * and K % 32 == 0
+ *
+ * @return true if WGMMA is supported for the current buffers, dtypes, and
+ *         transpose/shape constraints; false otherwise.
+ */
+bool GemmNode::CheckWGMMA() const {
+  if (B.scope() != "shared.dyn" && B.scope() != "shared") {
+    return false;
   }
-  auto block_size = *as_const_int(T.thread_bounds->extent);
-  bool maybe_wgmma = TargetIsHopper(T.target) && (this->M >= 64) &&
-                     (block_size / warp_size % 4 == 0);
 
+  if (C->dtype == DataType::Float(16)) {
+    if (A->dtype == DataType::Float(16) && B->dtype == DataType::Float(16))
+      return K % 16 == 0;
+    else if (A->dtype.is_float8_e4m3() && B->dtype.is_float8_e4m3())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype.is_float8_e4m3() && B->dtype.is_float8_e5m2())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype.is_float8_e5m2() && B->dtype.is_float8_e4m3())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype.is_float8_e5m2() && B->dtype.is_float8_e5m2())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else
+      return false;
+  } else if (C->dtype == DataType::Float(32)) {
+    if (A->dtype == DataType::Float(16) && B->dtype == DataType::Float(16))
+      return K % 16 == 0;
+    else if (A->dtype == DataType::BFloat(16) &&
+             B->dtype == DataType::BFloat(16))
+      return K % 16 == 0;
+    else if (A->dtype == DataType::Float(32) && B->dtype == DataType::Float(32))
+      return (!trans_A) && trans_B && K % 8 == 0;
+    else if (A->dtype.is_float8_e4m3() && B->dtype.is_float8_e4m3())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype.is_float8_e4m3() && B->dtype.is_float8_e5m2())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype.is_float8_e5m2() && B->dtype.is_float8_e4m3())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype.is_float8_e5m2() && B->dtype.is_float8_e5m2())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else
+      return false;
+  } else if (C->dtype == DataType::Int(32)) {
+    if (A->dtype == DataType::Int(8) && B->dtype == DataType::Int(8))
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype == DataType::Int(8) && B->dtype == DataType::UInt(8))
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype == DataType::UInt(8) && B->dtype == DataType::Int(8))
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype == DataType::UInt(8) && B->dtype == DataType::UInt(8))
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else
+      return false;
+  } else {
+    return false;
+  }
+}
+
+/**
+ * @brief Parse and return the numeric GPU architecture from a Target's "arch"
+ * attribute.
+ *
+ * Examines the target's "arch" string and, if it matches the pattern
+ * "sm_<num>", returns <num> as an int. If the attribute is present but does not
+ * match that pattern, returns 0.
+ *
+ * Preconditions: the target must have an "arch" attribute (this is checked via
+ * ICHECK).
+ *
+ * @return int The parsed architecture number (e.g., 80 for "sm_80"), or 0 if
+ * the arch string does not match "sm_<num>".
+ */
+static int GetArchInt(Target target) {
+  int arch_int = 0;
+  auto s = target->GetAttr<String>("arch");
+  ICHECK(s.defined());
+  std::string arch = s.value();
+  if (arch.rfind("sm_", 0) == 0) {
+    arch_int = std::stoi(arch.substr(3));
+  } else {
+    arch_int = 0;
+  }
+  return arch_int;
+}
+
+/**
+ * @brief Lower the GEMM operator to a TL TIR call expression.
+ *
+ * Constructs a tl::gemm call string parameterized by M, N, K, warp partition,
+ * transpose flags, accumulation clearing, target-specific stride/offset/kPack
+ * and optional workgroup wait value, then returns an Evaluate(call) node
+ * invoking tl::tl_gemm with the composed string and the A/B/C buffer handles.
+ *
+ * @param T Contains lowering context including thread bounds and target.
+ * @param analyzer Optional arithmetic analyzer used by lowering (may be
+ * nullptr).
+ * @return Stmt A TIR statement representing the evaluated TL GEMM call.
+ */
+Stmt GemmNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  auto block_size = *as_const_int(T.thread_bounds->extent);
+  GemmInst gemm_inst = GetGemmInst(block_size, T.target);
   auto [warp_m, warp_n] =
-      ComputeWarpPartition(block_size / warp_size, T.target, maybe_wgmma);
+      policy->ComputeWarpPartition(M, N, block_size, T.target, gemm_inst);
 
   std::stringstream ss;
-  std::string op_name = "tl::gemm_ss";
+  std::string op_name;
+
+  if (gemm_inst == GemmInst::kTCGEN5MMA) {
+    auto [can_use_tcgen5mma, meta] =
+        GetTCGEN5MMAMeta(M, N, K, A->dtype, C->dtype);
+    ICHECK(can_use_tcgen5mma);
+    ICHECK(B.scope() == "shared.dyn" || B.scope() == "shared");
+    ICHECK(C.scope() == "shared.tmem");
+    ICHECK(mbar.has_value()) << "mbar must be provided for TCGEN5MMA";
+    if (A.scope() == "shared.tmem") {
+      op_name = "tl::tcgen5mma_gemm_ts";
+    } else if (A.scope() == "shared.dyn" || A.scope() == "shared") {
+      op_name = "tl::tcgen5mma_gemm_ss";
+    } else {
+      ICHECK(0)
+          << "Unsupported A scope for TCGEN5MMA: "
+          << A.scope(); // If this is triggered, it means Tilelang has bugs.
+    }
+    ICHECK(wg_wait == -1)
+        << "Currently only wg_wait == -1 is supported for TCGEN5MMA. Please "
+           "use "
+           "wg_wait = -1 and manually synchronize with mbarrier.";
+
+    std::string accum_dtype = "";
+    if (C->dtype.is_float()) {
+      if (C->dtype.bits() == 32) {
+        accum_dtype = "float";
+      }
+    }
+    ICHECK(!accum_dtype.empty())
+        << "Unsupported C dtype for TCGEN5MMA: " << C->dtype;
+    ss << op_name << "<" << M << ", " << N << ", " << K << ", ";
+    ss << meta.atom_m << ", " << meta.atom_n << ", " << meta.atom_k << ", ";
+    ss << trans_A << ", " << trans_B << ", ";
+    ss << accum_dtype;
+    ss << ">";
+
+    auto C_buffer = T.buffer_remap.count(C) ? T.buffer_remap[C] : C;
+    Array<PrimExpr> new_args;
+    new_args.push_back(StringImm(ss.str()));
+    new_args.push_back(Aptr);
+    new_args.push_back(Bptr);
+    new_args.push_back(BufferLoad(C_buffer, C_coords));
+    new_args.push_back(mbarptr);
+    new_args.push_back(clear_accum);
+    auto new_call = Call(DataType::Handle(), builtin::call_extern(), new_args);
+
+    // Since TCGEN5MMA atoms provided by CUTLASS always have an internal
+    // `elect_one_sync()`, we check if we are calling it using full warps
+    constexpr int warp_size = 32;
+    ICHECK(
+        analyzer->CanProveEqual(FloorMod(T.thread_bounds->min, warp_size), 0) &&
+        analyzer->CanProveEqual(FloorMod(T.thread_bounds->extent, warp_size),
+                                0))
+        << "TCGEN5MMA requires thread bounds to be multiples of warp size (32) "
+           "and aligned to warps.";
+    if (analyzer->CanProveEqual(T.thread_bounds->extent, warp_size)) {
+      // If the thread bounds is exactly one warp, we can use the original call
+      return Evaluate(new_call);
+    } else {
+      // Add an if-else clause
+      auto tcgen5mma_call =
+          IfThenElse(EQ(FloorDiv(T.thread_var, warp_size),
+                        FloorDiv(T.thread_bounds->min, warp_size)),
+                     Evaluate(new_call));
+      return tcgen5mma_call;
+    }
+  }
+
   if (A.scope() == "local.fragment") {
     ICHECK(B.scope() != "local.fragment");
     op_name = "tl::gemm_rs";
   } else if (B.scope() == "local.fragment") {
     op_name = "tl::gemm_sr";
+  } else {
+    op_name = "tl::gemm_ss";
   }
+  ICHECK(C.scope() == "local.fragment");
+
   ss << op_name << "<" << M << ", " << N << ", " << K << ", ";
   ss << warp_m << ", " << warp_n << ", ";
   ss << trans_A << ", " << trans_B;
-  ss << ", " << clear_accum;
+  auto clear_accum_bool = clear_accum.as<Bool>();
+  ICHECK(clear_accum_bool.has_value())
+      << "clear_accum must be a constant Bool type, got " << clear_accum;
+  ss << ", " << bool(clear_accum_bool.value());
+  if (TargetIsCuda(T.target) && (GetArchInt(T.target) >= 75)) {
+    ss << ", " << stride_A << ", " << stride_B;
+    ss << ", " << offset_A << ", " << offset_B;
+  }
   if (TargetIsCDNA(T.target)) {
     // for cdna gemm, we need to specify kPack
     ss << ", " << kPack;
   } else if (TargetIsHopper(T.target)) {
-    ss << ", " << (maybe_wgmma ? "true" : "false");
+    ss << ", " << (gemm_inst == GemmInst::kWGMMA ? "true" : "false");
   }
-  if (wg_wait != 0) {
-    ss << ", " << wg_wait;
+
+  // Emit wg_wait if necessary
+  if (TargetIsHopper(T.target)) {
+    if (wg_wait != 0) {
+      ss << ", " << wg_wait;
+    }
+  } else if (TargetIsSm100(T.target)) {
+    // NOTE On sm100, only the leading thread issues the TCGEN5MMA instruction
+    // but all threads need to wait, so we emit another statement for cases
+    // where wg_wait == 0.
+    ICHECK(wg_wait == 0 || wg_wait == -1)
+        << "wg_wait must be 0 or -1 for Sm100";
+  } else {
+    ICHECK(wg_wait == 0)
+        << "wg_wait must be 0 for non-Hopper and non-Sm100 targets";
   }
   ss << ">";
-  auto A_buffer = T.buffer_remap.count(A) ? T.buffer_remap[A] : A;
-  auto B_buffer = T.buffer_remap.count(B) ? T.buffer_remap[B] : B;
-  auto C_buffer = T.buffer_remap[C];
-
-  Array<PrimExpr> new_args;
-  new_args.push_back(StringImm(ss.str()));
-  new_args.push_back(Aptr);
-  new_args.push_back(Bptr);
-  new_args.push_back(Cptr);
-  auto new_call = Call(DataType::Handle(), builtin::call_extern(), new_args);
+
+  auto new_call = Call(DataType::Handle(), tl::tl_gemm(),
+                       Array<PrimExpr>{StringImm(ss.str()), Aptr, Bptr, Cptr});
   return Evaluate(new_call);
 }
 
-LayoutMap Gemm::InferLayout(const LayoutInferArgs &T, InferLevel level) {
+/**
+ * @brief Infer and bind target-specific memory/layout mappings for A, B, and C.
+ *
+ * Infers per-buffer layouts (fragment or shared-memory layouts) for this GEMM
+ * operator according to the target architecture, thread bounds, warp
+ * partitioning, data types, and transpose flags, then binds fragment layouts
+ * to the thread range when required.
+ *
+ * Preconditions:
+ * - C.scope() == "local.fragment"
+ *
+ * Side effects:
+ * - Marks layout inference as completed (sets completed_ = true).
+ * - May abort via ICHECK on unsupported targets, invalid buffer scopes, or
+ *   incompatible shape constraints.
+ *
+ * @param T Input layout-inference context (provides thread bounds and target).
+ * @return LayoutMap mapping A, B, and C to their inferred layouts.
+ */
+LayoutMap GemmNode::InferLayout(const LayoutInferArgs &T,
+                                InferLevel level) const {
   if (completed_)
     return {};
   LayoutMap results;
-  ICHECK(C.scope() == "local.fragment");
   auto thread_range = T.thread_bounds;
   auto block_size = *as_const_int(thread_range->extent);
+  GemmInst gemm_inst = GetGemmInst(block_size, T.target);
+  auto [warp_m, warp_n] =
+      policy->ComputeWarpPartition(M, N, block_size, T.target, gemm_inst);
   if (TargetIsVolta(T.target)) {
-    const int warp_size = 32;
-    auto [warp_m, warp_n] =
-        ComputeWarpPartition(block_size / warp_size, T.target);
+    ICHECK(C.scope() == "local.fragment")
+        << "Volta gemm only supports C in local.fragment scope, got "
+        << C.scope();
     auto fragment =
         makeGemmVoltaFragmentC(M, N, M / warp_m, N / warp_n, C->dtype.bits());
     results.Set(C, fragment->BindThreadRange(thread_range));
@@ -284,7 +666,7 @@ LayoutMap Gemm::InferLayout(const LayoutInferArgs &T, InferLevel level) {
       int dim_A = A->shape.size();
       results.Set(A, makeGemmVoltaABLayout(*as_const_int(A->shape[dim_A - 2]),
                                            *as_const_int(A->shape[dim_A - 1]),
-                                           true, trans_A ? 1 : 2));
+                                           true, !trans_A));
     } else if (A.scope() == "local.fragment") {
       ICHECK(trans_A == false);
       auto fragment = makeGemmVoltaFragmentA(M, N, K, M / warp_m, N / warp_n);
@@ -297,11 +679,13 @@ LayoutMap Gemm::InferLayout(const LayoutInferArgs &T, InferLevel level) {
     int dim_B = B->shape.size();
     results.Set(B, makeGemmVoltaABLayout(*as_const_int(B->shape[dim_B - 2]),
                                          *as_const_int(B->shape[dim_B - 1]),
-                                         false, trans_B ? 2 : 1));
-  } else if (TargetIsAmpere(T.target) || TargetIsTuring(T.target)) {
-    const int warp_size = 32;
-    auto [warp_m, warp_n] =
-        ComputeWarpPartition(block_size / warp_size, T.target);
+                                         false, trans_B));
+  } else if (TargetIsAmpere(T.target) || TargetIsTuring(T.target) ||
+             TargetIsSM120(T.target) ||
+             (TargetIsSm100(T.target) && gemm_inst == GemmInst::kMMA)) {
+    ICHECK(C.scope() == "local.fragment")
+        << "MMA only supports C in local.fragment scope, got " << C.scope();
+
     auto fragment =
         makeGemmFragmentC(M, N, M / warp_m, N / warp_n, C->dtype.bits());
     results.Set(C, fragment->BindThreadRange(thread_range));
@@ -312,7 +696,7 @@ LayoutMap Gemm::InferLayout(const LayoutInferArgs &T, InferLevel level) {
       const int64_t mat_continuous = *as_const_int(A->shape[dim_A - 1]);
       results.Set(A,
                   makeGemmABLayout(mat_stride, mat_continuous, mat_continuous,
-                                   A->dtype.bits(), trans_A ? 1 : 2));
+                                   A->dtype.bits(), !trans_A));
     } else if (A.scope() == "local.fragment") {
       auto fragment = makeGemmFragmentA(M, N, K, M / warp_m, N / warp_n,
                                         A->dtype.bits(), trans_A);
@@ -326,7 +710,7 @@ LayoutMap Gemm::InferLayout(const LayoutInferArgs &T, InferLevel level) {
       const int64_t mat_continuous = *as_const_int(B->shape[dim_B - 1]);
       results.Set(B,
                   makeGemmABLayout(mat_stride, mat_continuous, mat_continuous,
-                                   B->dtype.bits(), trans_B ? 2 : 1));
+                                   B->dtype.bits(), trans_B));
     } else if (B.scope() == "local.fragment") {
       auto fragment =
           makeGemmFragmentB(M, N, K, M / warp_m, N / warp_n, trans_B);
@@ -335,12 +719,11 @@ LayoutMap Gemm::InferLayout(const LayoutInferArgs &T, InferLevel level) {
       ICHECK(0);
     }
   } else if (TargetIsHopper(T.target)) {
-    const int warp_size = 32;
-    bool maybe_wgmma = (this->M >= 64) && (block_size / warp_size % 4 == 0);
-    auto [warp_m, warp_n] =
-        ComputeWarpPartition(block_size / warp_size, T.target, maybe_wgmma);
+    ICHECK(C.scope() == "local.fragment")
+        << (gemm_inst == GemmInst::kWGMMA ? "WGMMA " : "MMA ")
+        << "only supports C in local.fragment scope, got " << C.scope();
     auto fragment =
-        maybe_wgmma
+        gemm_inst == GemmInst::kWGMMA
             ? makeGemmFragmentCHopper(M, N, M / warp_m, N / warp_n,
                                       C->dtype.bits())
             : makeGemmFragmentC(M, N, M / warp_m, N / warp_n, C->dtype.bits());
@@ -351,9 +734,13 @@ LayoutMap Gemm::InferLayout(const LayoutInferArgs &T, InferLevel level) {
       const int64_t mat_continuous = *as_const_int(A->shape[dim_A - 1]);
       const int64_t continuity =
           trans_A ? 4 * mat_continuous / warp_m : mat_continuous;
-      results.Set(A, makeGemmABLayoutHopper(mat_stride, mat_continuous,
-                                            mat_continuous, A->dtype.bits(),
-                                            trans_A ? 1 : 2));
+      auto ABLayout =
+          gemm_inst == GemmInst::kWGMMA
+              ? makeGemmABLayoutHopper(mat_stride, mat_continuous, continuity,
+                                       A->dtype.bits(), !trans_A)
+              : makeGemmABLayout(mat_stride, mat_continuous, mat_continuous,
+                                 A->dtype.bits(), !trans_A);
+      results.Set(A, ABLayout);
     } else {
       auto fragment = makeGemmFragmentA(M, N, K, M / warp_m, N / warp_n,
                                         A->dtype.bits(), trans_A);
@@ -365,17 +752,82 @@ LayoutMap Gemm::InferLayout(const LayoutInferArgs &T, InferLevel level) {
       const int64_t mat_continuous = *as_const_int(B->shape[dim_B - 1]);
       const int64_t continuity =
           trans_B ? mat_continuous : mat_continuous / warp_n;
-      results.Set(B,
-                  makeGemmABLayoutHopper(mat_stride, mat_continuous, continuity,
-                                         B->dtype.bits(), trans_B ? 2 : 1));
+
+      auto ABLayout =
+          gemm_inst == GemmInst::kWGMMA
+              ? makeGemmABLayoutHopper(mat_stride, mat_continuous, continuity,
+                                       B->dtype.bits(), trans_B)
+              : makeGemmABLayout(mat_stride, mat_continuous, mat_continuous,
+                                 B->dtype.bits(), trans_B);
+      results.Set(B, ABLayout);
     } else {
-      ICHECK(0) << "WGMMA only support B in shared.";
+      auto fragment =
+          makeGemmFragmentB(M, N, K, M / warp_m, N / warp_n, trans_B);
+      results.Set(B, fragment->BindThreadRange(thread_range));
+    }
+  } else if (gemm_inst == GemmInst::kTCGEN5MMA) {
+    ICHECK(C.scope() == "shared.tmem")
+        << "TCGEN5MMA only supports C in shared.tmem scope, got " << C.scope();
+    ICHECK(A.scope() == "shared.dyn" || A.scope() == "shared")
+        << "Current TCGEN5MMA only supports A in shared.dyn scope";
+    auto [can_use_tcgen5mma, meta] =
+        GetTCGEN5MMAMeta(M, N, K, A->dtype, C->dtype);
+    ICHECK(can_use_tcgen5mma);
+    {
+      int dim_A = A->shape.size();
+      const int64_t mat_stride = *as_const_int(A->shape[dim_A - 2]);
+      const int64_t mat_continuous = *as_const_int(A->shape[dim_A - 1]);
+      results.Set(A, makeGemmABLayoutSm100(mat_stride, mat_continuous,
+                                           mat_continuous, A->dtype.bits(),
+                                           trans_A ? 1 : 2));
+    }
+    {
+      int dim_B = B->shape.size();
+      const int64_t mat_stride = *as_const_int(B->shape[dim_B - 2]);
+      const int64_t mat_continuous = *as_const_int(B->shape[dim_B - 1]);
+      const int64_t continuity = mat_continuous;
+      results.Set(B,
+                  makeGemmABLayoutSm100(mat_stride, mat_continuous, continuity,
+                                        B->dtype.bits(), trans_B ? 2 : 1));
+    }
+    {
+      Layout res;
+      IterVar i = make_itervar("i", M);
+      IterVar j = make_itervar("j", N);
+      ICHECK(M % meta.atom_m == 0);
+      PrimExpr atom_idx = FloorDiv(i, meta.atom_m) +
+                          FloorDiv(j, meta.atom_n) * (M / meta.atom_m);
+      PrimExpr ai = FloorMod(i, meta.atom_m); // "ai" means "atom_i"
+      PrimExpr aj = FloorMod(j, meta.atom_n);
+      if (meta.atom_m == 128) {
+        // Layout D
+        // (https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-data-path-layout-d)
+        res = Layout(Array{i, j}, {ai, aj + atom_idx * meta.atom_n});
+      } else if (meta.atom_m == 64) {
+        // Layout E
+        // (https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-data-path-layout-e)
+        // since .ws variant is used About why we use .ws variant here, please
+        // refer to gemm_sm100.h
+        res = Layout(Array{i, j}, {FloorDiv(ai, 32) * 32 + FloorMod(ai, 32) +
+                                       FloorDiv(aj, meta.atom_n / 2) * 64,
+                                   FloorMod(aj, meta.atom_n / 2) +
+                                       atom_idx * (meta.atom_n / 2)});
+      } else if (meta.atom_m == 32) {
+        // Layout G
+        // (https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-data-path-layout-g)
+        res = Layout(
+            Array{i, j},
+            {FloorMod(ai, 32) + FloorDiv(aj, meta.atom_n / 4) * 32,
+             FloorMod(aj, meta.atom_n / 4) + atom_idx * (meta.atom_n / 4)});
+      } else {
+        ICHECK(0);
+      }
+      results.Set(C, res);
     }
   } else if (TargetIsCDNA(T.target)) {
-    const int warp_size = 64;
-    auto [warp_m, warp_n] =
-        ComputeWarpPartition(block_size / warp_size, T.target);
-
+    ICHECK(C.scope() == "local.fragment")
+        << "CDNA gemm (FMMA) only supports C in local.fragment scope, got "
+        << C.scope();
     auto fragment =
         makeGemmFragmentCCDNA(M, N, M / warp_m, N / warp_n, C->dtype.bits());
     results.Set(C, fragment->BindThreadRange(thread_range));
@@ -388,7 +840,7 @@ LayoutMap Gemm::InferLayout(const LayoutInferArgs &T, InferLevel level) {
       results.Set(A, shared_layout);
     } else if (A.scope() == "local.fragment") {
       auto fragment = makeGemmFragmentACDNA(M, N, K, M / warp_m, N / warp_n,
-                                            A->dtype.bits(), trans_A);
+                                            A->dtype.bits(), kPack, trans_A);
       results.Set(A, fragment->BindThreadRange(thread_range));
     } else {
       ICHECK(0);
@@ -419,5 +871,21 @@ TIR_REGISTER_TL_OP(Gemm, gemm)
     .set_attr<TCallEffectKind>("TCallEffectKind",
                                Integer(CallEffectKind::kOpaque));
 
+TVM_REGISTER_OP("tl.GemmWarpPolicy")
+    .set_attr<TScriptPrinterName>("TScriptPrinterName", "GemmWarpPolicy");
+
+TVM_FFI_STATIC_INIT_BLOCK({
+  GemmNode::RegisterReflection();
+  GemmWarpPolicyNode::RegisterReflection();
+  namespace refl = tvm::ffi::reflection;
+  refl::GlobalDef().def("tl.GemmWarpPolicyComputeWarpPartition",
+                        [](GemmWarpPolicy policy, int M, int N, int block_size,
+                           Target target, GemmInst gemm_inst) {
+                          policy->ComputeWarpPartition(M, N, block_size, target,
+                                                       gemm_inst);
+                          return;
+                        });
+});
+
 } // namespace tl
-} // namespace tvm
\ No newline at end of file
+} // namespace tvm
diff --git a/src/op/gemm.h b/src/op/gemm.h
index 8a01e8be..dd7e2401 100644
--- a/src/op/gemm.h
+++ b/src/op/gemm.h
@@ -7,42 +7,201 @@
 #ifndef TVM_TL_OP_GEMM_H_
 #define TVM_TL_OP_GEMM_H_
 
-#include "op.h"
+#include "operator.h"
 
 namespace tvm {
+
 namespace tl {
 
 using namespace tir;
 
-class Gemm : public Operator {
+enum class GemmWarpPolicyType : uint8_t {
+  kSquare = 0,
+  kFullRow = 1,
+  kFullCol = 2,
+  kFree = 3,
+};
+
+// Target GEMM instruction
+enum class GemmInst : uint8_t { kMMA, kWGMMA, kTCGEN5MMA, kMFMA };
+class GemmWarpPolicyNode : public Object {
 public:
-  Gemm(Array<PrimExpr> args, BufferMap vmap);
-  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const final;
-  LayoutMap InferLayout(const LayoutInferArgs &T, InferLevel level) final;
-  static const Op &Get();
-  enum class GemmWarpPolicy {
-    kSquare = 0,
-    kFullRow = 1,
-    kFullCol = 2,
-  } policy;
+  mutable int m_warp{0};
+  mutable int n_warp{0};
+  int policy_type;
 
-private:
-  std::pair<int, int>
-  ComputeWarpPartition(int num_warps, Target target,
-                       bool maybe_hopper_wgmma = true) const;
+  static constexpr const char *_type_key = "tl.GemmWarpPolicy";
+  TVM_DECLARE_FINAL_OBJECT_INFO(GemmWarpPolicyNode, Object);
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<GemmWarpPolicyNode>()
+        .def_ro("policy_type", &GemmWarpPolicyNode::policy_type)
+        .def_ro("m_warp", &GemmWarpPolicyNode::m_warp)
+        .def_ro("n_warp", &GemmWarpPolicyNode::n_warp);
+  }
+
+  bool SEqualReduce(const GemmWarpPolicyNode *other,
+                    SEqualReducer equal) const {
+    return equal(policy_type, other->policy_type) &&
+           equal(m_warp, other->m_warp) && equal(n_warp, other->n_warp);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(policy_type);
+    hash_reduce(m_warp);
+    hash_reduce(n_warp);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  std::pair<int, int> ComputeWarpPartition(int M, int N, int block_size,
+                                           Target target,
+                                           GemmInst gemm_inst) const;
+
+  bool isSquare() const {
+    return policy_type == int(GemmWarpPolicyType::kSquare);
+  }
+  bool isFullRow() const {
+    return policy_type == int(GemmWarpPolicyType::kFullRow);
+  }
+  bool isFullCol() const {
+    return policy_type == int(GemmWarpPolicyType::kFullCol);
+  }
+  bool isFree() const { return policy_type == int(GemmWarpPolicyType::kFree); }
+};
+
+class GemmWarpPolicy : public ObjectRef {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(GemmWarpPolicy, ObjectRef, GemmWarpPolicyNode);
+
+  explicit GemmWarpPolicy(GemmWarpPolicyType policy_type) {
+    auto node = make_object<GemmWarpPolicyNode>();
+    node->policy_type = (int)policy_type;
+    data_ = std::move(node);
+  }
+
+  explicit GemmWarpPolicy(int policy_type) {
+    auto node = make_object<GemmWarpPolicyNode>();
+    node->policy_type = policy_type;
+    data_ = std::move(node);
+  }
 
-  Array<PrimExpr> call_args;
+  explicit GemmWarpPolicy(int m_warp, int n_warp) {
+    auto node = make_object<GemmWarpPolicyNode>();
+    node->m_warp = m_warp;
+    node->n_warp = n_warp;
+    node->policy_type = (int)GemmWarpPolicyType::kFree;
+    data_ = std::move(node);
+  }
+};
+
+class GemmNode : public TileOperatorNode {
+public:
+  bool CheckWGMMA() const;
   tir::Buffer A, B, C;
   // pointer to the A, B, C
   PrimExpr Aptr, Bptr, Cptr;
   bool trans_A, trans_B;
   int M, N, K;
-  bool clear_accum = false;
+  int stride_A, stride_B;
+  int offset_A, offset_B;
+  PrimExpr clear_accum = const_false();
   // k_pack please ref to bitblas/tl/mfma_macro_generator.py::k_pack
   // only will be enabled under cdna mfma instructions
   int kPack = 1;
   int wg_wait = 0;
-  bool completed_ = false;
+  PrimExpr mbarptr;
+  std::optional<tir::Buffer> mbar; // mbar is optional, only used for TCGEN5MMA
+  Array<PrimExpr> C_coords;
+  mutable GemmWarpPolicy policy;
+
+  static constexpr const char *_type_key = "tl.Gemm";
+  TVM_DECLARE_FINAL_OBJECT_INFO(GemmNode, TileOperatorNode);
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<GemmNode>()
+        .def_ro("A", &GemmNode::A)
+        .def_ro("B", &GemmNode::B)
+        .def_ro("C", &GemmNode::C)
+        .def_ro("Aptr", &GemmNode::Aptr)
+        .def_ro("Bptr", &GemmNode::Bptr)
+        .def_ro("Cptr", &GemmNode::Cptr)
+        .def_ro("trans_A", &GemmNode::trans_A)
+        .def_ro("trans_B", &GemmNode::trans_B)
+        .def_ro("M", &GemmNode::M)
+        .def_ro("N", &GemmNode::N)
+        .def_ro("K", &GemmNode::K)
+        .def_ro("stride_A", &GemmNode::stride_A)
+        .def_ro("stride_B", &GemmNode::stride_B)
+        .def_ro("offset_A", &GemmNode::offset_A)
+        .def_ro("offset_B", &GemmNode::offset_B)
+        .def_ro("clear_accum", &GemmNode::clear_accum)
+        .def_ro("kPack", &GemmNode::kPack)
+        .def_ro("wg_wait", &GemmNode::wg_wait)
+        .def_ro("policy", &GemmNode::policy);
+  }
+
+  bool SEqualReduce(const GemmNode *other, SEqualReducer equal) const {
+    return equal(A, other->A) && equal(B, other->B) && equal(C, other->C) &&
+           equal(Aptr, other->Aptr) && equal(Bptr, other->Bptr) &&
+           equal(Cptr, other->Cptr) && equal(trans_A, other->trans_A) &&
+           equal(trans_B, other->trans_B) && equal(M, other->M) &&
+           equal(N, other->N) && equal(K, other->K) &&
+           equal(stride_A, other->stride_A) &&
+           equal(stride_B, other->stride_B) &&
+           equal(offset_A, other->offset_A) &&
+           equal(offset_B, other->offset_B) &&
+           equal(clear_accum, other->clear_accum) &&
+           equal(kPack, other->kPack) && equal(wg_wait, other->wg_wait) &&
+           equal(policy, other->policy);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(A);
+    hash_reduce(B);
+    hash_reduce(C);
+    hash_reduce(Aptr);
+    hash_reduce(Bptr);
+    hash_reduce(Cptr);
+    hash_reduce(trans_A);
+    hash_reduce(trans_B);
+    hash_reduce(M);
+    hash_reduce(N);
+    hash_reduce(K);
+    hash_reduce(stride_A);
+    hash_reduce(stride_B);
+    hash_reduce(offset_A);
+    hash_reduce(offset_B);
+    hash_reduce(clear_accum);
+    hash_reduce(kPack);
+    hash_reduce(wg_wait);
+    hash_reduce(policy);
+  }
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+
+  TileOperator Clone() const;
+
+private:
+  GemmInst GetGemmInst(int block_size, Target target) const;
+  bool AllowTCGEN5MMA(Target target) const;
+  bool AllowWGMMA(int block_size, Target target) const;
+
+  mutable bool completed_ = false;
+};
+
+class Gemm : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(Gemm, TileOperator, GemmNode);
+  TVM_DLL Gemm(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
 };
 
 } // namespace tl
diff --git a/src/op/gemm_py.cc b/src/op/gemm_py.cc
new file mode 100644
index 00000000..4e48389e
--- /dev/null
+++ b/src/op/gemm_py.cc
@@ -0,0 +1,293 @@
+/*!
+ * \file tl/op/gemm_py.cc
+ * \brief Implementation of General Matrix Multiplication (GEMM) operators
+ */
+
+#include "gemm_py.h"
+
+#include "builtin.h"
+#include <tvm/tir/builtin.h>
+#include <tvm/tir/op.h>
+#include <tvm/tir/op_attr_types.h>
+#include <tvm/tir/transform.h>
+
+#include "../target/utils.h"
+#include "tvm/ffi/string.h"
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+/**
+ * @brief Construct a Gemm operator from serialized TL arguments and a buffer
+ * map.
+ *
+ * This constructor deserializes operator parameters from `args` and resolves
+ * buffer references via `vmap`, populating an internal GemmPyNode with:
+ * - device pointers for A, B, C and their corresponding Buffer objects,
+ * - transpose flags for A and B,
+ * - matrix dimensions M, N, K,
+ * - warp allocation policy and clear_accum flag,
+ * - strides and memory offsets for A and B,
+ * - optional kPack (must be 1 or 2) and optional wg_wait.
+ *
+ * The populated GemmPyNode is stored into the wrapper's internal `data_`.
+ *
+ * @param args Positional serialized arguments produced by the TL frontend:
+ *   expected layout is:
+ *     [Aptr, Bptr, Cptr, trans_A (Bool), trans_B (Bool),
+ *      M (Int), N (Int), K (Int), policy (Int), clear_accum (Bool),
+ *      stride_A (Int), stride_B (Int), offset_A (Int), offset_B (Int),
+ *      (optional) kPack (Int), (optional) wg_wait (Int)]
+ * @param vmap Mapping from access pointer vars to Buffer objects used to
+ *   resolve the Buffer corresponding to each pointer argument.
+ *
+ * @note If `kPack` is provided it must be 1 or 2; otherwise the constructor
+ *       fails with an ICHECK (runtime assertion). No other validation is
+ *       performed here.
+ */
+GemmPy::GemmPy(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<GemmPyNode> node = make_object<GemmPyNode>();
+
+  node->Aptr = args[0];
+  node->Bptr = args[1];
+  node->Cptr = args[2];
+  node->A = vmap[GetVarFromAccessPtr(node->Aptr)];
+  node->B = vmap[GetVarFromAccessPtr(node->Bptr)];
+  node->C = vmap[GetVarFromAccessPtr(node->Cptr)];
+  node->trans_A = args[3].as<Bool>().value();
+  node->trans_B = args[4].as<Bool>().value();
+  node->M = args[5].as<IntImm>().value()->value;
+  node->N = args[6].as<IntImm>().value()->value;
+  node->K = args[7].as<IntImm>().value()->value;
+  node->policy = GemmWarpPolicy(args[8].as<IntImm>().value()->value);
+  node->clear_accum = args[9].as<PrimExpr>().value();
+  node->stride_A = args[10].as<IntImm>().value()->value;
+  node->stride_B = args[11].as<IntImm>().value()->value;
+  node->offset_A = args[12].as<IntImm>().value()->value;
+  node->offset_B = args[13].as<IntImm>().value()->value;
+  if (args.size() > 14) {
+    node->kPack = args[14].as<IntImm>().value()->value;
+    if (node->kPack != 1 && node->kPack != 2) {
+      ICHECK(false) << "kPack must be 1 or 2";
+    }
+  }
+  if (args.size() > 15) {
+    node->wg_wait = args[15].as<IntImm>().value()->value;
+  }
+  data_ = std::move(node);
+}
+
+/**
+ * @brief Create a copy of this GemmPyNode as a TileOperator.
+ *
+ * Constructs a new GemmPyNode by copying the current node state and returns it
+ * wrapped in a Gemm TileOperator.
+ *
+ * @return TileOperator A Gemm operator that owns a copy of this node.
+ */
+TileOperator GemmPyNode::Clone() const {
+  auto op = make_object<GemmPyNode>(*this);
+  return GemmPy(op);
+}
+
+GemmInst GemmPyNode::GetGemmInst(int block_size, Target target) const {
+  int warp_size = TargetGetWarpSize(target);
+  int num_warps = block_size / warp_size;
+  bool allow_wgmma = TargetIsHopper(target) && (this->M >= 64) &&
+                     (num_warps % 4 == 0) && CheckWGMMA();
+  if (allow_wgmma) {
+    return GemmInst::kWGMMA;
+  } else if (TargetIsCDNA(target)) {
+    return GemmInst::kMFMA;
+  } else if (TargetIsCuda(target)) {
+    return GemmInst::kMMA;
+  } else {
+    ICHECK(0) << "Unsupported target for gemm: " << target->str();
+    return GemmInst::kMMA; // This line will never be reached due to ICHECK, but
+                           // satisfies compiler
+  }
+}
+
+/**
+ * @brief Checks whether WGMMA (warp-group MMA) can be used for this GEMM.
+ *
+ * Evaluates device-memory placement, data-type combinations, transpose flags,
+ * and K divisibility constraints required for the Hopper WGMMA code path.
+ *
+ * The check returns true only when:
+ * - B resides in shared memory ("shared" or "shared.dyn"); and
+ * - (C, A, B) dtypes match one of the supported combinations below and K
+ *   satisfies the required alignment; and
+ * - for combinations that require specific orientations, A is not transposed
+ *   and B is transposed.
+ *
+ * Supported combinations and constraints:
+ * - C=float16:
+ *   - A=float16, B=float16: K % 16 == 0
+ *   - Various float8 mixes (e4m3/e5m2): require (!trans_A && trans_B) and K %
+ * 32 == 0
+ * - C=float32:
+ *   - A=float16, B=float16: K % 16 == 0
+ *   - A=bfloat16, B=bfloat16: K % 16 == 0
+ *   - A=float32, B=float32: require (!trans_A && trans_B) and K % 8 == 0
+ *   - Various float8 mixes: require (!trans_A && trans_B) and K % 32 == 0
+ * - C=int32:
+ *   - 8-bit integer combinations (Int8/UInt8): require (!trans_A && trans_B)
+ * and K % 32 == 0
+ *
+ * @return true if WGMMA is supported for the current buffers, dtypes, and
+ *         transpose/shape constraints; false otherwise.
+ */
+bool GemmPyNode::CheckWGMMA() const {
+  if (B.scope() != "shared.dyn" && B.scope() != "shared") {
+    return false;
+  }
+
+  if (C->dtype == DataType::Float(16)) {
+    if (A->dtype == DataType::Float(16) && B->dtype == DataType::Float(16))
+      return K % 16 == 0;
+    else if (A->dtype.is_float8_e4m3() && B->dtype.is_float8_e4m3())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype.is_float8_e4m3() && B->dtype.is_float8_e5m2())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype.is_float8_e5m2() && B->dtype.is_float8_e4m3())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype.is_float8_e5m2() && B->dtype.is_float8_e5m2())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else
+      return false;
+  } else if (C->dtype == DataType::Float(32)) {
+    if (A->dtype == DataType::Float(16) && B->dtype == DataType::Float(16))
+      return K % 16 == 0;
+    else if (A->dtype == DataType::BFloat(16) &&
+             B->dtype == DataType::BFloat(16))
+      return K % 16 == 0;
+    else if (A->dtype == DataType::Float(32) && B->dtype == DataType::Float(32))
+      return (!trans_A) && trans_B && K % 8 == 0;
+    else if (A->dtype.is_float8_e4m3() && B->dtype.is_float8_e4m3())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype.is_float8_e4m3() && B->dtype.is_float8_e5m2())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype.is_float8_e5m2() && B->dtype.is_float8_e4m3())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype.is_float8_e5m2() && B->dtype.is_float8_e5m2())
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else
+      return false;
+  } else if (C->dtype == DataType::Int(32)) {
+    if (A->dtype == DataType::Int(8) && B->dtype == DataType::Int(8))
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype == DataType::Int(8) && B->dtype == DataType::UInt(8))
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype == DataType::UInt(8) && B->dtype == DataType::Int(8))
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else if (A->dtype == DataType::UInt(8) && B->dtype == DataType::UInt(8))
+      return (!trans_A) && trans_B && K % 32 == 0;
+    else
+      return false;
+  } else {
+    return false;
+  }
+}
+
+/**
+ * @brief Parse and return the numeric GPU architecture from a Target's "arch"
+ * attribute.
+ *
+ * Examines the target's "arch" string and, if it matches the pattern
+ * "sm_<num>", returns <num> as an int. If the attribute is present but does not
+ * match that pattern, returns 0.
+ *
+ * Preconditions: the target must have an "arch" attribute (this is checked via
+ * ICHECK).
+ *
+ * @return int The parsed architecture number (e.g., 80 for "sm_80"), or 0 if
+ * the arch string does not match "sm_<num>".
+ */
+static int GetArchInt(Target target) {
+  int arch_int = 0;
+  auto s = target->GetAttr<String>("arch");
+  ICHECK(s.defined());
+  std::string arch = s.value();
+  if (arch.rfind("sm_", 0) == 0) {
+    arch_int = std::stoi(arch.substr(3));
+  } else {
+    arch_int = 0;
+  }
+  return arch_int;
+}
+
+Stmt GemmPyNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  auto block_size = *as_const_int(T.thread_bounds->extent);
+  GemmInst gemm_inst = GetGemmInst(block_size, T.target);
+
+  auto [warp_m, warp_n] =
+      policy->ComputeWarpPartition(M, N, block_size, T.target, gemm_inst);
+
+  if (const auto f = ffi::Function::GetGlobal("tl.gemm_py.lower")) {
+    auto prim_func =
+        Downcast<PrimFunc>((*f)(GetRef<GemmPy>(this), T.layout_map, T.target,
+                                T.thread_bounds, T.thread_var));
+    ICHECK(prim_func->attrs.defined());
+    auto global_symbol = prim_func->attrs.GetAttr<String>("global_symbol");
+    ICHECK(global_symbol.defined());
+    if (prim_func->body.as<BlockRealizeNode>()) {
+      BlockRealize block_realize = Downcast<BlockRealize>(prim_func->body);
+      auto block = block_realize->block;
+      {
+        BlockNode *n = block.CopyOnWrite();
+        n->name_hint = global_symbol.value();
+      }
+      return BlockRealize(block_realize->iter_values, block_realize->predicate,
+                          block);
+    }
+    // warp with block realize node
+    return BlockRealize(
+        /*iter_values=*/Array<PrimExpr>(),
+        /*predicate=*/const_true(),
+        /*block=*/
+        Block(/*iter_vars=*/{}, /*reads=*/{}, /*writes=*/{},
+              /*name_hint=*/global_symbol.value(), prim_func->body));
+  } else {
+    LOG(FATAL) << "No lower function found for gemm_py";
+    return Stmt(); // This line will never be reached due to LOG(FATAL), but
+                   // satisfies compiler
+  }
+}
+
+LayoutMap GemmPyNode::InferLayout(const LayoutInferArgs &T,
+                                  InferLevel level) const {
+  if (completed_)
+    return {};
+  LayoutMap results;
+
+  if (const auto f = ffi::Function::GetGlobal("tl.gemm_py.infer_layout")) {
+    results = Downcast<LayoutMap>(
+        (*f)(GetRef<GemmPy>(this), T.target, T.thread_bounds));
+  } else {
+    LOG(FATAL) << "No infer layout function found for gemm_py";
+  }
+
+  completed_ = true;
+  return results;
+}
+
+TIR_REGISTER_TL_OP(GemmPy, gemm_py)
+    .set_num_inputs(5)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TVM_FFI_STATIC_INIT_BLOCK({ GemmPyNode::RegisterReflection(); });
+
+TVM_FFI_STATIC_INIT_BLOCK({
+  namespace refl = tvm::ffi::reflection;
+  refl::GlobalDef().def("tl.GemmPyGemmInst",
+                        [](GemmPy gemm_py, int block_size, Target target) {
+                          return gemm_py->GetGemmInst(block_size, target);
+                        });
+});
+
+} // namespace tl
+} // namespace tvm
diff --git a/src/op/gemm_py.h b/src/op/gemm_py.h
new file mode 100644
index 00000000..65ed08c0
--- /dev/null
+++ b/src/op/gemm_py.h
@@ -0,0 +1,125 @@
+/*!
+ * \file tl/op/gemm_py.h
+ * \brief Define gemm operator.
+ *
+ */
+
+#ifndef TVM_TL_OP_GEMM_PY_H_
+#define TVM_TL_OP_GEMM_PY_H_
+
+#include "gemm.h"
+#include "operator.h"
+
+namespace tvm {
+
+namespace tl {
+
+using namespace tir;
+
+class GemmPyNode : public TileOperatorNode {
+public:
+  bool CheckWGMMA() const;
+  tir::Buffer A, B, C;
+  // pointer to the A, B, C
+  PrimExpr Aptr, Bptr, Cptr;
+  bool trans_A, trans_B;
+  int M, N, K;
+  int stride_A, stride_B;
+  int offset_A, offset_B;
+  PrimExpr clear_accum = const_false();
+  // k_pack please ref to bitblas/tl/mfma_macro_generator.py::k_pack
+  // only will be enabled under cdna mfma instructions
+  int kPack = 1;
+  int wg_wait = 0;
+  mutable GemmWarpPolicy policy;
+
+  static constexpr const char *_type_key = "tl.GemmPy";
+  TVM_DECLARE_FINAL_OBJECT_INFO(GemmPyNode, TileOperatorNode);
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<GemmPyNode>()
+        .def_ro("A", &GemmPyNode::A)
+        .def_ro("B", &GemmPyNode::B)
+        .def_ro("C", &GemmPyNode::C)
+        .def_ro("Aptr", &GemmPyNode::Aptr)
+        .def_ro("Bptr", &GemmPyNode::Bptr)
+        .def_ro("Cptr", &GemmPyNode::Cptr)
+        .def_ro("trans_A", &GemmPyNode::trans_A)
+        .def_ro("trans_B", &GemmPyNode::trans_B)
+        .def_ro("M", &GemmPyNode::M)
+        .def_ro("N", &GemmPyNode::N)
+        .def_ro("K", &GemmPyNode::K)
+        .def_ro("stride_A", &GemmPyNode::stride_A)
+        .def_ro("stride_B", &GemmPyNode::stride_B)
+        .def_ro("offset_A", &GemmPyNode::offset_A)
+        .def_ro("offset_B", &GemmPyNode::offset_B)
+        .def_ro("clear_accum", &GemmPyNode::clear_accum)
+        .def_ro("kPack", &GemmPyNode::kPack)
+        .def_ro("wg_wait", &GemmPyNode::wg_wait)
+        .def_ro("policy", &GemmPyNode::policy);
+  }
+
+  bool SEqualReduce(const GemmPyNode *other, SEqualReducer equal) const {
+    return equal(A, other->A) && equal(B, other->B) && equal(C, other->C) &&
+           equal(Aptr, other->Aptr) && equal(Bptr, other->Bptr) &&
+           equal(Cptr, other->Cptr) && equal(trans_A, other->trans_A) &&
+           equal(trans_B, other->trans_B) && equal(M, other->M) &&
+           equal(N, other->N) && equal(K, other->K) &&
+           equal(stride_A, other->stride_A) &&
+           equal(stride_B, other->stride_B) &&
+           equal(offset_A, other->offset_B) &&
+           equal(offset_B, other->offset_B) &&
+           equal(clear_accum, other->clear_accum) &&
+           equal(kPack, other->kPack) && equal(wg_wait, other->wg_wait) &&
+           equal(policy, other->policy);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(A);
+    hash_reduce(B);
+    hash_reduce(C);
+    hash_reduce(Aptr);
+    hash_reduce(Bptr);
+    hash_reduce(Cptr);
+    hash_reduce(trans_A);
+    hash_reduce(trans_B);
+    hash_reduce(M);
+    hash_reduce(N);
+    hash_reduce(K);
+    hash_reduce(stride_A);
+    hash_reduce(stride_B);
+    hash_reduce(offset_A);
+    hash_reduce(offset_B);
+    hash_reduce(clear_accum);
+    hash_reduce(kPack);
+    hash_reduce(wg_wait);
+    hash_reduce(policy);
+  }
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+
+  TileOperator Clone() const;
+
+  // Target GEMM instruction
+  GemmInst GetGemmInst(int block_size, Target target) const;
+
+private:
+  mutable bool completed_ = false;
+};
+
+class GemmPy : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(GemmPy, TileOperator, GemmPyNode);
+  TVM_DLL GemmPy(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+} // namespace tl
+} // namespace tvm
+
+#endif //  TVM_TL_OP_GEMM_PY_H_
\ No newline at end of file
diff --git a/src/op/gemm_sp.cc b/src/op/gemm_sp.cc
index 7a8b5831..dfa58b35 100644
--- a/src/op/gemm_sp.cc
+++ b/src/op/gemm_sp.cc
@@ -17,210 +17,138 @@
 
 namespace tvm {
 namespace tl {
-static std::vector<int> toPrimeFactors(int x) {
-  int i = 2;
-  std::vector<int> result;
-  while (x > 1) {
-    if (x % i == 0) {
-      x /= i;
-      result.push_back(i);
-    } else {
-      i++;
+
+std::pair<int, int> GemmSPWarpPolicyNode::ComputeWarpPartition(int M, int N,
+                                                               int block_size,
+                                                               Target target,
+                                                               bool use_wgmma,
+                                                               int bits) const {
+  int num_warps = block_size / TargetGetWarpSize(target);
+
+  auto [m_warp, n_warp] = GemmWarpPolicyNode::ComputeWarpPartition(
+      M, N, block_size, target, use_wgmma ? GemmInst::kWGMMA : GemmInst::kMMA);
+
+  // Special handling for gemm_sp when the tiling size is not a multiple
+  // This should be consistent with shape check in gemm_sp_sm80.h
+  int m_atom_size = bits == 16 ? 32 : 16;
+  int n_atom_size = bits == 16 ? 32 : 16;
+  static const char *err_msg =
+      "Cannot arrange the warp shape to be a multiple of atom size, please "
+      "reduce num threads or increase tiling size";
+  if (TargetIsAmpere(target)) {
+    int warp_shape_m = M / m_warp;
+    int warp_shape_n = N / n_warp;
+    if (warp_shape_m % m_atom_size) { // GemmWarpPolicy::kFullRow
+      m_warp = M / m_atom_size;
+      ICHECK(m_warp > 0) << err_msg;
+      n_warp = num_warps / m_warp;
+      warp_shape_n = N / n_warp;
+      ICHECK(warp_shape_n % n_atom_size == 0) << err_msg;
+    } else if (warp_shape_n % n_atom_size != 0) { // GemmWarpPolicy::kFullColumn
+      n_warp = N / n_atom_size;
+      ICHECK(n_warp > 0) << err_msg;
+      m_warp = num_warps / n_warp;
+      warp_shape_m = M / m_warp;
+      ICHECK(warp_shape_m % m_atom_size == 0) << err_msg;
     }
+    ICHECK(m_warp * n_warp == num_warps)
+        << "m_warp * n_warp must equal num_warps, please report an issue when "
+           "encounter this"
+        << ", m_warp: " << m_warp << ", n_warp: " << n_warp << ", num_warps"
+        << num_warps;
+    this->m_warp = m_warp;
+    this->n_warp = n_warp;
   }
-  return result;
+  return {m_warp, n_warp};
 }
 
+/**
+ * @brief Construct a GemmSP operator node from TL call arguments and a buffer
+ * map.
+ *
+ * Parses the expected call argument tuple and fills an internal GemmSPNode:
+ * - Buffers: A (args[0]), E (args[1]), B (args[2]), C (args[3]) are looked up
+ * in vmap.
+ * - Booleans: trans_A (args[4]), trans_B (args[5]).
+ * - Dimensions: M (args[6]), N (args[7]), K (args[8]) as integers.
+ * - Warp policy: policy (args[9]) mapped to GemmWarpPolicy.
+ * - clear_accum: boolean flag (args[10]).
+ * - Optional kPack (args[11]): must be 1 or 2 (checked via ICHECK).
+ * - Optional wg_wait (args[12]): integer workgroup wait parameter.
+ *
+ * The populated GemmSPNode is stored in the instance's internal data_ pointer.
+ *
+ * @param args Positional TL call arguments in the above order.
+ * @param vmap BufferMap mapping access pointers (from args) to Buffer objects.
+ *
+ * @note An ICHECK failure is raised if a provided kPack is not 1 or 2.
+ */
 GemmSP::GemmSP(Array<PrimExpr> args, BufferMap vmap) {
-  A = vmap[GetVarFromAccessPtr(args[0])];
-  E = vmap[GetVarFromAccessPtr(args[1])];
-  B = vmap[GetVarFromAccessPtr(args[2])];
-  C = vmap[GetVarFromAccessPtr(args[3])];
-  trans_A = args[4].as<Bool>().value();
-  trans_B = args[5].as<Bool>().value();
-  M = args[6].as<IntImm>().value()->value;
-  N = args[7].as<IntImm>().value()->value;
-  K = args[8].as<IntImm>().value()->value;
-  policy = static_cast<GemmWarpPolicy>(args[9].as<IntImm>().value()->value);
-  clear_accum = args[10].as<Bool>().value();
+  ObjectPtr<GemmSPNode> node = make_object<GemmSPNode>();
+  node->A = vmap[GetVarFromAccessPtr(args[0])];
+  node->E = vmap[GetVarFromAccessPtr(args[1])];
+  node->B = vmap[GetVarFromAccessPtr(args[2])];
+  node->C = vmap[GetVarFromAccessPtr(args[3])];
+  node->trans_A = args[4].as<Bool>().value();
+  node->trans_B = args[5].as<Bool>().value();
+  node->M = args[6].as<IntImm>().value()->value;
+  node->N = args[7].as<IntImm>().value()->value;
+  node->K = args[8].as<IntImm>().value()->value;
+  node->policy = GemmSPWarpPolicy(args[9].as<IntImm>().value()->value);
+  node->clear_accum = args[10].as<Bool>().value();
   if (args.size() > 11) {
-    kPack = args[11].as<IntImm>().value()->value;
-    if (kPack != 1 && kPack != 2) {
+    node->kPack = args[11].as<IntImm>().value()->value;
+    if (node->kPack != 1 && node->kPack != 2) {
       ICHECK(false) << "kPack must be 1 or 2";
     }
   }
   if (args.size() > 12) {
-    wg_wait = args[12].as<IntImm>().value()->value;
+    node->wg_wait = args[12].as<IntImm>().value()->value;
   }
+  data_ = std::move(node);
 }
 
-std::pair<int, int>
-GemmSP::ComputeWarpPartition(int num_warps, Target target,
-                             bool maybe_hopper_wgmma) const {
-  int m_warp = 1, n_warp = 1;
-  constexpr int kMPerWarp = 16; // Rows processed by a single warp
-  constexpr int kNPerWarp = 8;  // Columns processed by a single warp
-  bool allow_wgmma = TargetIsHopper(target) && maybe_hopper_wgmma &&
-                     (this->M >= 64) && (num_warps % 4 == 0);
-  if (allow_wgmma) {
-    ICHECK(num_warps % 4 == 0) << "Warp-Group MMA requires 128k threads.";
-
-    constexpr int kGroup = 4; // Number of warps in a warp-group
-
-    m_warp = kGroup; // Initially, only one warp-group on M dimension
-    n_warp = num_warps / m_warp; // Rest all on N dimension
-
-    if (this->policy == GemmWarpPolicy::kFullRow) {
-      // Try to put as many warp-groups as possible on M dimension
-      // (decreasing multiples of 4, ensuring divisibility by M)
-      for (int cand = num_warps; cand >= kGroup; cand -= kGroup) {
-        if (this->M % (cand * kMPerWarp) == 0) {
-          m_warp = cand;
-          n_warp = num_warps / m_warp;
-          break;
-        }
-      }
-    } else if (this->policy == GemmWarpPolicy::kFullCol) {
-      // Try to use warps on N dimension; if N is not divisible, split excess
-      // groups to M
-      int cand_n = n_warp;                       // Initially assume all on N
-      if (this->N % (cand_n * kNPerWarp) != 0) { // N direction division fails
-        int max_n = this->N / kNPerWarp;
-        // Find a feasible n_warp from max possible downwards, ensuring
-        // num_warps/n_warp is multiple of 4
-        for (int n = std::min(cand_n, max_n); n >= 1; --n) {
-          if (num_warps % n == 0 && (num_warps / n) % kGroup == 0) {
-            n_warp = n;
-            m_warp = num_warps / n_warp;
-            break;
-          }
-        }
-      }
-    } else if (this->policy == GemmWarpPolicy::kSquare) {
-      // Exhaustive search, but m must be multiple of 4
-      int max_m = this->M / kMPerWarp;
-      int max_n = this->N / kNPerWarp;
-
-      float ideal = this->N > 0 ? static_cast<float>(this->M) / this->N : 1.f;
-
-      float best_score = std::numeric_limits<float>::max();
-      int best_m = kGroup, best_n = n_warp;
-
-      for (int m = kGroup; m <= num_warps && m <= max_m; m += kGroup) {
-        if (num_warps % m)
-          continue;
-        int n = num_warps / m;
-        if (n > max_n)
-          continue;
-
-        float m_per_warp = static_cast<float>(this->M) / (m * kMPerWarp);
-        float n_per_warp = static_cast<float>(this->N) / (n * kNPerWarp);
-        float score = std::abs(m_per_warp / n_per_warp - ideal);
-
-        if (score < best_score) {
-          best_score = score;
-          best_m = m;
-          best_n = n;
-        }
-      }
-      m_warp = best_m;
-      n_warp = best_n;
-    } else {
-      ICHECK(0) << "Unknown GemmWarpPolicy";
-    }
-
-    ICHECK(m_warp * n_warp == num_warps)
-        << "m_warp * n_warp must equal num_warps";
-    return {m_warp, n_warp};
-  }
-
-  if (this->policy == GemmWarpPolicy::kFullRow) {
-    // Try to partition M first
-    m_warp = num_warps;
-    n_warp = 1;
-
-    // If M cannot be evenly divided by m_warp*16, try to split remaining warps
-    // to N
-    if (this->M % (m_warp * kMPerWarp) != 0) {
-      // Calculate how many warps we can use for M
-      int max_m_warps = this->M / kMPerWarp;
-      m_warp = max_m_warps;
-      // Use remaining warps for N
-      n_warp = num_warps / m_warp;
-      if (n_warp == 0)
-        n_warp = 1;
-    }
-  } else if (this->policy == GemmWarpPolicy::kFullCol) {
-    // Try to partition N first
-    m_warp = 1;
-    n_warp = num_warps;
-
-    // If N cannot be evenly divided by n_warp*8, try to split remaining warps
-    // to M
-    if (this->N % (n_warp * kNPerWarp) != 0) {
-      // Calculate how many warps we can use for N
-      int max_n_warps = this->N / kNPerWarp;
-      n_warp = max_n_warps;
-      // Use remaining warps for M
-      m_warp = num_warps / n_warp;
-      if (m_warp == 0)
-        m_warp = 1;
-    }
-  } else if (this->policy == GemmWarpPolicy::kSquare) {
-    // First calculate the maximum possible warps for each dimension
-    int max_m_warps =
-        this->M / kMPerWarp; // Each warp needs at least 16 elements in M
-    int max_n_warps =
-        this->N / kNPerWarp; // Each warp needs at least 8 elements in N
-
-    // Calculate the ideal ratio of M/N warps based on the matrix dimensions
-    float ideal_ratio = 1.0f;
-    if (this->N > 0) {
-      ideal_ratio = static_cast<float>(this->M) / this->N;
-    }
-
-    // Start with a balanced initial guess
-    m_warp = 1;
-    n_warp = 1;
-
-    // Try to find the best balanced partition
-    int best_m = 1;
-    int best_n = 1;
-    float best_balance = std::numeric_limits<float>::max();
-
-    // Try all possible combinations that satisfy the constraints
-    for (int m = 1; m <= max_m_warps && m <= num_warps; m++) {
-      int n = num_warps / m;
-
-      // Calculate how balanced this partition is
-      float m_per_warp = static_cast<float>(this->M) / (m * kMPerWarp);
-      float n_per_warp = static_cast<float>(this->N) / (n * kNPerWarp);
-      float balance = std::abs(m_per_warp / n_per_warp - ideal_ratio);
-
-      if (balance < best_balance) {
-        best_balance = balance;
-        best_m = m;
-        best_n = n;
-      }
-    }
-
-    m_warp = best_m;
-    n_warp = best_n;
-  } else {
-    ICHECK(0) << "Unknown GemmWarpPolicy";
-  }
-  return {m_warp, n_warp};
+/**
+ * @brief Create a deep copy of this GemmSPNode wrapped as a TileOperator.
+ *
+ * Returns a new TileOperator that owns a copy of this node. The cloned node
+ * duplicates all fields of the original; subsequent modifications to the
+ * clone do not affect the original node.
+ *
+ * @return TileOperator A TileOperator holding a cloned GemmSPNode.
+ */
+TileOperator GemmSPNode::Clone() const {
+  auto op = make_object<GemmSPNode>(*this);
+  return GemmSP(op);
 }
 
-Stmt GemmSP::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+/**
+ * @brief Lower this GemmSP node to a TL (tensile-like) intrinsic call.
+ *
+ * Constructs and returns an Evaluate statement containing a call to the
+ * TL gemm_sp intrinsic that encodes this GEMM's template parameters
+ * (M, N, K, warp partition, transposition flags, clear_accum, and optional
+ * Hopper/WGMMA and wg_wait modifiers) and the remapped buffer access pointers.
+ *
+ * The function validates that A, B, and E reside in shared (or shared.dyn)
+ * memory (ICHECK failures otherwise), computes the warp partition based on
+ * the launch configuration and target, and emits a single tl::tl_gemm_sp call
+ * with a string template describing the configuration.
+ *
+ * @param T Lowering context containing thread bounds, target, and optional
+ *          buffer remapping used to obtain the final buffer AccessPtr
+ *          arguments for the TL call.
+ * @return Stmt An Evaluate wrapping the constructed tl::tl_gemm_sp call.
+ */
+Stmt GemmSPNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
   int warp_size = 32;
 
   auto block_size = *as_const_int(T.thread_bounds->extent);
   bool maybe_wgmma = TargetIsHopper(T.target) && (this->M >= 64) &&
                      (block_size / warp_size % 4 == 0);
 
-  auto [warp_m, warp_n] =
-      ComputeWarpPartition(block_size / warp_size, T.target, maybe_wgmma);
+  auto [warp_m, warp_n] = policy->ComputeWarpPartition(
+      M, N, block_size, T.target, maybe_wgmma, A->dtype.bits());
 
   std::stringstream ss;
   std::string op_name = "tl::gemm_sp_ss";
@@ -230,7 +158,7 @@ Stmt GemmSP::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
       << " and " << B.scope();
   ICHECK((E.scope() == "shared" || E.scope() == "shared.dyn"))
       << "Only support shared.dyn scope for E as copy from smem to rmem are "
-         "delegated to cute implemntation, found "
+         "delegated to cute implementation, found "
       << E.scope();
   ss << op_name << "<" << M << ", " << N << ", " << K << ", ";
   ss << warp_m << ", " << warp_n << ", ";
@@ -248,17 +176,44 @@ Stmt GemmSP::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
   auto C_buffer = T.buffer_remap[C];
   auto E_buffer = T.buffer_remap.count(E) ? T.buffer_remap[E] : E;
 
-  Array<PrimExpr> new_args;
-  new_args.push_back(StringImm(ss.str()));
-  new_args.push_back(A_buffer.access_ptr(1));
-  new_args.push_back(B_buffer.access_ptr(1));
-  new_args.push_back(C_buffer.access_ptr(3));
-  new_args.push_back(E_buffer.access_ptr(1));
-  auto new_call = Call(DataType::Handle(), builtin::call_extern(), new_args);
+  auto new_call =
+      Call(DataType::Handle(), tl::tl_gemm_sp(),
+           Array<PrimExpr>{StringImm(ss.str()), A_buffer.access_ptr(1),
+                           B_buffer.access_ptr(1), C_buffer.access_ptr(3),
+                           E_buffer.access_ptr(1)});
   return Evaluate(new_call);
 }
 
-LayoutMap GemmSP::InferLayout(const LayoutInferArgs &T, InferLevel level) {
+/**
+ * @brief Infers and returns the memory/layout mapping for the GemmSP operator.
+ *
+ * Infers thread-local fragment layout for C and shared-memory layouts for A and
+ * B based on the target (Hopper-only path), block/thread bounds in T,
+ * transposition flags, and matrix dimensions stored in the node. The function
+ * caches its work: if layout inference has already completed (completed_ ==
+ * true) it returns an empty LayoutMap.
+ *
+ * Precondition:
+ * - C.scope() must be "local.fragment".
+ *
+ * Behavior notes:
+ * - Only the Hopper target is supported; non-Hopper targets trigger a fatal
+ * check.
+ * - For Hopper, the function computes a warp partition from block size and may
+ *   enable WGMMA-specific fragment creation when conditions on M and block size
+ *   are met.
+ * - A and B must reside in "shared" or "shared.dyn"; otherwise the function
+ *   aborts with a check failure.
+ * - The method sets completed_ = true before returning to avoid re-entrance.
+ *
+ * @param T LayoutInferArgs containing thread bounds and the target (used to
+ *          select Hopper-specific layouts).
+ * @param level Currently unused inference detail level.
+ * @return LayoutMap mapping A, B, and C to their inferred layouts (or empty if
+ *         inference was already completed).
+ */
+LayoutMap GemmSPNode::InferLayout(const LayoutInferArgs &T,
+                                  InferLevel level) const {
   if (completed_)
     return {};
   LayoutMap results;
@@ -270,8 +225,8 @@ LayoutMap GemmSP::InferLayout(const LayoutInferArgs &T, InferLevel level) {
     constexpr int wgmma_m = 16 * 4;
     bool maybe_wgmma =
         (this->M >= wgmma_m) && (block_size / warp_size % 4 == 0);
-    auto [warp_m, warp_n] =
-        ComputeWarpPartition(block_size / warp_size, T.target, maybe_wgmma);
+    auto [warp_m, warp_n] = policy->ComputeWarpPartition(
+        M, N, block_size, T.target, maybe_wgmma, A->dtype.bits());
     auto fragment =
         maybe_wgmma
             ? makeGemmFragmentCHopper(M, N, M / warp_m, N / warp_n,
@@ -282,8 +237,6 @@ LayoutMap GemmSP::InferLayout(const LayoutInferArgs &T, InferLevel level) {
       int dim_A = A->shape.size();
       const int64_t mat_stride = *as_const_int(A->shape[dim_A - 2]);
       const int64_t mat_continuous = *as_const_int(A->shape[dim_A - 1]);
-      const int64_t continuity =
-          trans_A ? 4 * mat_continuous / warp_m : mat_continuous;
       results.Set(A, makeGemmABLayoutHopper(mat_stride, mat_continuous,
                                             mat_continuous, A->dtype.bits(),
                                             trans_A ? 1 : 2));
@@ -303,17 +256,54 @@ LayoutMap GemmSP::InferLayout(const LayoutInferArgs &T, InferLevel level) {
     } else {
       ICHECK(false) << "WGMMA only support B in shared.";
     }
+  } else if (TargetIsAmpere(T.target)) {
+    auto [warp_m, warp_n] = policy->ComputeWarpPartition(
+        M, N, block_size, T.target, false, A->dtype.bits());
+    auto fragment =
+        makeGemmSparseFragmentC(M, N, M / warp_m, N / warp_n, C->dtype.bits());
+    results.Set(C, fragment->BindThreadRange(thread_range));
+
+    if (A.scope() == "shared" || A.scope() == "shared.dyn") {
+      int dim_A = A->shape.size();
+      const int64_t mat_stride = *as_const_int(A->shape[dim_A - 2]);
+      const int64_t mat_continuous = *as_const_int(A->shape[dim_A - 1]);
+      results.Set(A, makeGemmSparseAmpereABLayout(mat_stride, mat_continuous,
+                                                  A->dtype.bits()));
+    } else if (A.scope() == "local.fragment") {
+      // auto fragment = makeGemmFragmentA(M, N, K, M / warp_m, N / warp_n,
+      //                                   A->dtype.bits(), trans_A);
+      // results.Set(A, fragment->BindThreadRange(thread_range));
+      ICHECK(false) << "Not Implemented";
+    } else {
+      ICHECK(0);
+    }
+    if (B.scope() == "shared" || B.scope() == "shared.dyn") {
+      int dim_B = B->shape.size();
+      const int64_t mat_stride = *as_const_int(B->shape[dim_B - 2]);
+      const int64_t mat_continuous = *as_const_int(B->shape[dim_B - 1]);
+      results.Set(B, makeGemmSparseAmpereABLayout(mat_stride, mat_continuous,
+                                                  B->dtype.bits()));
+    } else if (B.scope() == "local.fragment") {
+      // auto fragment =
+      //     makeGemmFragmentB(M, N, K, M / warp_m, N / warp_n, trans_B);
+      // results.Set(B, fragment->BindThreadRange(thread_range));
+      ICHECK(false) << "Not Implemented";
+    } else {
+      ICHECK(0);
+    }
   } else {
-    ICHECK(0) << "Not supported " << T.target->str()
-              << " Currently only Hopper are supported";
+    ICHECK(0) << "Architecture is not supported: " << T.target->str();
   }
   completed_ = true;
   return results;
 }
+
 TIR_REGISTER_TL_OP(GemmSP, gemm_sp)
     .set_num_inputs(5)
     .set_attr<TCallEffectKind>("TCallEffectKind",
                                Integer(CallEffectKind::kOpaque));
 
+TVM_FFI_STATIC_INIT_BLOCK({ GemmSPNode::RegisterReflection(); });
+
 } // namespace tl
 } // namespace tvm
diff --git a/src/op/gemm_sp.h b/src/op/gemm_sp.h
index dbb62b69..eee7cd79 100644
--- a/src/op/gemm_sp.h
+++ b/src/op/gemm_sp.h
@@ -7,31 +7,50 @@
 #ifndef TVM_TL_OP_GEMM_SP_H_
 #define TVM_TL_OP_GEMM_SP_H_
 
-#include "op.h"
+#include "gemm.h"
+#include "operator.h"
 
 namespace tvm {
+
 namespace tl {
 
 using namespace tir;
 
-class GemmSP : public Operator {
+class GemmSPWarpPolicyNode : public GemmWarpPolicyNode {
 public:
-  GemmSP(Array<PrimExpr> args, BufferMap vmap);
-  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const final;
-  LayoutMap InferLayout(const LayoutInferArgs &T, InferLevel level) final;
-  static const Op &Get();
-  enum class GemmWarpPolicy {
-    kSquare = 0,
-    kFullRow = 1,
-    kFullCol = 2,
-  } policy;
+  std::pair<int, int> ComputeWarpPartition(int M, int N, int block_size,
+                                           Target target, bool use_wgmma,
+                                           int bits) const;
+};
 
-private:
-  std::pair<int, int>
-  ComputeWarpPartition(int num_warps, Target target,
-                       bool maybe_hopper_wgmma = true) const;
+class GemmSPWarpPolicy : public ObjectRef {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(GemmSPWarpPolicy, ObjectRef,
+                                GemmSPWarpPolicyNode);
+
+  explicit GemmSPWarpPolicy(GemmWarpPolicyType policy_type) {
+    auto node = make_object<GemmSPWarpPolicyNode>();
+    node->policy_type = (int)policy_type;
+    data_ = std::move(node);
+  }
 
-  Array<PrimExpr> call_args;
+  explicit GemmSPWarpPolicy(int policy_type) {
+    auto node = make_object<GemmSPWarpPolicyNode>();
+    node->policy_type = policy_type;
+    data_ = std::move(node);
+  }
+
+  explicit GemmSPWarpPolicy(int m_warp, int n_warp) {
+    auto node = make_object<GemmSPWarpPolicyNode>();
+    node->m_warp = m_warp;
+    node->n_warp = n_warp;
+    node->policy_type = (int)GemmWarpPolicyType::kFree;
+    data_ = std::move(node);
+  }
+};
+
+class GemmSPNode : public TileOperatorNode {
+public:
   tir::Buffer A, B, C, E;
   bool trans_A, trans_B;
   int M, N, K;
@@ -40,7 +59,69 @@ private:
   // only will be enabled under cdna mfma instructions
   int kPack = 1;
   int wg_wait = 0;
-  bool completed_ = false;
+
+  mutable GemmSPWarpPolicy policy;
+
+  static constexpr const char *_type_key = "tl.GemmSP";
+  TVM_DECLARE_FINAL_OBJECT_INFO(GemmSPNode, TileOperatorNode);
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+
+  TileOperator Clone() const;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<GemmSPNode>()
+        .def_ro("policy", &GemmSPNode::policy)
+        .def_ro("A", &GemmSPNode::A)
+        .def_ro("B", &GemmSPNode::B)
+        .def_ro("C", &GemmSPNode::C)
+        .def_ro("E", &GemmSPNode::E)
+        .def_ro("trans_A", &GemmSPNode::trans_A)
+        .def_ro("trans_B", &GemmSPNode::trans_B)
+        .def_ro("M", &GemmSPNode::M)
+        .def_ro("N", &GemmSPNode::N)
+        .def_ro("K", &GemmSPNode::K)
+        .def_ro("clear_accum", &GemmSPNode::clear_accum)
+        .def_ro("kPack", &GemmSPNode::kPack)
+        .def_ro("wg_wait", &GemmSPNode::wg_wait);
+  }
+
+  bool SEqualReduce(const GemmSPNode *other, SEqualReducer equal) const {
+    return equal(A, other->A) && equal(B, other->B) && equal(C, other->C) &&
+           equal(E, other->E) && equal(trans_A, other->trans_A) &&
+           equal(trans_B, other->trans_B) && equal(M, other->M) &&
+           equal(N, other->N) && equal(K, other->K) &&
+           equal(clear_accum, other->clear_accum) &&
+           equal(kPack, other->kPack) && equal(wg_wait, other->wg_wait);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(policy);
+    hash_reduce(A);
+    hash_reduce(B);
+    hash_reduce(C);
+    hash_reduce(E);
+    hash_reduce(trans_A);
+    hash_reduce(trans_B);
+    hash_reduce(M);
+    hash_reduce(N);
+    hash_reduce(K);
+    hash_reduce(clear_accum);
+    hash_reduce(kPack);
+    hash_reduce(wg_wait);
+  }
+
+private:
+  mutable bool completed_ = false;
+};
+
+class GemmSP : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(GemmSP, TileOperator, GemmSPNode);
+  TVM_DLL GemmSP(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
 };
 
 } // namespace tl
diff --git a/src/op/logical.cc b/src/op/logical.cc
index 49afd8a8..0398c38c 100644
--- a/src/op/logical.cc
+++ b/src/op/logical.cc
@@ -4,7 +4,7 @@
  *
  */
 
-#include <tvm/runtime/registry.h>
+#include <tvm/ffi/function.h>
 #include <tvm/tir/builtin.h>
 #include <tvm/tir/op.h>
 #include <tvm/tir/op_attr_types.h>
diff --git a/src/op/math.cc b/src/op/math.cc
index 1a10f8c2..57239987 100644
--- a/src/op/math.cc
+++ b/src/op/math.cc
@@ -4,7 +4,7 @@
  *
  */
 
-#include <tvm/runtime/registry.h>
+#include <tvm/ffi/function.h>
 #include <tvm/tir/builtin.h>
 #include <tvm/tir/op.h>
 #include <tvm/tir/op_attr_types.h>
diff --git a/src/op/op.cc b/src/op/op.cc
deleted file mode 100644
index 14594962..00000000
--- a/src/op/op.cc
+++ /dev/null
@@ -1,92 +0,0 @@
-/*!
- * \file tl/op/op.cc
- *
- * Define operators usd in tile library.
- */
-
-#include "op.h"
-
-#include <tvm/tir/builtin.h>
-#include <tvm/tir/op.h>
-#include <tvm/tir/op_attr_types.h>
-
-namespace tvm {
-namespace tl {
-
-using namespace tir;
-
-TIR_REGISTER_TL_OP(RegionOp, region)
-    .set_num_inputs(-1)
-    .set_attr<TCallEffectKind>("TCallEffectKind",
-                               Integer(CallEffectKind::kPure));
-
-std::unique_ptr<Operator> ParseOperator(Call call, BufferMap vmap) {
-  auto op_map = Op::GetAttrMap<OpBuilderFunc>("TLOpBuilder");
-  Op op = call->op.as<Op>().value();
-  if (op_map.count(op)) {
-    Operator *ptr = static_cast<Operator *>(op_map[op](call->args, vmap));
-    ICHECK(ptr != nullptr);
-    return std::unique_ptr<Operator>(ptr);
-  }
-  return nullptr;
-}
-
-std::unique_ptr<Operator> ParseOperator(Stmt stmt, BufferMap vmap) {
-  if (stmt.as<Evaluate>() && stmt.as<EvaluateNode>()->value.as<CallNode>()) {
-    auto call = stmt.as<EvaluateNode>()->value.as<CallNode>();
-    return ParseOperator(GetRef<Call>(call), vmap);
-  }
-  return nullptr;
-}
-
-Var GetVarFromAccessPtr(const PrimExpr &expr) {
-  auto call = expr.as<CallNode>();
-  ICHECK(call);
-  ICHECK(call->op.same_as(builtin::tvm_access_ptr()));
-  auto var = call->args[1].as<VarNode>();
-  ICHECK(var);
-  return GetRef<Var>(var);
-}
-
-RegionOp::RegionOp(Array<PrimExpr> args, BufferMap vmap) {
-  size_t n = args.size();
-  size_t ndim = n - 2;
-  auto load = args[0].as<BufferLoadNode>();
-  ICHECK(load);
-  ICHECK(load->indices.size() == ndim)
-      << "load->indices.size() = " << load->indices << " ndim = " << ndim;
-  buffer_ = load->buffer;
-  access_mask_ = static_cast<int>(*as_const_int(args[1]));
-  for (size_t i = 0; i < ndim; i++) {
-    PrimExpr min = load->indices[i];
-    PrimExpr extent = args[2 + i];
-    ranges_.push_back(Range::FromMinExtent(min, extent));
-  }
-}
-
-bool RegionOp::IsFullRegion() const {
-  for (size_t i = 0; i < ranges_.size(); i++) {
-    if (!is_zero(ranges_[i]->min))
-      return false;
-    if (!StructuralEqual()(ranges_[i]->extent, buffer_->shape[i]))
-      return false;
-  }
-  return true;
-}
-
-Stmt Operator::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
-  ICHECK(0) << "Not Implemented Lower method.";
-  return Evaluate(0);
-}
-
-Stmt Operator::Canonialize(const CanonializeArgs &T,
-                           arith::Analyzer *analyzer) const {
-  return {};
-}
-
-LayoutMap Operator::InferLayout(const LayoutInferArgs &T, InferLevel level) {
-  return {};
-}
-
-} // namespace tl
-} // namespace tvm
diff --git a/src/op/operator.cc b/src/op/operator.cc
new file mode 100644
index 00000000..aa589460
--- /dev/null
+++ b/src/op/operator.cc
@@ -0,0 +1,84 @@
+/*!
+ * \file tl/op/op.cc
+ *
+ * Define operators usd in tile library.
+ */
+
+#include "operator.h"
+
+#include <tvm/tir/builtin.h>
+#include <tvm/tir/op_attr_types.h>
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+/**
+ * @brief Construct a TileOperator from a TIR Call using a registered builder.
+ *
+ * Looks up a builder function in the "TLOpBuilder" Op attribute map for the
+ * operator referenced by `call` and invokes it to produce a TileOperator. If no
+ * builder is registered for the operator, returns a default-constructed (empty)
+ * TileOperator.
+ *
+ * @param call The TIR Call whose operator and arguments will be used to build
+ * the TileOperator.
+ * @param vmap Buffer mapping passed through to the builder to resolve buffer
+ * references.
+ * @return TileOperator The constructed TileOperator, or a default (empty)
+ * TileOperator if no builder exists.
+ */
+TileOperator ParseOperator(Call call, BufferMap vmap) {
+  auto op_map = Op::GetAttrMap<OpBuilderFunc>("TLOpBuilder");
+  Op op = call->op.as<Op>().value();
+  if (op_map.count(op)) {
+    auto tile_op = op_map[op](call->args, vmap);
+    ICHECK(tile_op.defined());
+    return tile_op;
+  }
+  return TileOperator();
+}
+
+/**
+ * @brief Parse a TileOperator from a TIR statement if it contains a call.
+ *
+ * If `stmt` is an Evaluate node whose value is a Call, delegates to
+ * ParseOperator(Call, BufferMap) and returns the resulting TileOperator.
+ * Otherwise returns a default-constructed (empty) TileOperator.
+ *
+ * @param stmt TIR statement to inspect; expected to be an Evaluate of a Call.
+ * @param vmap Mapping of buffer variables used when building the operator.
+ * @return TileOperator Parsed operator on success, or a default (empty)
+ * TileOperator if `stmt` is not an Evaluate(Call).
+ */
+TileOperator ParseOperator(Stmt stmt, BufferMap vmap) {
+  if (stmt.as<Evaluate>() && stmt.as<EvaluateNode>()->value.as<CallNode>()) {
+    auto call = stmt.as<EvaluateNode>()->value.as<CallNode>();
+    return ParseOperator(GetRef<Call>(call), vmap);
+  }
+  return TileOperator();
+}
+
+/**
+ * @brief Extracts the Var referenced by a `tvm_access_ptr` call expression.
+ *
+ * The function expects `expr` to be a `Call` to the builtin `tvm_access_ptr`
+ * and returns the `Var` found in the call's second argument (`args[1]`). The
+ * function performs runtime checks and will abort if `expr` is not a call, the
+ * call is not `tvm_access_ptr`, or the second argument is not a `Var`.
+ *
+ * @param expr A `PrimExpr` representing a `tvm_access_ptr(...)` call.
+ * @return tvm::Var The `Var` referenced by the `tvm_access_ptr` call.
+ */
+Var GetVarFromAccessPtr(const PrimExpr &expr) {
+  auto call = expr.as<CallNode>();
+  ICHECK(call);
+  ICHECK(call->op.same_as(builtin::tvm_access_ptr()));
+  auto var = call->args[1].as<VarNode>();
+  ICHECK(var);
+  return GetRef<Var>(var);
+}
+
+} // namespace tl
+} // namespace tvm
diff --git a/src/op/op.h b/src/op/operator.h
similarity index 60%
rename from src/op/op.h
rename to src/op/operator.h
index 5b230ccf..5c1b223a 100644
--- a/src/op/op.h
+++ b/src/op/operator.h
@@ -11,6 +11,9 @@
 #include <tvm/ir/op.h>
 #include <tvm/target/target.h>
 #include <tvm/tir/buffer.h>
+#include <tvm/tir/op.h>
+#include <tvm/tir/op_attr_types.h>
+#include <tvm/tir/stmt.h>
 
 #include "../layout/layout.h"
 
@@ -22,21 +25,8 @@ using namespace tir;
 using AddWorkspaceCallback = std::function<PrimExpr(int, DataType)>;
 using LayoutMap = Map<Buffer, Layout>;
 using BufferMap = Map<Var, Buffer>;
-using OpBuilderFunc = TypedPackedFunc<void *(Array<PrimExpr>, BufferMap)>;
 
-#define TIR_REGISTER_TL_OP(Entry, OpName)                                      \
-  const Op &Entry::Get() {                                                     \
-    static const Op &op = Op::Get("tl." #OpName);                              \
-    return op;                                                                 \
-  }                                                                            \
-  TVM_REGISTER_OP("tl." #OpName)                                               \
-      .set_attr<TScriptPrinterName>("TScriptPrinterName", #OpName)             \
-      .set_attr<OpBuilderFunc>("TLOpBuilder",                                  \
-                               [](Array<PrimExpr> a, BufferMap b) {            \
-                                 return (void *)(new Entry(a, b));             \
-                               })
-
-enum class InferLevel {
+enum class InferLevel : uint8_t {
   kFree = 0,
   kCommon = 1,
   kStrict = 2,
@@ -49,49 +39,58 @@ struct LowerArgs {
   AddWorkspaceCallback AddWorkspace;
   LayoutMap layout_map;
   Map<Buffer, Buffer> buffer_remap;
-  bool disable_tma_lower;
+  Array<Var> buffer_var_gemm;
 };
 
 struct LayoutInferArgs {
   Target target;
   Range thread_bounds;
   LayoutMap layout_map;
+  arith::Analyzer *analyzer;
+  bool buffer_oob = false;
   Map<Buffer, Buffer> buffer_remap;
 };
 
-struct CanonializeArgs {
-  Target target;
-};
+class TileOperator;
 
-class Operator {
+class TileOperatorNode : public Object {
 public:
-  virtual Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const;
-  virtual Stmt Canonialize(const CanonializeArgs &T,
-                           arith::Analyzer *analyzer) const;
-  virtual LayoutMap InferLayout(const LayoutInferArgs &T, InferLevel level);
-  virtual ~Operator() = default;
+  virtual Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const = 0;
+
+  virtual LayoutMap InferLayout(const LayoutInferArgs &T,
+                                InferLevel level) const = 0;
+
+  virtual TileOperator Clone() const = 0;
+
+  static constexpr const char *_type_key = "tl.TileOperator";
+
+  TVM_DECLARE_BASE_OBJECT_INFO(TileOperatorNode, Object);
 };
 
-class RegionOp : public Operator {
+class TileOperator : public ObjectRef {
 public:
-  RegionOp(Array<PrimExpr> args, BufferMap vmap);
-  static const Op &Get();
-
-  const Buffer &GetBuffer() const { return buffer_; }
-  const Array<Range> &GetRanges() const { return ranges_; }
-  int GetAccessMask() const { return access_mask_; }
-  bool IsFullRegion() const;
-
-private:
-  Buffer buffer_;
-  Array<Range> ranges_;
-  int access_mask_;
+  TVM_DEFINE_OBJECT_REF_METHODS(TileOperator, ObjectRef, TileOperatorNode);
 };
 
 Var GetVarFromAccessPtr(const PrimExpr &expr);
 
-std::unique_ptr<Operator> ParseOperator(Call call, BufferMap vmap);
-std::unique_ptr<Operator> ParseOperator(Stmt stmt, BufferMap vmap);
+TileOperator ParseOperator(Call call, BufferMap vmap);
+TileOperator ParseOperator(Stmt stmt, BufferMap vmap);
+
+using OpBuilderFunc =
+    ffi::TypedFunction<TileOperator(Array<PrimExpr>, BufferMap)>;
+
+#define TIR_REGISTER_TL_OP(Entry, OpName)                                      \
+  const Op &Entry::Get() {                                                     \
+    static const Op &op = Op::Get("tl." #OpName);                              \
+    return op;                                                                 \
+  }                                                                            \
+  TVM_REGISTER_OP("tl." #OpName)                                               \
+      .set_attr<TScriptPrinterName>("TScriptPrinterName", #OpName)             \
+      .set_attr<OpBuilderFunc>("TLOpBuilder",                                  \
+                               [](Array<PrimExpr> args, BufferMap vmap) {      \
+                                 return Entry(args, vmap);                     \
+                               })
 
 } // namespace tl
 } // namespace tvm
diff --git a/src/op/parallel.cc b/src/op/parallel.cc
index a2266cb8..c0ef00cc 100644
--- a/src/op/parallel.cc
+++ b/src/op/parallel.cc
@@ -5,6 +5,7 @@
 
 #include "parallel.h"
 
+#include <algorithm>
 #include <tvm/tir/op.h>
 
 #include "../layout/utils.h"
@@ -22,6 +23,64 @@ namespace attr {
 constexpr const char *coalesced_width = "coalesced_width";
 } // namespace attr
 
+// ProveFragmentContains checks whether the threads that access elements of a
+// smaller fragment (small_frag) are a subset of the threads that access
+// elements of a larger fragment (large_frag) for any given loop index. This
+// function ensures that if the small fragment's layout corresponds to the loop
+// itself, accessing the large fragment's elements is valid. Additionally, if
+// small is updated to large, the originally valid access remains valid. The
+// proof is performed by:
+//
+// 1. Defining a variable `rep_small` to represent the replicate index of the
+//    small fragment that is being checked.
+// 2. Using the `small_frag_indices` and `rep_small` to derive the thread
+// accessing
+//    the element in the small fragment.
+// 3. Using `large_frag_indices` to derive the physical index of the large
+// fragment
+//    along with the thread information, and then feeding these into the inverse
+//    of the large fragment to obtain the logical index and replicate index.
+// 4. Verifying the mapping by checking whether the computed thread using the
+// inverse
+//    layout corresponds to the original thread calculated for the small
+//    fragment. If they don't match, this indicates that the inverse layout's
+//    domain does not include the thread and thus the access is invalid.
+bool ProveFragmentContains(Fragment small_frag, Fragment large_frag,
+                           Array<PrimExpr> small_frag_indices,
+                           Array<PrimExpr> large_frag_indices,
+                           arith::Analyzer &analyzer_) {
+  Var rep_small("__checking_frag_contains_rep");
+  analyzer_.Bind(rep_small,
+                 Range(IntImm(small_frag->ReplicateExtent()->dtype, 0),
+                       small_frag->ReplicateExtent()),
+                 true); // Bind the replicate extent of small_frag.
+  // Derive thread for small_frag.
+  auto thread = small_frag->ForwardThread(small_frag_indices, rep_small);
+
+  // Get physical index and thread for large_frag.
+  auto large_frag_physical_and_thread = large_frag->Forward(large_frag_indices);
+  // Add small_frag's thread to the large fragment's thread info.
+  large_frag_physical_and_thread.push_back(thread);
+  // Get the inverse of the large fragment.
+  auto inv_large_frag = large_frag->Inverse();
+  // Compute logical index and replicate index using inverse layout.
+  auto inv_large_frag_logical_and_rep =
+      inv_large_frag->Forward(large_frag_physical_and_thread);
+
+  // Extract replicate index from the result.
+  auto inv_large_frag_rep =
+      inv_large_frag_logical_and_rep[inv_large_frag_logical_and_rep.size() - 1];
+
+  // Calculate thread based on the logical index and replicate index.
+  auto check_thread =
+      large_frag->ForwardThread(large_frag_indices, inv_large_frag_rep);
+
+  // Simplify the difference between the threads.
+  auto diff = analyzer_.Simplify(thread - check_thread);
+  // If the difference is zero, the threads match and the access is valid.
+  return is_zero(diff);
+}
+
 class IfBufferRemapLoopGenerator : public StmtExprMutator {
 public:
   static For run(Stmt stmt, Map<Buffer, Buffer> buffer_remap,
@@ -61,11 +120,29 @@ private:
   Map<Buffer, Layout> layout_map_;
 };
 
+/**
+ * @brief Handle a parallel For node during traversal, collecting loop metadata.
+ *
+ * Visits a parallel loop, asserts the loop is parallel, records a data-parallel
+ * IterVar for the loop, binds the loop variable range into the analyzer scope,
+ * and extracts any reducer information from the loop's annotations into the
+ * visitor's reducer_info_map_. Continues traversal into the loop body.
+ */
 void ParallelLoopNestVisitor::VisitStmt_(const ForNode *op) {
-  ICHECK(op->kind == ForKind::kParallel);
-  p->loop_vars_.push_back(
-      IterVar(Range(op->min, op->extent), op->loop_var, IterVarType::kDataPar));
+  if (op->kind == ForKind::kParallel)
+    p->loop_vars_.push_back(IterVar(Range(op->min, op->extent), op->loop_var,
+                                    IterVarType::kDataPar));
+  else
+    p->inner_vars_.Set(op->loop_var,
+                       IterVar(Range(op->min, op->extent), op->loop_var,
+                               IterVarType::kOrdered));
   p->analyzer_.Bind(op->loop_var, Range::FromMinExtent(op->min, op->extent));
+  auto reducer_info_map =
+      op->annotations.Get(attr::kReducerInfo)->as<Map<Var, ReducerInfo>>();
+  if (reducer_info_map) {
+    for (auto &&[buffer, info] : reducer_info_map.value())
+      p->reducer_info_map_.Set(buffer, info);
+  }
   StmtExprVisitor::VisitStmt_(op);
 }
 
@@ -96,9 +173,21 @@ void ParallelLoopNestVisitor::VisitExpr_(const BufferLoadNode *op) {
   StmtExprVisitor::VisitExpr_(op);
 }
 
-ParallelOp::ParallelOp(For root) : root_(root), V(this) { V.VisitStmt(root); }
+ParallelOpNode::ParallelOpNode(For root) : root_(root), V(this) {
+  V.VisitStmt(root);
+}
 
-bool ParallelOp::IsCommonAccessIndice(const Buffer &buffer) const {
+TileOperator ParallelOpNode::Clone() const {
+  auto op = make_object<ParallelOpNode>(*this);
+  return ParallelOp(op);
+}
+
+Stmt ParallelOpNode::Lower(const LowerArgs &T,
+                           arith::Analyzer *analyzer) const {
+  return root_;
+}
+
+bool ParallelOpNode::IsCommonAccessIndice(const Buffer &buffer) const {
   auto common_indice = loop_vars_.Map([](const auto &iv) { return iv->var; });
   return StructuralEqual()(indice_map_[buffer], common_indice);
 }
@@ -121,26 +210,134 @@ bool ParallelOp::IsCommonAccessIndice(const Buffer &buffer) const {
  *                Can generate new layouts based on vectorization and thread
  * bounds. Used when maximum performance optimization is desired.
  */
-LayoutMap ParallelOp::InferLayout(const LayoutInferArgs &T, InferLevel level) {
+LayoutMap ParallelOpNode::InferLayout(const LayoutInferArgs &T,
+                                      InferLevel level) const {
   if (loop_layout_.defined())
     return {};
-  if (level == InferLevel::kStrict)
-    return {};
+  if (level == InferLevel::kStrict) {
+    LayoutMap results;
+    // Deduce buffers that should be complicated replicated.
+    // For example:
+    // for i in T.Parallel(m):
+    //   fragment[0] = x[i]
+    // then fragment[0] must be replicated on all threads.
+    for (const auto &[buffer, indices] : indice_map_) {
+      if (T.layout_map.count(buffer)) {
+        continue;
+      }
+      if (buffer.scope() != "local.fragment")
+        continue;
+
+      // Check if all indices are zero
+      bool all_indices_zero = true;
+      for (const auto &index : indices) {
+        if (const auto *imm = index.as<IntImmNode>()) {
+          if (imm->value != 0) {
+            all_indices_zero = false;
+            LOG(FATAL)
+                << "Fragment buffer access with non-zero index [" << imm->value
+                << "] is not supported. "
+                << "Only fragment[0] access is allowed within T.Parallel loop.";
+          }
+        } else {
+          // Non-constant index, not all zero
+          all_indices_zero = false;
+        }
+      }
+
+      // Only set layout if all indices are zero
+      if (all_indices_zero) {
+        Array<IterVar> forward_vars;
+        for (const auto &s : buffer->shape) {
+          forward_vars.push_back(
+              IterVar(Range(0, s), Var(), IterVarType::kDataPar));
+        }
+        Array<PrimExpr> forward_index;
+        for (const auto &iv : forward_vars) {
+          forward_index.push_back(iv->var);
+        }
+        Var rep;
+        auto rep_iter =
+            IterVar({0, T.thread_bounds->extent}, rep, IterVarType::kDataPar);
+
+        const PrimExpr &forward_thread = rep;
+        results.Set(buffer, Fragment(forward_vars, forward_index,
+                                     forward_thread, rep_iter));
+      }
+    }
+    return results;
+  }
+  auto buffer_is_completed_replicated = [&](const Buffer &buffer) {
+    if (buffer.scope() != "local.fragment")
+      return false;
+    auto frag = T.layout_map[buffer].as<Fragment>().value();
+    // buffer indices should be IntImm
+    for (const auto &index : indice_map_[buffer]) {
+      if (!index.as<IntImmNode>()) {
+        return false;
+      } else if (index.as<IntImmNode>()->value != 0) {
+        LOG(FATAL) << "buffer " << buffer << " is not completed replicated";
+      }
+    }
+    return frag->IsCompletedReplicated();
+  };
+  // Collect fragment buffers with const index and all fragment_buffers
+  std::vector<Buffer> const_index_fragment_buffer, fragment_buffers;
+  for (const auto &[buffer, indices] : indice_map_) {
+    if (buffer.scope() != "local.fragment")
+      continue;
+    fragment_buffers.push_back(buffer);
+
+    bool is_const_index = true;
+    for (const auto &index : indices) {
+      if (!index.as<IntImmNode>()) {
+        is_const_index = false;
+        break;
+      }
+    }
+    if (is_const_index) {
+      const_index_fragment_buffer.push_back(buffer);
+    }
+  }
+
+  // Determine if common layout propagation should be applied.
+  // If there are fragment buffers with non-constant indices, we need to
+  // propagate the common layout pattern to ensure consistency across all
+  // fragments. Example cases:
+  //   - Need propagation: frag_a[0] = T.min(frag_a[0], frag_b[i])
+  //     (const index frag_a interacts with non-const index frag_b)
+  //   - No propagation needed: shared_a[i] = frag_a[0]
+  //     (const index frag_a with non-fragment buffer)
+
+  bool allow_layout_propgate =
+      const_index_fragment_buffer.empty() ||
+      (fragment_buffers.size() > const_index_fragment_buffer.size());
 
   // Step 1: try to infer loop's partition from a source fragment
   Buffer source_buffer, read_source_buffer;
+  Buffer replicated_write_buffer; // Backup: fully replicated write buffer
+
   for (const auto &[buffer, indices] : indice_map_) {
     if (T.layout_map.count(buffer)) {
+      // skip reducers with rep=ALL
+      if (auto info = reducer_info_map_.Get(buffer->data);
+          info && info.value()->rep == ReducerRepType::ALL)
+        continue;
+
       auto frag = T.layout_map[buffer].as<Fragment>().value();
+      bool is_fully_replicated = buffer_is_completed_replicated(buffer);
+
       if (buffer_is_write_.count(buffer)) {
         source_buffer = buffer;
       } else {
         // Keep the buffer with largest number of indices
         // (which means the inference based on that buffer is more accurate)
         // as read_source_buffer to get more accurate layout
-        if (!read_source_buffer.defined() ||
-            indice_map_[buffer].size() >
-                indice_map_[read_source_buffer].size()) {
+        // if the buffer is completed replicated, we don't need to infer the
+        // layout from this buffer.
+        if ((!read_source_buffer.defined() ||
+             indice_map_[buffer].size() >
+                 indice_map_[read_source_buffer].size())) {
           read_source_buffer = buffer;
         }
         // If the buffer is not replicated and shape is equal to the
@@ -154,83 +351,129 @@ LayoutMap ParallelOp::InferLayout(const LayoutInferArgs &T, InferLevel level) {
   }
   auto compute_loop_layout_from_buffer = [&](const Buffer &buffer) {
     Fragment src_layout = T.layout_map[buffer].as<Fragment>().value();
+    DLOG(INFO) << "[compute_loop_layout_from_buffer] infer from buffer `"
+               << buffer << "` of layout " << src_layout->DebugOutput() << '\n';
+
+    Fragment result;
     if (IsCommonAccessIndice(buffer)) {
-      return src_layout;
+      result = src_layout;
     } else {
       Var rep;
       auto rep_iter = IterVar({0, src_layout->ReplicateExtent()}, rep,
                               IterVarType::kDataPar);
       PrimExpr loop_var_to_thread =
           src_layout->ForwardThread(indice_map_[buffer], rep);
-      return Fragment(loop_vars_, {}, loop_var_to_thread, rep_iter)
-          ->BindThreadRange(T.thread_bounds);
+      loop_var_to_thread = analyzer_.Simplify(loop_var_to_thread);
+      PostOrderVisit(loop_var_to_thread, [&](const ObjectRef &objref) {
+        if (auto opt_var = objref.as<Var>();
+            opt_var && inner_vars_.count(*opt_var)) {
+          std::ostringstream oss;
+          oss << "loop_var_to_thread = " << loop_var_to_thread
+              << "contains inner var" << *opt_var;
+          throw LayoutConflictException(oss.str());
+        }
+      });
+
+      try {
+        result = Fragment(loop_vars_, {}, loop_var_to_thread, rep_iter)
+                     ->BindThreadRange(T.thread_bounds);
+      } catch (const tvm::runtime::Error &err) {
+        std::ostringstream msg;
+        msg << "Layout inference for buffer `" << buffer->name
+            << "` failed inside `T.parallel` loop.";
+
+        msg << "\nUnderlying TVM error: " << err.what();
+        msg << "\nProblematic loop AST:\n " << root_;
+        msg << "\nHint: ensure the loop extent divides the thread binding or "
+               "adjust the fragment mapping.";
+        LOG(FATAL) << msg.str();
+      }
     }
+    DLOG(INFO) << "[compute_loop_layout_from_buffer] ... and get "
+               << result->DebugOutput() << '\n';
+    return result;
   };
-  if (source_buffer.defined()) {
+
+  // Try to infer loop layout from buffers in order of preference:
+  // 1. Non-replicated write buffer (most reliable)
+  // 2. Non-replicated read buffer
+  // 3. Fully replicated write buffer (backup, may cause issues)
+  // 4. Free inference mode (no source buffer)
+
+  if (source_buffer.defined() && allow_layout_propgate) {
     loop_layout_ = compute_loop_layout_from_buffer(source_buffer);
   } else if (level == InferLevel::kFree) {
-    if (read_source_buffer.defined()) {
-      loop_layout_ = compute_loop_layout_from_buffer(read_source_buffer);
-      // // Loop don't need to be replicated.
-      // if (!is_one(loop_layout_->ReplicateExtent()))
-      //   loop_layout_ = loop_layout_->DeReplicate();
-
-      // For free layout inference
-      // If replication exists and buffer has cross-thread shared memory access,
-      // add predicate
-      bool has_cross_thread_access = false;
-      PostOrderVisit(root_, [&](const ObjectRef &obj) {
-        if (const auto *store = obj.as<BufferStoreNode>()) {
-          // check if scope is shared or global
-          if (store->buffer.scope() == "shared" ||
-              store->buffer.scope() == "shared.dyn" ||
-              store->buffer.scope() == "global") {
-            has_cross_thread_access = true;
-          }
-        } else if (const auto *load = obj.as<BufferLoadNode>()) {
-          // check if scope is shared or global
-          if (load->buffer.scope() == "shared" ||
-              load->buffer.scope() == "shared.dyn" ||
-              load->buffer.scope() == "global") {
-            has_cross_thread_access = true;
-          }
+    // For free layout inference
+    // If replication exists and buffer has cross-thread shared memory access,
+    // add predicate
+    bool has_cross_thread_access = false;
+    PostOrderVisit(root_, [&](const ObjectRef &obj) {
+      if (const auto *store = obj.as<BufferStoreNode>()) {
+        // check if scope is shared or global
+        if (store->buffer.scope() == "shared" ||
+            store->buffer.scope() == "shared.dyn" ||
+            store->buffer.scope() == "global") {
+          has_cross_thread_access = true;
         }
-      });
-
-      // check if loop body contains a "pure" buffer store (i.e., direct
-      // assignment, not compound update)
-      bool has_pure_buffer_store = false;
-      PostOrderVisit(root_, [&](const ObjectRef &obj) {
-        if (const auto *store = obj.as<BufferStoreNode>()) {
-          // Check if the value is a direct load from another buffer (i.e., b[i]
-          // = a[i])
-          if (const auto *load = store->value.as<BufferLoadNode>()) {
-            has_pure_buffer_store = true;
-          }
+      } else if (const auto *load = obj.as<BufferLoadNode>()) {
+        // check if scope is shared or global
+        if (load->buffer.scope() == "shared" ||
+            load->buffer.scope() == "shared.dyn" ||
+            load->buffer.scope() == "global") {
+          has_cross_thread_access = true;
         }
-      });
-
-      if (!is_one(loop_layout_->ReplicateExtent()) && has_cross_thread_access &&
-          !has_pure_buffer_store) {
-        auto inv = loop_layout_->Inverse();
-        Array<PrimExpr> fwd;
-        for (size_t i = 0; i < loop_layout_->OutputDim(); i++)
-          fwd.push_back(0);
-        fwd.push_back(InputPlaceholder(0) - T.thread_bounds->min);
-        auto rep = inv->Forward(fwd).back();
-        AddPredicate(EQ(rep, 0));
       }
-    } else {
+    });
+
+    // check if loop body contains a "pure" buffer store (i.e., direct
+    // assignment, not compound update)
+    std::vector<Buffer> store_shared_global_buffers, store_fragment_buffers;
+    // Buffers that scope is above fragments.
+    // global, shared, shared.dyn
+    // which can be used to analysis replicate case
+    PostOrderVisit(root_, [&](const ObjectRef &obj) {
+      if (const auto *store = obj.as<BufferStoreNode>()) {
+        auto buffer = store->buffer;
+        if (buffer.scope() == "shared" || buffer.scope() == "shared.dyn" ||
+            buffer.scope() == "global") {
+          store_shared_global_buffers.emplace_back(buffer);
+        } else if (buffer.scope() == "local.fragment") {
+          store_fragment_buffers.emplace_back(buffer);
+        }
+      }
+    });
+    if (read_source_buffer.defined() && allow_layout_propgate) {
+      loop_layout_ = compute_loop_layout_from_buffer(read_source_buffer);
+    }
+
+    if (!loop_layout_.defined()) {
+      // No source buffer available, use free mode inference
       // Vectorize Size must be aware of the buffer_remap
       // As the pass will do post processing to the layout
       auto maybe_remapped_root_ =
           IfBufferRemapLoopGenerator::run(root_, T.buffer_remap, T.layout_map);
       int vector_size = GetVectorizeSize(maybe_remapped_root_);
 
+      DLOG(INFO) << "[PlanLoopPartition] vector_size = " << vector_size << '\n';
+
+      PrimExpr loop_total_size = 1;
+      for (Stmt l = root_; l.as<For>().has_value();
+           l = l.as<For>().value()->body)
+        loop_total_size = loop_total_size * l.as<For>().value()->extent;
+      DLOG(INFO) << "[PlanLoopPartition] loop_total_size = " << loop_total_size
+                 << '\n';
+      while (!analyzer_.CanProve(
+                 floormod(loop_total_size,
+                          T.thread_bounds->extent * vector_size) == 0) &&
+             vector_size > 1)
+        vector_size /= 2;
+      DLOG(INFO) << "[PlanLoopPartition] after adjust: vector_size = "
+                 << vector_size << '\n';
+
       // Check if coalesced_width is defined
       if (auto coalesced_width =
               root_->annotations.Get(tl::attr::coalesced_width)) {
-        if (const auto *imm = coalesced_width.as<IntImmNode>()) {
+        if (const auto *imm = coalesced_width->as<IntImmNode>()) {
           int expected = imm->value;
           // Verify that vector_size is divisible by expected
           if (vector_size % expected != 0) {
@@ -242,8 +485,80 @@ LayoutMap ParallelOp::InferLayout(const LayoutInferArgs &T, InferLevel level) {
           LOG(FATAL) << "coalesced_width should be an IntImmNode.";
         }
       }
+      DLOG(INFO) << "[PlanLoopPartition] root_ = " << root_
+                 << " ############# vector_size = " << vector_size
+                 << ", thread_bounds = " << T.thread_bounds << '\n';
       loop_layout_ = PlanLoopPartition(root_, vector_size, T.thread_bounds);
+      DLOG(INFO) << "[PlanLoopPartition] loop_layout_ = "
+                 << loop_layout_->DebugOutput() << '\n';
     }
+
+    // Lambda that guards replicated accesses:
+    // - When a loop layout replicates a fragment buffer (rep > 1), each thread
+    //   observes the same fragment elements. Blindly storing to shared/global
+    //   memory in that case would add the same value multiple times.
+    // - We therefore restrict the store so that only the replica with rep == 0
+    //   performs the update (e.g. global[i] += fragment[i] only fires once).
+    // Trigger conditions for this guard:
+    // 1) There are cross-thread stores targeting shared/global memory (no
+    //    fragment stores in this branch; atomic_add and similar remain TODO).
+    // 2) The loop layout replicate extent is greater than 1, inferred from the
+    //    thread bounds captured in the layout.
+
+    [this, &store_shared_global_buffers, &store_fragment_buffers,
+     &has_cross_thread_access, &const_index_fragment_buffer, &T]() {
+      if (is_one(loop_layout_->ReplicateExtent()))
+        return;
+      if (!has_cross_thread_access)
+        return;
+
+      if (!store_fragment_buffers.empty()) {
+        // Iterate replicated fragment stores: when the fragment index is a
+        // constant (e.g. fragment[0]), every thread touches the same slot, so
+        // the rep == 0 predicate is unnecessary. Example: for i in
+        // T.Parallel(...):
+        //   shared[i] = ...
+        //   fragment[0] = ...
+        bool replicate_is_from_dynamic_index_fragment = false;
+        for (const auto &fragment : store_fragment_buffers) {
+          if (!T.layout_map.count(fragment)) {
+            continue;
+          }
+
+          auto fragment_layout = T.layout_map[fragment].as<Fragment>().value();
+          if (is_one(fragment_layout->ReplicateExtent()))
+            continue;
+
+          if (analyzer_.CanProveEqual(fragment_layout->ReplicateExtent(),
+                                      loop_layout_->ReplicateExtent()))
+            continue;
+          if (std::find(const_index_fragment_buffer.begin(),
+                        const_index_fragment_buffer.end(),
+                        fragment) == const_index_fragment_buffer.end()) {
+            replicate_is_from_dynamic_index_fragment = true;
+          }
+        }
+
+        if (!replicate_is_from_dynamic_index_fragment)
+          return;
+
+        ICHECK(store_shared_global_buffers.empty())
+            << "Invalid layout: cannot have both fragment and shared store "
+               "buffers "
+               "in replicated loop layout.";
+        return;
+      } else {
+        // Now, store is global or shared
+        // or T.call_extern or T.call_intrin ...
+        auto inv = loop_layout_->Inverse();
+        Array<PrimExpr> fwd;
+        for (size_t i = 0; i < loop_layout_->OutputDim(); i++)
+          fwd.push_back(0);
+        fwd.push_back(InputPlaceholder(0) - T.thread_bounds->min);
+        auto rep = inv->Forward(fwd).back();
+        AddPredicate(EQ(rep, 0));
+      }
+    }();
   } else {
     return {};
   }
@@ -267,76 +582,40 @@ LayoutMap ParallelOp::InferLayout(const LayoutInferArgs &T, InferLevel level) {
   }
 
   // Step 2: Check that the loop's partition can correctly align with all source
-  // fragment
+  // fragment, and infer layout only when it's not yet layout-ed
+  LayoutMap results;
   for (const auto &[buffer, _] : indice_map_) {
     if (T.layout_map.count(buffer)) {
       auto fragment = T.layout_map[buffer].as<Fragment>().value();
-      // TODO: Add thread checks for replicated cases
-      // need to wildcard match the rhs with lhs
-      if (!is_one(loop_layout_->ReplicateExtent()) ||
-          !is_one(fragment->ReplicateExtent()))
-        continue;
       auto vars =
           loop_vars_.Map([](const IterVar &iv) { return PrimExpr(iv->var); });
-      auto lhs = loop_layout_->ForwardThread(vars, NullOpt);
-      auto rhs = fragment->ForwardThread(indice_map_[buffer], NullOpt);
-      auto diff = analyzer_.Simplify(lhs - rhs);
-      ICHECK(is_zero(diff))
-          << "Layout infer conflict for " << buffer << " " << source_buffer
-          << "\nLHS = " << lhs << "\nRHS = " << rhs;
-    }
-  }
-  // Step 3: Infer other fragment's layout from the loop's partition
-  LayoutMap results;
-  for (const auto &[buffer, _] : indice_map_) {
-    if (!T.layout_map.count(buffer)) {
-      results.Set(buffer, CompleteBufferFragment(buffer)->BindThreadRange(
-                              T.thread_bounds));
-    }
-
-    // Layout infer conflict for local.fragment can noy be handled here
-    // because the source_buffer is not always available
-    if (buffer.scope() == "local.fragment" && source_buffer.defined() &&
-        source_buffer.scope() == "local.fragment") {
-      if (T.layout_map.count(buffer)) {
-        const FragmentNode *src_layout =
-            T.layout_map[buffer].as<Fragment>().get();
-        Fragment dst_layout_fragment =
-            CompleteBufferFragment(buffer)->BindThreadRange(T.thread_bounds);
-        const FragmentNode *dst_layout =
-            dst_layout_fragment.as<Fragment>().get();
-        if (as_const_int(dst_layout->ReplicateExtent()) &&
-            as_const_int(src_layout->ReplicateExtent()) &&
-            (*as_const_int(dst_layout->ReplicateExtent()) >
-             *as_const_int(src_layout->ReplicateExtent()))) {
-          results.Set(buffer, dst_layout_fragment);
-          continue;
-        }
-        if (src_layout && dst_layout) {
-          ICHECK(src_layout->IsEqual(dst_layout, true))
-              << "Layout may conflict with ParallelOp for buffer " << buffer
-              << " vs. " << source_buffer << "\nError body begin:\n"
-              << GetRoot()->body << "\nError body end"
-              << "\nLHS = " << src_layout->DebugOutput()
-              << "\nRHS = " << dst_layout->DebugOutput()
-              << "\nYou may need to use a shared memory to transform the "
-                 "layout";
-        }
+      if (!ProveFragmentContains(loop_layout_, fragment, vars,
+                                 indice_map_[buffer], analyzer_)) {
+        std::ostringstream oss;
+        oss << "Layout infer conflict between " << buffer << " and "
+            << source_buffer << " in T.Parallel loop:" << '\n'
+            << "    loop " << loop_layout_->DebugOutput() << '\n'
+            << "    fragment " << fragment->DebugOutput() << '\n';
+        throw LayoutConflictException(oss.str());
       }
+    } else {
+      auto dst_layout =
+          CompleteBufferFragment(buffer)->BindThreadRange(T.thread_bounds);
+      results.Set(buffer, dst_layout);
     }
   }
   return results;
 }
 
-Optional<PrimExpr> ParallelOp::GetPredicate(Var thread_var) const {
+Optional<PrimExpr> ParallelOpNode::GetPredicate(Var thread_var) const {
   if (predicate_.defined()) {
     return Substitute(predicate_.value(), {{InputPlaceholder(0), thread_var}});
   } else {
-    return NullOpt;
+    return std::nullopt;
   }
 }
 
-Fragment ParallelOp::CompleteBufferFragment(const Buffer &buffer) {
+Fragment ParallelOpNode::CompleteBufferFragment(const Buffer &buffer) const {
   ICHECK(loop_layout_.defined());
   if (IsCommonAccessIndice(buffer)) {
     return loop_layout_;
@@ -358,9 +637,12 @@ Fragment ParallelOp::CompleteBufferFragment(const Buffer &buffer) {
   PrimExpr thd_b = loop_layout_->ForwardThread(
       ind_inv->Forward(fwd),
       FloorDiv(ReplicationPlaceholder(), indice_rep_extent));
-  return Fragment(buffer->shape, {}, thd_b, dest_buffer_rep_extent, NullOpt)
+  return Fragment(buffer->shape, {}, thd_b, dest_buffer_rep_extent,
+                  std::nullopt)
       ->CondenseReplicateVar();
 }
 
+TVM_FFI_STATIC_INIT_BLOCK({ ParallelOpNode::RegisterReflection(); });
+
 } // namespace tl
 } // namespace tvm
diff --git a/src/op/parallel.h b/src/op/parallel.h
index e84ca98a..9c6b7180 100644
--- a/src/op/parallel.h
+++ b/src/op/parallel.h
@@ -10,57 +10,152 @@
 #include <tvm/tir/stmt_functor.h>
 
 #include "../layout/layout.h"
-#include "op.h"
-
+#include "../transform/layout_reducer.h"
+#include "./operator.h"
+
+/**
+ * Conjoin `expr` into the operator's predicate (logical AND). If no predicate
+ * exists yet, `expr` becomes the predicate.
+ *
+ * @param expr Predicate expression to add.
+ */
 namespace tvm {
 namespace tl {
 
 using namespace tir;
 
-class ParallelOp;
+class LayoutConflictException : public std::exception {
+public:
+  const char *what() const noexcept override { return msg_.c_str(); }
+  LayoutConflictException(const std::string &msg) : msg_(msg) {}
+
+private:
+  std::string msg_;
+};
+
+bool ProveFragmentContains(Fragment small_frag, Fragment large_frag,
+                           Array<PrimExpr> small_frag_indices,
+                           Array<PrimExpr> large_frag_indices,
+                           arith::Analyzer &analyzer_);
+
+class ParallelOpNode;
 
 class ParallelLoopNestVisitor : public StmtExprVisitor {
 private:
-  ParallelLoopNestVisitor(ParallelOp *op) : p(op){};
-  void VisitStmt_(const ForNode *op) final;
-  void VisitStmt_(const BufferStoreNode *op) final;
-  void VisitExpr_(const BufferLoadNode *op) final;
+  ParallelLoopNestVisitor(ParallelOpNode *op) : p(op) {};
+  void VisitStmt_(const ForNode *op) override;
+  void VisitStmt_(const BufferStoreNode *op) override;
+  void VisitExpr_(const BufferLoadNode *op) override;
 
-  ParallelOp *p;
+  ParallelOpNode *p;
 
-  friend class ParallelOp;
+  friend class ParallelOpNode;
 };
 
-class ParallelOp : public Operator {
+// ParallelOpNode represents a parallel for loop operator in TileLang.
+// It is responsible for inferring layouts, holding loop structure, and managing
+// predicates.
+class ParallelOpNode : public TileOperatorNode {
 public:
-  ParallelOp(For root);
-  LayoutMap InferLayout(const LayoutInferArgs &T, InferLevel level) final;
+  // The root For loop node.
+  For root_;
+  // The inferred layout for the loop, mutable to allow lazy inference.
+  mutable Fragment loop_layout_;
+  // The predicate expression for the loop, if any, mutable for lazy
+  // construction.
+  mutable Optional<PrimExpr> predicate_;
+
+  // Type key for TVM object system.
+  static constexpr const char *_type_key = "tl.ParallelOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(ParallelOpNode, TileOperatorNode);
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<ParallelOpNode>()
+        .def_ro("root", &ParallelOpNode::root_)
+        .def_ro("loop_layout", &ParallelOpNode::loop_layout_)
+        .def_ro("predicate", &ParallelOpNode::predicate_);
+  }
+
+  bool SEqualReduce(const ParallelOpNode *other, SEqualReducer equal) const {
+    return equal(root_, other->root_) &&
+           equal(loop_layout_, other->loop_layout_) &&
+           equal(predicate_, other->predicate_);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(root_);
+    hash_reduce(loop_layout_);
+    hash_reduce(predicate_);
+  }
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  // Construct from a root For loop.
+  ParallelOpNode(For root);
 
+  // Lower the operator to a TIR statement.
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+
+  // Infer the layout for this parallel operator.
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+
+  // Copy constructor for ParallelOpNode.
+  ParallelOpNode(const ParallelOpNode &other) : ParallelOpNode(other.root_) {
+    loop_layout_ = other.loop_layout_;
+    predicate_ = other.predicate_;
+  }
+
+  // Get the inferred loop layout.
   Fragment GetLoopLayout() const { return loop_layout_; }
+  // Get the root For loop.
   For GetRoot() const { return root_; }
+  // Get the mapping from buffer to access indices.
   Map<Buffer, Array<PrimExpr>> GetIndiceMap() const { return indice_map_; }
+  // Get the predicate for a given thread variable.
   Optional<PrimExpr> GetPredicate(Var thread_var) const;
 
+  // Clone this operator.
+  TileOperator Clone() const override;
+
 private:
-  Fragment CompleteBufferFragment(const Buffer &buffer);
+  // Complete the fragment layout for a given buffer.
+  Fragment CompleteBufferFragment(const Buffer &buffer) const;
+  // Check if the buffer is accessed with common indices (i.e., loop variables).
   bool IsCommonAccessIndice(const Buffer &buffer) const;
-  void AddPredicate(PrimExpr expr) {
+  // Add a predicate to the current predicate expression.
+  void AddPredicate(const PrimExpr &expr) const {
     predicate_ = predicate_.defined() ? And(expr, predicate_.value()) : expr;
   }
 
-  For root_;
+  // Allow ParallelLoopNestVisitor to access private members.
+  friend class ParallelLoopNestVisitor;
 
+  // Visitor for collecting loop nest information.
   ParallelLoopNestVisitor V;
-
+  // Mapping from buffer to their access indices in the loop.
   Map<Buffer, Array<PrimExpr>> indice_map_;
+  // Set of buffers that are written to in the loop.
   std::unordered_set<Buffer, ObjectPtrHash, ObjectPtrEqual> buffer_is_write_;
+  // The loop variables for the parallel loop nest.
   Array<IterVar> loop_vars_;
-
-  Fragment loop_layout_;
+  // The inner_vars_
+  Map<Var, IterVar> inner_vars_;
+  // Analyzer for simplifying and analyzing expressions, mutable for lazy use.
   mutable arith::Analyzer analyzer_;
-  Optional<PrimExpr> predicate_;
+  // Mapping from buffer to reducer info.
+  Map<Var, ReducerInfo> reducer_info_map_;
+};
 
-  friend class ParallelLoopNestVisitor;
+class ParallelOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(ParallelOp, TileOperator, ParallelOpNode);
+
+  ParallelOp(const For &root) {
+    auto op = make_object<ParallelOpNode>(root);
+    data_ = std::move(op);
+  }
 };
 
 } // namespace tl
diff --git a/src/op/reduce.cc b/src/op/reduce.cc
index 6d594da1..fe49e00b 100644
--- a/src/op/reduce.cc
+++ b/src/op/reduce.cc
@@ -1,7 +1,6 @@
 /*!
  * \file tl/op/reduce.cc
- *
- * Define reduce operator.
+ * \brief Implementation of reduction operators
  */
 
 #include "reduce.h"
@@ -12,6 +11,8 @@
 #include <tvm/tir/stmt_functor.h>
 
 #include "../layout/utils.h"
+#include "../op/parallel.h"
+#include "../target/utils.h"
 #include "../transform/loop_partition.h"
 #include "tir/transforms/ir_utils.h"
 
@@ -21,37 +22,37 @@ namespace tl {
 using namespace tir;
 
 ReduceOp::ReduceOp(Array<PrimExpr> args, BufferMap vmap) {
-  src = vmap[GetVarFromAccessPtr(args[0])];
-  dst = vmap[GetVarFromAccessPtr(args[1])];
-  String reduce_type = args[2].as<StringImm>().value()->value;
-  dim = args[3].as<IntImm>().value()->value;
-  if (reduce_type == "sum")
-    type = ReduceType::kSum;
-  else if (reduce_type == "abssum")
-    type = ReduceType::kAbsSum;
-  else if (reduce_type == "absmax")
-    type = ReduceType::kAbsMax;
-  else if (reduce_type == "max")
-    type = ReduceType::kMax;
-  else if (reduce_type == "min")
-    type = ReduceType::kMin;
-  else
-    ICHECK(0) << "Unknown reduce type: " << reduce_type;
-  clear = args[4].as<Bool>().value();
+  ObjectPtr<ReduceOpNode> node = make_object<ReduceOpNode>();
+  node->src = vmap[GetVarFromAccessPtr(args[0])];
+  node->dst = vmap[GetVarFromAccessPtr(args[1])];
+  std::string reduce_type = args[2].as<StringImm>().value()->value;
+  node->dim = args[3].as<IntImm>().value()->value;
+  node->type = ReduceType(reduce_type);
+  node->clear = args[4].as<Bool>().value();
+  data_ = std::move(node);
+}
+
+TileOperator ReduceOpNode::Clone() const {
+  auto op = make_object<ReduceOpNode>(*this);
+  return ReduceOp(op);
+}
+
+TileOperator CumSumOpNode::Clone() const {
+  auto op = make_object<CumSumOpNode>(*this);
+  return CumSumOp(op);
 }
 
-PrimExpr ReduceOp::MakeInitValue() const {
+PrimExpr ReduceOpNode::MakeInitValue() const {
   auto dst_dtype = dst->dtype;
   auto is_int = dst_dtype.is_int();
   bool is_uint = dst_dtype.is_uint();
   auto bits = dst_dtype.bits();
 
-  switch (type) {
-  case ReduceType::kSum:
+  if (type->isSum()) {
     return make_zero(dst->dtype);
-  case ReduceType::kAbsSum:
+  } else if (type->isAbsSum()) {
     return make_zero(dst->dtype);
-  case ReduceType::kMax:
+  } else if (type->isMax()) {
     if (is_int) {
       return make_const(dst->dtype, -(1 << (bits - 1)));
     } else if (is_uint) {
@@ -59,7 +60,7 @@ PrimExpr ReduceOp::MakeInitValue() const {
     } else {
       return make_const(dst->dtype, -INFINITY);
     }
-  case ReduceType::kMin:
+  } else if (type->isMin()) {
     if (is_int) {
       return make_const(dst->dtype, (1 << (bits - 1)) - 1);
     } else if (is_uint) {
@@ -67,227 +68,372 @@ PrimExpr ReduceOp::MakeInitValue() const {
     } else {
       return make_const(dst->dtype, INFINITY);
     }
-  case ReduceType::kAbsMax:
+  } else if (type->isAbsMax()) {
     return make_const(dst->dtype, 0);
-  default:
-    ICHECK(0);
+  } else if (type->isBitAnd()) {
+    if (is_int) {
+      return make_const(dst->dtype, -1);
+    } else if (is_uint) {
+      return make_const(dst->dtype, (1 << bits) - 1);
+    } else {
+      // Should not arrive here
+      return make_const(dst->dtype, -INFINITY);
+    }
+  } else if (type->isBitOr()) {
+    return make_zero(dst->dtype);
+  } else if (type->isBitXor()) {
+    return make_zero(dst->dtype);
+  } else {
+    LOG(FATAL) << "Unsupported reduce type: " << type->type;
   }
 }
 
-PrimExpr ReduceOp::MakeReduce(const PrimExpr &a, const PrimExpr &b) const {
-  PrimExpr lhs = a, rhs = b;
+PrimExpr ReduceOpNode::MakeReduce(const PrimExpr &lhs,
+                                  const PrimExpr &b) const {
+  PrimExpr rhs = b;
   if (lhs->dtype != rhs->dtype) {
     rhs = Cast(lhs->dtype, rhs);
   }
-  switch (type) {
-  case ReduceType::kSum:
+  if (type->isSum()) {
     return lhs + rhs;
-  case ReduceType::kAbsSum:
+  } else if (type->isAbsSum()) {
     return lhs + Max(rhs, -rhs);
-  case ReduceType::kMax:
+  } else if (type->isMax()) {
     return Max(lhs, rhs);
-  case ReduceType::kMin:
+  } else if (type->isMin()) {
     return Min(lhs, rhs);
-  case ReduceType::kAbsMax:
+  } else if (type->isAbsMax()) {
     return Max(Max(lhs, rhs), -Min(lhs, rhs));
-  default:
-    ICHECK(0);
-    return PrimExpr(0);
+  } else if (type->isBitAnd()) {
+    return lhs & rhs;
+  } else if (type->isBitOr()) {
+    return lhs | rhs;
+  } else if (type->isBitXor()) {
+    return lhs ^ rhs;
+  } else {
+    LOG(FATAL) << "Unsupported reduce type: " << type->type;
   }
 }
 
-std::string ReduceOp::MakeCodegenReducer() const {
-  switch (type) {
-  case ReduceType::kSum:
+std::string ReduceOpNode::MakeCodegenReducer() const {
+  if (type->isSum()) {
     return "tl::SumOp";
-  case ReduceType::kAbsSum:
+  } else if (type->isAbsSum()) {
     return "tl::SumOp";
-  case ReduceType::kMax:
+  } else if (type->isMax()) {
     return "tl::MaxOp";
-  case ReduceType::kMin:
+  } else if (type->isMin()) {
     return "tl::MinOp";
-  case ReduceType::kAbsMax:
+  } else if (type->isAbsMax()) {
     return "tl::MaxOp";
-  default:
-    ICHECK(0);
+  } else if (type->isBitAnd()) {
+    return "tl::BitAndOp";
+  } else if (type->isBitOr()) {
+    return "tl::BitOrOp";
+  } else if (type->isBitXor()) {
+    return "tl::BitXorOp";
+  } else {
+    LOG(FATAL) << "Unsupported reduce type: " << type->type;
     return "";
   }
 }
 
-Stmt ReduceOp::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
-  ICHECK(this->src.scope() == "local.fragment" &&
-         this->dst.scope() == "local.fragment")
-      << "Reduce for shared memory not implemented.";
-  auto src_buffer = T.buffer_remap[this->src];
-  auto dst_buffer = T.buffer_remap[this->dst];
-  Fragment src_layout = T.layout_map[this->src].as<Fragment>().value();
-  Fragment dst_layout = T.layout_map[this->dst].as<Fragment>().value();
-  size_t src_dim = src_layout->InputDim();
-  size_t dst_dim = dst_layout->InputDim();
-
-  bool is_1d_reduce = src_dim == dst_dim && dst_dim == 1;
-
-  if (is_1d_reduce) {
-    ICHECK(is_one(dst_layout->OutputShape().back()))
-        << "Reduce for scalar not implemented.";
-  } else {
-    ICHECK(src_dim == dst_dim + 1) << "Reduce dimension mismatch.";
-  }
+/**
+ * @brief Lower the Reduce operator to a TIR statement.
+ *
+ * Lowers a ReduceOpNode operating on fragment-scoped buffers into a sequence of
+ * TIR statements implementing: optional initialization, thread-local reduction
+ * (unrolled inner loops), inter-thread reduction via a runtime AllReduce call
+ * (Hopper-specific `run_hopper` variant when TargetIsHopper(T.target) is true),
+ * and an optional accumulation or copy back to the destination buffer when a
+ * temporary clear buffer is used.
+ *
+ * Behavior notes:
+ * - Only supports src and dst in "local.fragment" scope; otherwise it checks
+ *   and aborts with "Reduce for shared memory not implemented.".
+ * - Supports both 1D reductions (scalar output) and reductions along a single
+ *   extra dimension; validates layout dimensionality consistency.
+ * - If `clear` is set (or for sum/abssum reductions), an initial value is
+ *   written to the clear buffer; for non-clearing sum/abssum a duplicate
+ *   temporary buffer is allocated and accumulated back into dst after
+ * reduction.
+ * - Performs iterator compression for local reduction loops using `analyzer`.
+ * - Detects parallel thread splitting from the normalized iterator sum and
+ *   emits a call to a templated `tl::AllReduce<...>::run` (or `run_hopper`)
+ *   via `builtin::call_extern`. For sufficiently large reducing thread counts
+ *   (>= 32) a workspace is allocated via T.AddWorkspace and passed to the
+ *   AllReduce call.
+ * - The final body is wrapped in parallel loops over the destination spatial
+ *   dimensions and partitioned by the lowering thread variable. If a temporary
+ *   clear buffer is used, it is allocated for the body.
+ *
+ * @param T Lowering context providing buffer and layout maps, thread bounds,
+ *          target information, thread variable, and workspace allocation
+ * helper.
+ * @param analyzer Analyzer used for iterator compression and arithmetic
+ * normalization.
+ * @return Stmt Lowered TIR statement implementing the reduction.
+ */
+Stmt ReduceOpNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  auto get_buffer = [&](const Buffer &buf) {
+    if (T.buffer_remap.count(buf))
+      return T.buffer_remap[buf];
+    return buf;
+  };
+
+  auto src_scope = this->src.scope();
+  auto dst_scope = this->dst.scope();
+
+  if (src_scope == "local.fragment" && dst_scope == "local.fragment") {
+    Buffer src_buffer = get_buffer(this->src);
+    Buffer dst_buffer = get_buffer(this->dst);
+    Fragment src_layout = T.layout_map[this->src].as<Fragment>().value();
+    Fragment dst_layout = T.layout_map[this->dst].as<Fragment>().value();
+    size_t src_dim = src_layout->InputDim();
+    size_t dst_dim = dst_layout->InputDim();
+
+    bool is_1d_reduce = src_dim == dst_dim && dst_dim == 1;
+
+    if (is_1d_reduce) {
+      ICHECK(is_one(dst_layout->OutputShape().back()))
+          << "Reduce for scalar not implemented.";
+    } else {
+      ICHECK_EQ(src_dim, dst_dim + 1) << "Reduce dimension mismatch.";
+    }
 
-  Array<IterVar> dst_vars;
-  for (size_t i = 0; i < dst_dim; i++) {
-    Var var = Var(std::string{char('i' + i)});
-    dst_vars.push_back(IterVar(Range(0, dst_layout->InputShape()[i]), var,
-                               IterVarType::kDataPar));
-  }
-  Array<IterVar> src_vars;
-  if (!is_1d_reduce) {
-    src_vars = dst_vars;
-  }
-  src_vars.insert(src_vars.begin() + this->dim,
-                  {Range(0, src_layout->InputShape()[this->dim]), Var("rv"),
-                   IterVarType::kDataPar});
-  Array<PrimExpr> src_indices = src_layout->Forward(
-      src_vars.Map([](const auto &iv) { return PrimExpr(iv->var); }));
-  Array<PrimExpr> dst_indices = dst_layout->Forward(
-      dst_vars.Map([](const auto &iv) { return PrimExpr(iv->var); }));
-
-  Array<Stmt> stmts;
-
-  bool require_init = this->clear;
-  // sum op must be cleared
-  if (this->type == ReduceType::kSum) {
-    require_init = true;
-  } else if (this->type == ReduceType::kAbsSum) {
-    require_init = true;
-  }
+    Array<IterVar> dst_vars;
+    for (size_t i = 0; i < dst_dim; ++i) {
+      Var var = Var(std::string{char('i' + i)});
+      dst_vars.push_back(IterVar(Range(0, dst_layout->InputShape()[i]), var,
+                                 IterVarType::kDataPar));
+    }
 
-  Buffer clear_buffer = dst_buffer;
-  bool need_duplicate = false;
-  if (this->type == ReduceType::kSum && !this->clear) {
-    need_duplicate = true;
-  } else if (this->type == ReduceType::kAbsSum && !this->clear) {
-    need_duplicate = true;
-  }
+    Array<IterVar> src_vars;
+    if (!is_1d_reduce) {
+      src_vars = dst_vars;
+    }
+    Range reduce_dom(0, src_layout->InputShape()[this->dim]);
+    IterVar reduce_iv(reduce_dom, Var("rv"), IterVarType::kDataPar);
+    src_vars.insert(src_vars.begin() + this->dim, reduce_iv);
+
+    Array<PrimExpr> src_indices = src_layout->Forward(
+        src_vars.Map([](const auto &iv) { return PrimExpr(iv->var); }));
+    Array<PrimExpr> dst_indices = dst_layout->Forward(
+        dst_vars.Map([](const auto &iv) { return PrimExpr(iv->var); }));
+
+    Array<Stmt> stmts;
+
+    bool require_init = this->clear;
+    if (this->type->isSum() || this->type->isAbsSum() ||
+        this->type->isBitAnd() || this->type->isBitOr() ||
+        this->type->isBitXor()) {
+      require_init = true;
+    }
 
-  if (need_duplicate) {
-    // Create a new buffer with same shape and dtype as dst_buffer
-    clear_buffer = decl_buffer(dst_buffer->shape, dst_buffer->dtype,
-                               dst_buffer->name + "_clear",
-                               GetPtrStorageScope(dst_buffer->data));
-  }
+    Buffer clear_buffer = dst_buffer;
+    bool need_duplicate = false;
+    if ((this->type->isSum() || this->type->isAbsSum()) && !this->clear) {
+      need_duplicate = true;
+    } else if (this->type->isBitAnd() && !this->clear) {
+      need_duplicate = true;
+    } else if ((this->type->isBitOr() || this->type->isBitXor()) &&
+               !this->clear) {
+      need_duplicate = true;
+    }
 
-  // make reduce-init stmt
-  if (require_init)
-    stmts.push_back(
-        BufferStore(clear_buffer, this->MakeInitValue(), dst_indices));
-
-  // make thread-local reduce
-  Array<PrimExpr> src_indice_compressed;
-  Array<IterVar> src_var_compressed;
-  for (size_t i = 0; i < src_layout->OutputDim(); i++) {
-    PrimExpr expr;
-    IterVar var;
-    std::tie(expr, var) = CompressIterator(src_indices[i], src_vars,
-                                           src_vars[this->dim]->var, analyzer);
-    src_indice_compressed.push_back(expr);
-    src_var_compressed.push_back(var);
-  }
-  Stmt reduce_local = BufferStore(
-      clear_buffer,
-      this->MakeReduce(BufferLoad(clear_buffer, dst_indices),
-                       BufferLoad(src_buffer, src_indice_compressed)),
-      dst_indices);
-  for (int i = src_layout->OutputDim() - 1; i >= 0; i--) {
-    reduce_local =
-        For(src_var_compressed[i]->var, 0, src_var_compressed[i]->dom->extent,
-            ForKind::kUnrolled, reduce_local, NullOpt,
-            {{tir::attr::pragma_unroll_explicit, Bool(false)}});
-  }
-  stmts.push_back(reduce_local);
-
-  // make inter-thread reduce
-  PrimExpr src_thread = src_layout->ForwardThread(
-      src_vars.Map([](const auto &iv) { return PrimExpr(iv->var); }), {});
-  auto iter_sum =
-      arith::NormalizeToIterSum(src_thread, ToVMap(src_vars), analyzer);
-  for (const auto &iter_split : iter_sum->args) {
-    auto mark = iter_split->source->source.as<Var>();
-    ICHECK(mark.defined());
-    if (mark.value().same_as(src_vars[this->dim]->var)) {
-      auto scale = as_const_int(iter_split->scale);
-      auto extent = as_const_int(iter_split->extent);
-      ICHECK(scale != nullptr && extent != nullptr);
-      if (*extent == 1)
-        continue;
-
-      int reducing_threads = (*extent) * (*scale);
-      std::stringstream ss;
-
-      bool has_arch = T.target->attrs.count("arch") > 0;
-      auto thread_offset = T.thread_bounds->min;
-      if (has_arch && Downcast<String>(T.target->attrs["arch"]) == "sm_90") {
-        auto all_threads = T.thread_bounds->extent;
-        ss << "tl::AllReduce<" << this->MakeCodegenReducer() << ", "
-           << reducing_threads << ", " << (*scale) << ", " << thread_offset
-           << ", " << all_threads << ">::run_hopper";
-      } else {
-        ss << "tl::AllReduce<" << this->MakeCodegenReducer() << ", "
-           << reducing_threads << ", " << (*scale) << ", " << thread_offset
-           << ">::run";
+    if (need_duplicate) {
+      // Create a new buffer with same shape and dtype as dst_buffer
+      clear_buffer = decl_buffer(dst_buffer->shape, dst_buffer->dtype,
+                                 dst_buffer->name + "_clear",
+                                 GetPtrStorageScope(dst_buffer->data));
+    }
+    // make reduce-init stmt
+    if (require_init) {
+      stmts.push_back(
+          BufferStore(clear_buffer, this->MakeInitValue(), dst_indices));
+    }
+
+    // make thread-local reduce
+    Array<PrimExpr> src_indice_compressed;
+    Array<IterVar> src_var_compressed;
+    for (size_t i = 0; i < src_layout->OutputDim(); ++i) {
+      PrimExpr expr;
+      IterVar var;
+      std::tie(expr, var) = CompressIterator(
+          src_indices[i], src_vars, src_vars[this->dim]->var, analyzer);
+      src_indice_compressed.push_back(expr);
+      src_var_compressed.push_back(var);
+    }
+
+    Stmt reduce_local = BufferStore(
+        clear_buffer,
+        this->MakeReduce(BufferLoad(clear_buffer, dst_indices),
+                         BufferLoad(src_buffer, src_indice_compressed)),
+        dst_indices);
+
+    for (int i = static_cast<int>(src_layout->OutputDim()) - 1; i >= 0; --i) {
+      reduce_local =
+          For(src_var_compressed[i]->var, 0, src_var_compressed[i]->dom->extent,
+              ForKind::kUnrolled, reduce_local, std::nullopt,
+              {{tir::attr::pragma_unroll_explicit, Bool(false)}});
+    }
+    stmts.push_back(reduce_local);
+
+    PrimExpr src_thread = src_layout->ForwardThread(
+        src_vars.Map([](const auto &iv) { return PrimExpr(iv->var); }), {});
+    auto iter_sum =
+        arith::NormalizeToIterSum(src_thread, ToVMap(src_vars), analyzer);
+    for (const auto &iter_split : iter_sum->args) {
+      auto mark = iter_split->source->source.as<Var>();
+      ICHECK(mark) << "Not a normalized iterator: " << iter_split->source;
+      if (mark.value().same_as(src_vars[this->dim]->var)) {
+        auto scale = as_const_int(iter_split->scale);
+        auto extent = as_const_int(iter_split->extent);
+        ICHECK(scale != nullptr && extent != nullptr);
+        if (*extent == 1)
+          continue;
+
+        int reducing_threads = (*extent) * (*scale);
+        std::stringstream ss;
+
+        auto thread_offset = T.thread_bounds->min;
+        if (TargetIsHopper(T.target) || TargetIsSm100(T.target)) {
+          auto all_threads = T.thread_bounds->extent;
+          ss << "tl::AllReduce<" << this->MakeCodegenReducer() << ", "
+             << reducing_threads << ", " << (*scale) << ", " << thread_offset
+             << ", " << all_threads << ">::run_hopper";
+        } else {
+          ss << "tl::AllReduce<" << this->MakeCodegenReducer() << ", "
+             << reducing_threads << ", " << (*scale) << ", " << thread_offset
+             << ">::run";
+        }
+        Array<PrimExpr> thread_reduce_args = {
+            StringImm(ss.str()), BufferLoad(clear_buffer, dst_indices)};
+        if (reducing_threads >= 32) {
+          PrimExpr workspace = T.AddWorkspace(
+              *as_const_int(T.thread_bounds->extent), clear_buffer->dtype);
+          thread_reduce_args.push_back(workspace);
+        }
+        auto call = Call(clear_buffer->dtype, builtin::call_extern(),
+                         thread_reduce_args);
+        stmts.push_back(BufferStore(clear_buffer, call, dst_indices));
       }
-      Array<PrimExpr> thread_reduce_args = {
-          StringImm(ss.str()), BufferLoad(clear_buffer, dst_indices)};
-      if (reducing_threads >= 32) {
-        PrimExpr workspace = T.AddWorkspace(
-            *as_const_int(T.thread_bounds->extent), clear_buffer->dtype);
-        thread_reduce_args.push_back(workspace);
+    }
+
+    if (need_duplicate) {
+      PrimExpr src_val = BufferLoad(clear_buffer, dst_indices);
+      PrimExpr dst_val = BufferLoad(dst_buffer, dst_indices);
+      PrimExpr update;
+      if (this->type->isSum() || this->type->isAbsSum()) {
+        update = dst_val + src_val;
+      } else if (this->type->isBitAnd()) {
+        update = this->clear ? src_val : bitwise_and(dst_val, src_val);
+      } else if (this->type->isBitOr()) {
+        update = bitwise_or(dst_val, src_val);
+      } else if (this->type->isBitXor()) {
+        update = bitwise_xor(dst_val, src_val);
+      } else {
+        LOG(FATAL) << "Unsupported reduce type: " << this->type->type;
       }
-      auto call =
-          Call(clear_buffer->dtype, builtin::call_extern(), thread_reduce_args);
-      stmts.push_back(BufferStore(clear_buffer, call, dst_indices));
+      stmts.push_back(BufferStore(dst_buffer, update, dst_indices));
     }
-  }
-  Stmt reduce_interthread = BufferStore(
-      clear_buffer, BufferLoad(clear_buffer, dst_indices), dst_indices);
-
-  // copy clear_buffer to dst_buffer
-  if (need_duplicate) {
-    // if is reduce sum, we should add a copy from clear_buffer to dst_buffer
-    if (this->type == ReduceType::kSum) {
-      stmts.push_back(BufferStore(dst_buffer,
-                                  Add(BufferLoad(dst_buffer, dst_indices),
-                                      BufferLoad(clear_buffer, dst_indices)),
-                                  dst_indices));
-    } else if (this->type == ReduceType::kAbsSum) {
-      stmts.push_back(BufferStore(dst_buffer,
-                                  Add(BufferLoad(dst_buffer, dst_indices),
-                                      BufferLoad(clear_buffer, dst_indices)),
-                                  dst_indices));
+
+    Stmt body = stmts.size() > 1 ? SeqStmt(stmts) : stmts[0];
+    for (int i = static_cast<int>(dst_layout->InputDim()) - 1; i >= 0; --i) {
+      body = For(dst_vars[i]->var, 0, dst_vars[i]->dom->extent,
+                 ForKind::kParallel, body);
+    }
+
+    if (dst_layout->InputDim() > 0) {
+      body = PartitionLoop(Downcast<For>(body), T.thread_var, analyzer,
+                           dst_layout);
     } else {
-      ICHECK(false) << "Unsupported reduce type: " << (int)this->type;
+      PrimExpr guard = (T.thread_var == T.thread_bounds->min);
+      body = IfThenElse(guard, body);
     }
-  }
-  // make the outer spatial loop
-  Stmt body = stmts.size() > 1 ? SeqStmt(stmts) : stmts[0];
-  for (int i = dst_layout->InputDim() - 1; i >= 0; i--) {
-    body = For(dst_vars[i]->var, 0, dst_vars[i]->dom->extent,
-               ForKind::kParallel, body);
+
+    if (need_duplicate) {
+      body = Allocate(clear_buffer->data, clear_buffer->dtype,
+                      clear_buffer->shape, const_true(), body);
+    }
+    return body;
   }
 
-  body = PartitionLoop(Downcast<For>(body), T.thread_var, analyzer, dst_layout);
-  if (need_duplicate) {
-    body = Allocate(clear_buffer->data, clear_buffer->dtype,
-                    clear_buffer->shape, const_true(), body);
+  auto is_shared_scope = [](const std::string &scope) {
+    return scope == "shared" || scope == "shared.dyn";
+  };
+
+  if (is_shared_scope(src_scope) && is_shared_scope(dst_scope)) {
+    Buffer src_buffer = get_buffer(this->src);
+    Buffer dst_buffer = get_buffer(this->dst);
+
+    size_t src_dim = src_buffer->shape.size();
+    size_t dst_dim = dst_buffer->shape.size();
+    bool is_1d_reduce = (src_dim == dst_dim && dst_dim == 1);
+    if (!is_1d_reduce) {
+      ICHECK_EQ(src_dim, dst_dim + 1) << "Reduce dimension mismatch.";
+    } else {
+      ICHECK_EQ(dst_dim, 1U) << "Expect scalar layout for 1D reduce.";
+    }
+
+    auto thread_extent = as_const_int(T.thread_bounds->extent);
+    ICHECK(thread_extent)
+        << "Shared-memory reduce requires static thread extent.";
+    int threads = *thread_extent;
+
+    if (TargetIsCuda(T.target)) {
+      ICHECK_EQ(threads % 32, 0)
+          << "Shared reduce expects blockDim.x to be a multiple of 32 on CUDA.";
+    } else if (TargetIsRocm(T.target)) {
+      ICHECK_EQ(threads % 64, 0)
+          << "Shared reduce expects blockDim.x to be a multiple of 64 on HIP.";
+    }
+
+    bool use_abs = this->type->isAbsSum() || this->type->isAbsMax();
+    bool need_accumulate =
+        (!this->clear) && (this->type->isSum() || this->type->isAbsSum() ||
+                           this->type->isBitAnd() || this->type->isBitOr() ||
+                           this->type->isBitXor());
+
+    PrimExpr reduce_extent = src_buffer->shape[this->dim];
+    PrimExpr tail_extent = make_const(DataType::Int(32), 1);
+    for (size_t i = this->dim + 1; i < src_dim; ++i) {
+      tail_extent = analyzer->Simplify(tail_extent * src_buffer->shape[i]);
+    }
+
+    PrimExpr total_dest = make_const(DataType::Int(32), 1);
+    for (size_t i = 0; i < dst_dim; ++i) {
+      total_dest = analyzer->Simplify(total_dest * dst_buffer->shape[i]);
+    }
+
+    std::stringstream ss;
+    std::string reducer = this->MakeCodegenReducer();
+    ss << "tl::SharedReduceWarp<" << reducer << ", " << threads << ", "
+       << (use_abs ? "true" : "false") << ", "
+       << (need_accumulate ? "true" : "false") << ">::run";
+
+    Array<PrimExpr> call_args = {StringImm(ss.str()),
+                                 src_buffer.access_ptr(1),
+                                 dst_buffer.access_ptr(3),
+                                 cast(DataType::Int(32), total_dest),
+                                 cast(DataType::Int(32), reduce_extent),
+                                 cast(DataType::Int(32), tail_extent),
+                                 this->MakeInitValue()};
+
+    return Evaluate(Call(dst_buffer->dtype, builtin::call_extern(), call_args));
   }
-  return body;
+
+  LOG(FATAL) << "Reduce for buffers in scope (" << src_scope << ", "
+             << dst_scope << ") is not implemented.";
+  return Stmt();
 }
 
-LayoutMap ReduceOp::InferLayout(const LayoutInferArgs &T, InferLevel level) {
+LayoutMap ReduceOpNode::InferLayout(const LayoutInferArgs &T,
+                                    InferLevel level) const {
   if (level >= InferLevel::kStrict)
     return {};
   if (src.scope() == "local.fragment" && dst.scope() == "local.fragment" &&
-      T.layout_map.count(src) && !T.layout_map.count(dst)) {
+      T.layout_map.count(src)) {
     auto src_layout = T.layout_map[src].as<Fragment>().value();
 
     PrimExpr indice_rep_extent = src->shape[dim];
@@ -307,10 +453,49 @@ LayoutMap ReduceOp::InferLayout(const LayoutInferArgs &T, InferLevel level) {
     auto thd = src_layout->ForwardThread(
         fwd, FloorDiv(ReplicationPlaceholder(), indice_rep_extent));
     Fragment dst_layout =
-        Fragment(dst->shape, {}, thd, dest_buffer_rep_extent, NullOpt)
+        Fragment(dst->shape, {}, thd, dest_buffer_rep_extent, std::nullopt)
             ->CondenseReplicateVar()
             ->BindThreadRange(T.thread_bounds);
-    return {{dst, dst_layout}};
+    if (!T.layout_map.count(dst))
+      return {{dst, dst_layout}};
+    else {
+      // Check if computed layout is compatible with existing: the existing one
+      // must strictly contains the computed layout
+      auto orig_dst_layout =
+          T.layout_map.Get(dst).value().as<Fragment>().value();
+      ICHECK(dst_layout->InputDim() == orig_dst_layout->InputDim());
+      Array<PrimExpr> indices;
+      indices.reserve(dst_layout->InputDim());
+      arith::Analyzer inner_analyzer;
+      for (int i = 0; i < dst_layout->InputDim(); ++i) {
+        auto x = InputPlaceholder(i);
+        indices.push_back(x);
+        // should be literal - literal = 0, any analyzer will work
+        ICHECK(is_zero(inner_analyzer.Simplify(
+            dst_layout->InputShape()[i] - orig_dst_layout->InputShape()[i])));
+        inner_analyzer.Bind(x, Range(0, dst_layout->InputShape()[i]));
+      }
+
+      ICHECK(as_const_int(dst_layout->ReplicateExtent()));
+      ICHECK(as_const_int(src_layout->ReplicateExtent()));
+      auto dst_rep = *as_const_int(dst_layout->ReplicateExtent());
+      auto src_rep = *as_const_int(src_layout->ReplicateExtent());
+      if (dst_rep < src_rep ||
+          !ProveFragmentContains(orig_dst_layout, dst_layout, indices, indices,
+                                 inner_analyzer)) {
+        std::ostringstream oss;
+        oss << "Layout may conflict with ReduceOp for buffer " << dst << " vs. "
+            << src << "\nLHS = " << src_layout->DebugOutput()
+            << "\nRHS = " << orig_dst_layout->DebugOutput()
+            << "\nYou may need to use a shared memory to transform the "
+               "layout";
+        throw LayoutConflictException(oss.str());
+      }
+
+      if (dst_rep > src_rep) {
+        return {{dst, dst_layout}};
+      }
+    }
   }
   return {};
 }
@@ -321,22 +506,22 @@ TIR_REGISTER_TL_OP(ReduceOp, reduce)
                                Integer(CallEffectKind::kOpaque));
 
 CumSumOp::CumSumOp(Array<PrimExpr> args, BufferMap vmap) {
-  /*
-    CumSum arguments:
-      src: input buffer
-      dst: output buffer
-      dim: dimension to cumsum
-      reverse: whether to cumsum in reverse order
-   */
+  /// CumSum constructor arguments:
+  /// - src: input buffer
+  /// - dst: output buffer
+  /// - dim: dimension to cumsum
+  /// - reverse: whether to cumsum in reverse order
   CHECK_EQ(args.size(), 4);
-  src = vmap[GetVarFromAccessPtr(args[0])];
-  dst = vmap[GetVarFromAccessPtr(args[1])];
-  dim = args[2].as<IntImm>().value()->value;
-  reverse = args[3].as<Bool>().value();
-  CHECK_LT(dim, static_cast<int>(src->shape.size()));
+  ObjectPtr<CumSumOpNode> node = make_object<CumSumOpNode>();
+  node->src = vmap[GetVarFromAccessPtr(args[0])];
+  node->dst = vmap[GetVarFromAccessPtr(args[1])];
+  node->dim = args[2].as<IntImm>().value()->value;
+  node->reverse = args[3].as<Bool>().value();
+  CHECK_LT(node->dim, static_cast<int>(node->src->shape.size()));
+  data_ = std::move(node);
 }
 
-Stmt CumSumOp::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+Stmt CumSumOpNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
   if (this->src.scope() == "local.fragment" &&
       this->dst.scope() == "local.fragment") {
     LOG(FATAL) << "CumSum for fragment not implemented, please raise an issue "
@@ -346,12 +531,23 @@ Stmt CumSumOp::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
     ICHECK(this->dst.scope() == "shared.dyn" || this->dst.scope() == "shared");
     std::stringstream ss;
     auto threads = T.thread_bounds->extent;
-    ss << "tl::CumSum2D<" << threads << ", " << dim << ", "
-       << (reverse ? "true" : "false") << ">::run";
-    Array<PrimExpr> args = {StringImm(ss.str()), src.access_ptr(1),
-                            dst.access_ptr(3)};
-    for (int i = 0; i < src->shape.size(); i++) {
-      args.push_back(src->shape[i]);
+    Array<PrimExpr> args;
+    int ndim = static_cast<int>(src->shape.size());
+    if (ndim == 1) {
+      ICHECK_EQ(dim, 0) << "Cumulative sum over a 1D buffer only supports dim "
+                           "= 0.";
+      ss << "tl::CumSum1D<" << threads << ", " << (reverse ? "true" : "false")
+         << ">::run";
+      args = {StringImm(ss.str()), src.access_ptr(1), dst.access_ptr(3),
+              src->shape[0]};
+    } else if (ndim == 2) {
+      ss << "tl::CumSum2D<" << threads << ", " << dim << ", "
+         << (reverse ? "true" : "false") << ">::run";
+      args = {StringImm(ss.str()), src.access_ptr(1), dst.access_ptr(3),
+              src->shape[0], src->shape[1]};
+    } else {
+      LOG(FATAL) << "CumSum currently supports only 1D or 2D buffers, got "
+                 << ndim << "D.";
     }
     return Evaluate(Call(dst->dtype, builtin::call_extern(), args));
   } else {
@@ -362,7 +558,8 @@ Stmt CumSumOp::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
   return Stmt();
 }
 
-LayoutMap CumSumOp::InferLayout(const LayoutInferArgs &T, InferLevel level) {
+LayoutMap CumSumOpNode::InferLayout(const LayoutInferArgs &T,
+                                    InferLevel level) const {
   return {};
 }
 
@@ -371,4 +568,4 @@ TIR_REGISTER_TL_OP(CumSumOp, cumsum)
     .set_attr<TCallEffectKind>("TCallEffectKind",
                                Integer(CallEffectKind::kOpaque));
 } // namespace tl
-} // namespace tvm
\ No newline at end of file
+} // namespace tvm
diff --git a/src/op/reduce.h b/src/op/reduce.h
index 381f64e6..853d6e0d 100644
--- a/src/op/reduce.h
+++ b/src/op/reduce.h
@@ -1,54 +1,177 @@
 /*!
  * \file tl/op/reduce.h
- * \brief Define reduce operator.
- *
+ * \brief Reduction operators for tensor computations
  */
 
 #ifndef TVM_TL_OP_REDUCE_H_
 #define TVM_TL_OP_REDUCE_H_
 
-#include "op.h"
+#include "operator.h"
 
 namespace tvm {
+
 namespace tl {
 
 using namespace tir;
 
-class ReduceOp : public Operator {
+/// Supported reduction operation types
+enum class ReduceTypeEnum : uint8_t {
+  kSum,    ///< Sum reduction
+  kAbsSum, ///< Absolute sum reduction
+  kMax,    ///< Maximum value reduction
+  kMin,    ///< Minimum value reduction
+  kAbsMax, ///< Maximum absolute value reduction
+  kBitAnd, ///< Bitwise and reduction
+  kBitOr,  ///< Bitwise or reduction
+  kBitXor, ///< Bitwise xor reduction
+};
+
+/// Node class representing a reduction type
+class ReduceTypeNode : public Object {
 public:
-  ReduceOp(Array<PrimExpr> args, BufferMap vmap);
-  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const final;
-  LayoutMap InferLayout(const LayoutInferArgs &T, InferLevel level) final;
+  int type{-1}; ///< Internal type identifier
+  static constexpr const char *_type_key = "tl.ReduceType";
+  TVM_DECLARE_FINAL_OBJECT_INFO(ReduceTypeNode, Object);
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<ReduceTypeNode>().def_ro("type", &ReduceTypeNode::type);
+  }
+
+  bool SEqualReduce(const ReduceTypeNode *other, SEqualReducer equal) const {
+    return equal(type, other->type);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const { hash_reduce(type); }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  /// Type checking methods
+  bool isSum() const { return type == int(ReduceTypeEnum::kSum); }
+  bool isAbsSum() const { return type == int(ReduceTypeEnum::kAbsSum); }
+  bool isMax() const { return type == int(ReduceTypeEnum::kMax); }
+  bool isMin() const { return type == int(ReduceTypeEnum::kMin); }
+  bool isAbsMax() const { return type == int(ReduceTypeEnum::kAbsMax); }
+  bool isBitAnd() const { return type == int(ReduceTypeEnum::kBitAnd); }
+  bool isBitOr() const { return type == int(ReduceTypeEnum::kBitOr); }
+  bool isBitXor() const { return type == int(ReduceTypeEnum::kBitXor); }
+};
+
+/// Wrapper class for reduction type with string-based construction
+class ReduceType : public ObjectRef {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(ReduceType, ObjectRef, ReduceTypeNode);
+  TVM_DLL ReduceType(std::string type) {
+    auto node = make_object<ReduceTypeNode>();
+    if (type == "sum") {
+      node->type = int(ReduceTypeEnum::kSum);
+    } else if (type == "abssum") {
+      node->type = int(ReduceTypeEnum::kAbsSum);
+    } else if (type == "max") {
+      node->type = int(ReduceTypeEnum::kMax);
+    } else if (type == "absmax") {
+      node->type = int(ReduceTypeEnum::kAbsMax);
+    } else if (type == "min") {
+      node->type = int(ReduceTypeEnum::kMin);
+    } else if (type == "bitand") {
+      node->type = int(ReduceTypeEnum::kBitAnd);
+    } else if (type == "bitor") {
+      node->type = int(ReduceTypeEnum::kBitOr);
+    } else if (type == "bitxor") {
+      node->type = int(ReduceTypeEnum::kBitXor);
+    } else {
+      LOG(FATAL) << "Invalid reduce type: " << type;
+    }
+    data_ = std::move(node);
+  }
+};
+
+/// Node class for reduction operations
+class ReduceOpNode : public TileOperatorNode {
+public:
+  tir::Buffer src, dst; ///< Source and destination buffers
+  int dim;              ///< Dimension to reduce along
+  ReduceType type;      ///< Type of reduction operation
+  bool clear;           ///< Whether to clear destination before reduction
+
+  static constexpr const char *_type_key = "tl.ReduceOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(ReduceOpNode, TileOperatorNode);
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<ReduceOpNode>()
+        .def_ro("src", &ReduceOpNode::src)
+        .def_ro("dst", &ReduceOpNode::dst)
+        .def_ro("dim", &ReduceOpNode::dim)
+        .def_ro("type", &ReduceOpNode::type)
+        .def_ro("clear", &ReduceOpNode::clear);
+  }
+
+  bool SEqualReduce(const ReduceOpNode *other, SEqualReducer equal) const {
+    return equal(src, other->src) && equal(dst, other->dst) &&
+           equal(dim, other->dim) && equal(type, other->type) &&
+           equal(clear, other->clear);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(src);
+    hash_reduce(dst);
+    hash_reduce(dim);
+    hash_reduce(type);
+    hash_reduce(clear);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  /// Lower the operator to TIR statements
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  /// Infer memory layout for buffers
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
   static const Op &Get();
+  TileOperator Clone() const;
 
 private:
-  tir::Buffer src, dst;
-  int dim;
-  enum class ReduceType {
-    kSum,
-    kAbsSum,
-    kMax,
-    kMin,
-    kAbsMax,
-  } type;
-  bool clear;
-
+  /// Generate initial value for reduction
   PrimExpr MakeInitValue() const;
+  /// Generate reduction expression
   PrimExpr MakeReduce(const PrimExpr &a, const PrimExpr &b) const;
+  /// Generate codegen reducer string
   std::string MakeCodegenReducer() const;
 };
 
-class CumSumOp : public Operator {
+/// Wrapper class for reduction operations
+class ReduceOp : public TileOperator {
 public:
-  CumSumOp(Array<PrimExpr> args, BufferMap vmap);
-  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const final;
-  LayoutMap InferLayout(const LayoutInferArgs &T, InferLevel level) final;
+  TVM_DEFINE_OBJECT_REF_METHODS(ReduceOp, TileOperator, ReduceOpNode);
+  TVM_DLL ReduceOp(Array<PrimExpr> args, BufferMap vmap);
   static const Op &Get();
+};
 
-private:
-  tir::Buffer src, dst;
-  int dim;
-  bool reverse;
+/// Node class for cumulative sum operations
+class CumSumOpNode : public TileOperatorNode {
+public:
+  tir::Buffer src, dst; ///< Source and destination buffers
+  int dim;              ///< Dimension along which to compute cumulative sum
+  bool reverse;         ///< Whether to compute in reverse order
+  static constexpr const char *_type_key = "tl.CumSumOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(CumSumOpNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+  static const Op &Get();
+  TileOperator Clone() const;
+};
+
+/// Wrapper class for cumulative sum operations
+class CumSumOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(CumSumOp, TileOperator, CumSumOpNode);
+  TVM_DLL CumSumOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
 };
 
 } // namespace tl
diff --git a/src/op/region.cc b/src/op/region.cc
new file mode 100644
index 00000000..95a0b429
--- /dev/null
+++ b/src/op/region.cc
@@ -0,0 +1,122 @@
+/*!
+ * \file tl/op/region.cc
+ * \brief Define region operator.
+ *
+ */
+
+#include "region.h"
+#include <tvm/tir/op.h>
+
+namespace tvm {
+namespace tl {
+using namespace tir;
+
+/**
+ * @brief Construct a RegionOp from TL operator arguments.
+ *
+ * Parses the TL `region` operator call arguments to populate the RegionOpNode:
+ * - Expects args[0] to be a `BufferLoad` whose `indices` are the per-dimension
+ * minima.
+ * - args[1] must be a constant integer used as the access mask.
+ * - args[2 + i] provides the extent for dimension `i`.
+ *
+ * The constructor validates that the number of load indices equals `args.size()
+ * - 2` and will abort via ICHECK on mismatch or if args[0] is not a
+ * `BufferLoad`.
+ *
+ * Parameters:
+ * - args: TL operator call arguments in the form
+ *     [BufferLoad(min_i...), access_mask, extent_0, extent_1, ...,
+ * extent_{n-1}] where n = number of dimensions.
+ * - vmap: BufferMap passed through by the caller (not documented here as a
+ * generic utility).
+ */
+RegionOp::RegionOp(Array<PrimExpr> args, BufferMap vmap) {
+  size_t n = args.size();
+  size_t ndim = n - 2;
+  auto load = args[0].as<BufferLoadNode>();
+  ICHECK(load);
+  ICHECK(load->indices.size() == ndim)
+      << "load->indices.size() = " << load->indices << " ndim = " << ndim;
+  Array<Range> ranges;
+  for (size_t i = 0; i < ndim; i++) {
+    PrimExpr min = load->indices[i];
+    PrimExpr extent = args[2 + i];
+    ranges.push_back(Range::FromMinExtent(min, extent));
+  }
+  ObjectPtr<RegionOpNode> node = make_object<RegionOpNode>();
+  node->buffer_ = load->buffer;
+  node->access_mask_ = static_cast<int>(*as_const_int(args[1]));
+  node->ranges_ = ranges;
+  data_ = std::move(node);
+}
+
+/**
+ * @brief Create a copy of this RegionOpNode and return it as a TileOperator.
+ *
+ * @return TileOperator A new TileOperator that owns a copied RegionOpNode.
+ */
+TileOperator RegionOpNode::Clone() const {
+  auto op = make_object<RegionOpNode>(*this);
+  return RegionOp(op);
+}
+
+/**
+ * @brief Check whether the region spans the entire underlying buffer.
+ *
+ * Returns true if for every dimension the range minimum is zero and the
+ * range extent is structurally equal to the corresponding buffer shape
+ * dimension. Otherwise returns false.
+ *
+ * @return true if the region covers the full buffer in all dimensions; false
+ * otherwise.
+ */
+bool RegionOpNode::IsFullRegion() const {
+  for (size_t i = 0; i < ranges_.size(); i++) {
+    if (!is_zero(ranges_[i]->min))
+      return false;
+    if (!StructuralEqual()(ranges_[i]->extent, buffer_->shape[i]))
+      return false;
+  }
+  return true;
+}
+
+/**
+ * @brief Lower the region operator to a TIR statement.
+ *
+ * Lowers this RegionOpNode into a TIR Stmt by delegating to the operator's
+ * evaluation path (currently `Evaluate(0)`).
+ *
+ * @param T Lowering context (provides buffers, producers/consumers and other
+ *          environment required for lowering).
+ * @param analyzer Optional arithmetic analyzer used for simplification during
+ *                 lowering.
+ * @return Stmt The lowered TIR statement representing this region operation.
+ */
+Stmt RegionOpNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  return Evaluate(0);
+}
+
+/**
+ * @brief Infers data layout for the region operator.
+ *
+ * This operator does not provide any layout inference; the function always
+ * returns an empty LayoutMap regardless of the provided arguments or inference
+ * level.
+ *
+ * @param T Layout inference arguments (ignored).
+ * @param level Inference granularity level (ignored).
+ * @return LayoutMap Empty map indicating no inferred layouts.
+ */
+LayoutMap RegionOpNode::InferLayout(const LayoutInferArgs &T,
+                                    InferLevel level) const {
+  return {};
+}
+
+TIR_REGISTER_TL_OP(RegionOp, region)
+    .set_num_inputs(-1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kPure));
+
+} // namespace tl
+} // namespace tvm
diff --git a/src/op/region.h b/src/op/region.h
new file mode 100644
index 00000000..2d3c9d8e
--- /dev/null
+++ b/src/op/region.h
@@ -0,0 +1,131 @@
+/*!
+ * \file tl/op/op.h
+ * \brief Tile library operations.
+ *
+ */
+
+#ifndef TVM_TL_OP_REGION_H_
+#define TVM_TL_OP_REGION_H_
+
+#include "./operator.h"
+#include <tvm/arith/analyzer.h>
+#include <tvm/ir/op.h>
+#include <tvm/target/target.h>
+#include <tvm/tir/buffer.h>
+
+/**
+ * Tile operator representing a memory region (buffer + ranges) used by TL
+ * passes.
+ *
+ * Encapsulates the target tir::Buffer, the region extents as an Array<Range>,
+ * and an access mask that indicates permitted or intended accesses for lowering
+ * and layout inference.
+ */
+
+/**
+ * Lower this RegionOp into a TIR statement representing the region access.
+ *
+ * @param T Lowering-time arguments (e.g., loop/build context and value
+ * mappings).
+ * @param analyzer Arithmetic analyzer used to simplify and reason about
+ * expressions.
+ * @return A tir::Stmt that implements the region access/mutation described by
+ * this operator.
+ */
+
+/**
+ * Infer the layout mapping for this region operator.
+ *
+ * Produces a LayoutMap describing how loop/axis indices map to buffer axes for
+ * layout-aware scheduling and subsequent operators.
+ *
+ * @param T Layout inference arguments (e.g., input layouts and shapes).
+ * @param level The inference detail level to use.
+ * @return A LayoutMap describing inferred mappings for the operator.
+ */
+
+/**
+ * Return true when this RegionOp represents the full buffer region (i.e.,
+ * ranges cover the entire buffer extent).
+ */
+
+/**
+ * Create a shallow copy of this operator as a TileOperator handle.
+ *
+ * @return A TileOperator that references a cloned RegionOpNode.
+ */
+
+/**
+ * Construct a RegionOp from argument expressions and a buffer map.
+ *
+ * @param args Positional expressions used to instantiate the operator
+ * (semantics depend on how RegionOp is invoked in TL pipelines).
+ * @param vmap Mapping from Buffer to replacement Buffer or buffer metadata used
+ * during creation.
+ */
+
+/**
+ * Return the global Op registration for RegionOp.
+ *
+ * @return Reference to the registered tvm::Op describing the RegionOp.
+ */
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+class RegionOpNode : public TileOperatorNode {
+public:
+  Buffer buffer_;
+  Array<Range> ranges_;
+  int access_mask_;
+
+  static constexpr const char *_type_key = "tl.RegionOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(RegionOpNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+
+  const Buffer &GetBuffer() const { return buffer_; }
+  const Array<Range> &GetRanges() const { return ranges_; }
+  int GetAccessMask() const { return access_mask_; }
+  bool IsFullRegion() const;
+
+  TileOperator Clone() const override;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<RegionOpNode>()
+        .def_ro("buffer", &RegionOpNode::buffer_)
+        .def_ro("ranges", &RegionOpNode::ranges_)
+        .def_ro("access_mask", &RegionOpNode::access_mask_);
+  }
+
+  bool SEqualReduce(const RegionOpNode *other, SEqualReducer equal) const {
+    return equal(buffer_, other->buffer_) && equal(ranges_, other->ranges_) &&
+           equal(access_mask_, other->access_mask_);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(buffer_);
+    hash_reduce(ranges_);
+    hash_reduce(access_mask_);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+};
+
+class RegionOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(RegionOp, TileOperator, RegionOpNode);
+  TVM_DLL RegionOp(Array<PrimExpr> args, BufferMap vmap);
+
+  static const Op &Get();
+};
+
+} // namespace tl
+} // namespace tvm
+
+#endif // TVM_TL_OP_REGION_H_
diff --git a/src/op/remote_copy.cc b/src/op/remote_copy.cc
new file mode 100644
index 00000000..fba501e4
--- /dev/null
+++ b/src/op/remote_copy.cc
@@ -0,0 +1,407 @@
+/*!
+ * \file tl/op/remote_copy.cc
+ * \brief Remote copy operators.
+ *
+ */
+
+#include "remote_copy.h"
+
+#include <tvm/tir/builtin.h>
+#include <tvm/tir/op.h>
+#include <tvm/tir/op_attr_types.h>
+
+#include <sstream>
+
+#include "../target/cuda.h"
+#include "../target/utils.h"
+#include "builtin.h"
+#include "distributed.h"
+#include "parallel.h"
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+PrimExpr PutOpNode::get_offset(const BufferLoadNode *load) const {
+  PrimExpr offset = 0;
+  PrimExpr stride = 1;
+  auto buffer_shape = load->buffer->shape;
+  for (int i = load->indices.size() - 1; i >= 0; i--) {
+    offset += load->indices[i] * stride;
+    stride *= buffer_shape[i];
+  }
+  return div(offset * load->dtype.bits(), 8);
+}
+
+PrimExpr PutOpNode::MakeAddress(const Buffer &buffer,
+                                const Array<PrimExpr> &indices) const {
+  return Call(DataType::Handle(), builtin::address_of(),
+              {BufferLoad(buffer, indices)});
+}
+
+PrimExpr PutOpNode::MakeRemappedAddress(const LowerArgs &T,
+                                        const Buffer &buffer,
+                                        const Array<PrimExpr> &indices) const {
+  Buffer remapped = buffer;
+  if (T.buffer_remap.count(buffer)) {
+    remapped = T.buffer_remap[buffer];
+  }
+  return MakeAddress(remapped, indices);
+}
+
+PutOp::PutOp(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<PutOpNode> node = make_object<PutOpNode>();
+  node->src_addr = args[0];
+  node->dst_addr = args[1];
+  ICHECK(node->src_addr.as<CallNode>()) << "src_addr must be a call node";
+  ICHECK(node->src_addr.as<CallNode>()->op.same_as(builtin::address_of()))
+      << "src_addr must be address_of op";
+  ICHECK(node->dst_addr.as<CallNode>()) << "dst_addr must be a call node";
+  ICHECK(node->dst_addr.as<CallNode>()->op.same_as(builtin::address_of()))
+      << "dst_addr must be address_of op";
+
+  const auto *src_load =
+      node->src_addr.as<CallNode>()->args[0].as<BufferLoadNode>();
+  const auto *dst_load =
+      node->dst_addr.as<CallNode>()->args[0].as<BufferLoadNode>();
+  ICHECK(src_load && dst_load) << "address_of must wrap BufferLoad nodes";
+
+  node->src_offset = node->get_offset(src_load);
+  node->dst_offset = node->get_offset(dst_load);
+  node->src_buffer = src_load->buffer;
+  node->dst_buffer = dst_load->buffer;
+  node->src_indices = src_load->indices;
+  node->dst_indices = dst_load->indices;
+
+  node->copy_size = args[2];
+  node->dst_pe = args[3];
+  node->unroll_factor = args[4].as<IntImm>().value()->value;
+  node->scope = args[5].as<StringImm>().value()->value;
+  node->enable_aggressive_vectorize = bool(args[6].as<IntImm>().value()->value);
+  data_ = std::move(node);
+  (void)vmap;
+}
+
+bool PutOpNode::is_distributed() const {
+  return !(dst_pe->IsInstance<IntImmNode>() &&
+           dst_pe.as<IntImmNode>()->value == -1);
+}
+
+Stmt PutOpNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  (void)analyzer;
+  Array<PrimExpr> new_args;
+  std::stringstream ss;
+  if (scope == "warp") {
+    ss << "tl::cp_warp<" << copy_size << ", " << unroll_factor << ", "
+       << (enable_aggressive_vectorize ? "true" : "false") << ">";
+  } else if (scope == "block") {
+    ss << "tl::cp_block<" << copy_size << ">";
+  } else {
+    LOG(FATAL) << "Invalid scope: " << scope;
+  }
+
+  new_args.push_back(StringImm(ss.str()));
+  if (is_distributed()) {
+    PrimExpr dst_addr_expr = MakeRemappedAddress(T, dst_buffer, dst_indices);
+    PrimExpr local_rank = Call(DataType::Int(64), tl::get_rank(), {});
+    PrimExpr local_base_ptr =
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {local_rank});
+    PrimExpr offset_to_base =
+        Sub(Call(DataType::Handle(), tl::get_uintptr_t(), {dst_addr_expr}),
+            local_base_ptr);
+    new_args.push_back(
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {dst_pe}) +
+        offset_to_base);
+  } else {
+    new_args.push_back(MakeRemappedAddress(T, dst_buffer, dst_indices));
+  }
+  new_args.push_back(MakeRemappedAddress(T, src_buffer, src_indices));
+  auto put = Call(DataType::Handle(), builtin::call_extern(), new_args);
+  return Evaluate(put);
+}
+
+LayoutMap PutOpNode::InferLayout(const LayoutInferArgs &T,
+                                 InferLevel level) const {
+  (void)T;
+  (void)level;
+  return {};
+}
+
+TileOperator PutOpNode::Clone() const {
+  auto node = make_object<PutOpNode>(*this);
+  return PutOp(node);
+}
+
+PrimExpr GetOpNode::get_offset(const BufferLoadNode *load) const {
+  PrimExpr offset = 0;
+  PrimExpr stride = 1;
+  auto buffer_shape = load->buffer->shape;
+  for (int i = load->indices.size() - 1; i >= 0; i--) {
+    offset += load->indices[i] * stride;
+    stride *= buffer_shape[i];
+  }
+  return div(offset * load->dtype.bits(), 8);
+}
+
+PrimExpr GetOpNode::MakeAddress(const Buffer &buffer,
+                                const Array<PrimExpr> &indices) const {
+  return Call(DataType::Handle(), builtin::address_of(),
+              {BufferLoad(buffer, indices)});
+}
+
+PrimExpr GetOpNode::MakeRemappedAddress(const LowerArgs &T,
+                                        const Buffer &buffer,
+                                        const Array<PrimExpr> &indices) const {
+  Buffer remapped = buffer;
+  if (T.buffer_remap.count(buffer)) {
+    remapped = T.buffer_remap[buffer];
+  }
+  return MakeAddress(remapped, indices);
+}
+
+GetOp::GetOp(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<GetOpNode> node = make_object<GetOpNode>();
+  node->src_addr = args[0];
+  node->dst_addr = args[1];
+  ICHECK(node->src_addr.as<CallNode>()) << "src_addr must be a call node";
+  ICHECK(node->src_addr.as<CallNode>()->op.same_as(builtin::address_of()))
+      << "src_addr must be address_of op";
+  ICHECK(node->dst_addr.as<CallNode>()) << "dst_addr must be a call node";
+  ICHECK(node->dst_addr.as<CallNode>()->op.same_as(builtin::address_of()))
+      << "dst_addr must be address_of op";
+
+  const auto *src_load =
+      node->src_addr.as<CallNode>()->args[0].as<BufferLoadNode>();
+  const auto *dst_load =
+      node->dst_addr.as<CallNode>()->args[0].as<BufferLoadNode>();
+  ICHECK(src_load && dst_load) << "address_of must wrap BufferLoad nodes";
+
+  node->src_offset = node->get_offset(src_load);
+  node->dst_offset = node->get_offset(dst_load);
+  node->src_buffer = src_load->buffer;
+  node->dst_buffer = dst_load->buffer;
+  node->src_indices = src_load->indices;
+  node->dst_indices = dst_load->indices;
+
+  node->copy_size = args[2];
+  node->src_pe = args[3];
+  node->unroll_factor = args[4].as<IntImm>().value()->value;
+  node->scope = args[5].as<StringImm>().value()->value;
+  node->enable_aggressive_vectorize = bool(args[6].as<IntImm>().value()->value);
+  data_ = std::move(node);
+  (void)vmap;
+}
+
+bool GetOpNode::is_distributed() const {
+  return !(src_pe->IsInstance<IntImmNode>() &&
+           src_pe.as<IntImmNode>()->value == -1);
+}
+
+Stmt GetOpNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  (void)analyzer;
+  Array<PrimExpr> new_args;
+  std::stringstream ss;
+  if (scope == "warp") {
+    ss << "tl::cp_warp<" << copy_size << ", " << unroll_factor << ", "
+       << (enable_aggressive_vectorize ? "true" : "false") << ">";
+  } else if (scope == "block") {
+    ss << "tl::cp_block<" << copy_size << ">";
+  } else {
+    LOG(FATAL) << "Invalid scope: " << scope;
+  }
+
+  new_args.push_back(StringImm(ss.str()));
+  PrimExpr dst_addr_expr = MakeRemappedAddress(T, dst_buffer, dst_indices);
+  new_args.push_back(dst_addr_expr); // Always dst first in tl_templates
+  if (is_distributed()) {
+    PrimExpr src_addr_expr = MakeRemappedAddress(T, src_buffer, src_indices);
+    PrimExpr local_rank = Call(DataType::Int(64), tl::get_rank(), {});
+    PrimExpr local_base_ptr =
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {local_rank});
+    PrimExpr offset_to_base =
+        Sub(Call(DataType::Handle(), tl::get_uintptr_t(), {src_addr_expr}),
+            local_base_ptr);
+    new_args.push_back(
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {src_pe}) +
+        offset_to_base);
+  } else {
+    new_args.push_back(MakeRemappedAddress(T, src_buffer, src_indices));
+  }
+
+  auto get = Call(DataType::Handle(), builtin::call_extern(), new_args);
+  return Evaluate(get);
+}
+
+LayoutMap GetOpNode::InferLayout(const LayoutInferArgs &T,
+                                 InferLevel level) const {
+  (void)T;
+  (void)level;
+  return {};
+}
+
+TileOperator GetOpNode::Clone() const {
+  auto node = make_object<GetOpNode>(*this);
+  return GetOp(node);
+}
+
+StOp::StOp(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<StOpNode> node = make_object<StOpNode>();
+  node->dst = args[0];
+  ICHECK(node->dst.as<CallNode>()) << "dst must be a call node";
+  ICHECK(node->dst.as<CallNode>()->op.same_as(builtin::address_of()))
+      << "dst must be address_of op";
+
+  node->value = args[1];
+  node->sem = args[2].as<IntImm>().value()->value;
+  node->scope = args[3].as<IntImm>().value()->value;
+  node->na = args[4].as<IntImm>().value()->value;
+  node->dst_pe = args[5];
+  data_ = std::move(node);
+  (void)vmap;
+}
+
+bool StOpNode::is_distributed() const {
+  return !(dst_pe->IsInstance<IntImmNode>() &&
+           dst_pe.as<IntImmNode>()->value == -1);
+}
+
+Stmt StOpNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  (void)analyzer;
+  (void)T;
+  Array<PrimExpr> new_args;
+  std::stringstream ss;
+
+  // Map integers to enum literal strings
+  const char *sem_str[] = {"Semantic::WEAK", "Semantic::VOLATILE",
+                           "Semantic::ACQUIRE", "Semantic::RELEASE",
+                           "Semantic::RELAXED"};
+  const char *scope_str[] = {"Scope::CTA", "Scope::GPU", "Scope::SYS"};
+
+  // Build function name: tl::st<Semantic::X, Scope::Y, bool>
+  ss << "tl::st<" << sem_str[sem] << ", " << scope_str[scope] << ", "
+     << (na ? "true" : "false") << ">";
+
+  new_args.push_back(StringImm(ss.str()));
+  if (is_distributed()) {
+    PrimExpr local_rank = Call(DataType::Int(64), tl::get_rank(), {});
+    PrimExpr local_base_ptr =
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {local_rank});
+    PrimExpr offset_to_base = Sub(
+        Call(DataType::Handle(), tl::get_uintptr_t(), {dst}), local_base_ptr);
+    new_args.push_back(
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {dst_pe}) +
+        offset_to_base);
+  } else {
+    new_args.push_back(dst);
+  }
+  new_args.push_back(value);
+
+  auto st = Call(DataType::Handle(), builtin::call_extern(), new_args);
+  return Evaluate(st);
+}
+
+LayoutMap StOpNode::InferLayout(const LayoutInferArgs &T,
+                                InferLevel level) const {
+  (void)T;
+  (void)level;
+  return {};
+}
+
+TileOperator StOpNode::Clone() const {
+  auto node = make_object<StOpNode>(*this);
+  return StOp(node);
+}
+
+LdOp::LdOp(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<LdOpNode> node = make_object<LdOpNode>();
+  node->src = args[0];
+  ICHECK(node->src.as<CallNode>()) << "src must be a call node";
+  ICHECK(node->src.as<CallNode>()->op.same_as(builtin::address_of()))
+      << "src must be address_of op";
+
+  node->value = args[1];
+  node->sem = args[2].as<IntImm>().value()->value;
+  node->scope = args[3].as<IntImm>().value()->value;
+  node->na = args[4].as<IntImm>().value()->value;
+  node->nc = args[5].as<IntImm>().value()->value;
+  node->src_pe = args[6];
+  data_ = std::move(node);
+  (void)vmap;
+}
+
+bool LdOpNode::is_distributed() const {
+  return !(src_pe->IsInstance<IntImmNode>() &&
+           src_pe.as<IntImmNode>()->value == -1);
+}
+
+Stmt LdOpNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  (void)analyzer;
+  (void)T;
+  Array<PrimExpr> new_args;
+  std::stringstream ss;
+
+  // Map integers to enum literal strings
+  const char *sem_str[] = {"Semantic::WEAK", "Semantic::VOLATILE",
+                           "Semantic::ACQUIRE", "Semantic::RELEASE",
+                           "Semantic::RELAXED"};
+  const char *scope_str[] = {"Scope::CTA", "Scope::GPU", "Scope::SYS"};
+
+  // Build function name: tl::ld<Semantic::X, Scope::Y, bool, bool>
+  ss << "tl::ld<" << sem_str[sem] << ", " << scope_str[scope] << ", "
+     << (nc ? "true" : "false") << ", " << (na ? "true" : "false") << ">";
+
+  new_args.push_back(StringImm(ss.str()));
+  if (is_distributed()) {
+    PrimExpr local_rank = Call(DataType::Int(64), tl::get_rank(), {});
+    PrimExpr local_base_ptr =
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {local_rank});
+    PrimExpr offset_to_base = Sub(
+        Call(DataType::Handle(), tl::get_uintptr_t(), {src}), local_base_ptr);
+    new_args.push_back(
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {src_pe}) +
+        offset_to_base);
+  } else {
+    new_args.push_back(src);
+  }
+  new_args.push_back(value);
+
+  auto ld = Call(DataType::Handle(), builtin::call_extern(), new_args);
+  return Evaluate(ld);
+}
+
+LayoutMap LdOpNode::InferLayout(const LayoutInferArgs &T,
+                                InferLevel level) const {
+  (void)T;
+  (void)level;
+  return {};
+}
+
+TileOperator LdOpNode::Clone() const {
+  auto node = make_object<LdOpNode>(*this);
+  return LdOp(node);
+}
+
+TIR_REGISTER_TL_OP(PutOp, put)
+    .set_num_inputs(7)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_REGISTER_TL_OP(GetOp, get)
+    .set_num_inputs(7)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_REGISTER_TL_OP(StOp, st).set_num_inputs(6).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_REGISTER_TL_OP(LdOp, ld).set_num_inputs(7).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TVM_FFI_STATIC_INIT_BLOCK({ PutOpNode::RegisterReflection(); });
+TVM_FFI_STATIC_INIT_BLOCK({ GetOpNode::RegisterReflection(); });
+TVM_FFI_STATIC_INIT_BLOCK({ StOpNode::RegisterReflection(); });
+TVM_FFI_STATIC_INIT_BLOCK({ LdOpNode::RegisterReflection(); });
+
+} // namespace tl
+} // namespace tvm
diff --git a/src/op/remote_copy.h b/src/op/remote_copy.h
new file mode 100644
index 00000000..3c118f33
--- /dev/null
+++ b/src/op/remote_copy.h
@@ -0,0 +1,325 @@
+/*!
+ * \file tl/op/remote_copy.h
+ * \brief Remote copy operators.
+ *
+ */
+
+#ifndef TVM_TL_OP_BULK_COPY_H_
+#define TVM_TL_OP_BULK_COPY_H_
+
+#include <tvm/target/target.h>
+#include <tvm/tir/stmt_functor.h>
+
+#include "../layout/layout.h"
+#include "operator.h"
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+class PutOpNode : public TileOperatorNode {
+public:
+  PrimExpr src_addr;           ///< Address of the source buffer (address_of)
+  PrimExpr dst_addr;           ///< Address of the destination buffer
+  PrimExpr src_offset;         ///< Byte offset within the source buffer
+  PrimExpr dst_offset;         ///< Byte offset within the destination buffer
+  PrimExpr copy_size;          ///< Number of bytes/elements to copy
+  PrimExpr dst_pe;             ///< Destination processing element (optional)
+  int unroll_factor;           ///< Unroll factor for warp copies
+  Buffer src_buffer;           ///< Source buffer reference
+  Buffer dst_buffer;           ///< Destination buffer reference
+  Array<PrimExpr> src_indices; ///< Source indices used for address computation
+  Array<PrimExpr>
+      dst_indices;   ///< Destination indices used for address computation
+  std::string scope; ///< Scope: {warp, block}
+  bool enable_aggressive_vectorize; ///< Whether to enable aggressive
+                                    ///< vectorization, only effctive for
+                                    ///< warp-scope
+
+  bool is_distributed() const;
+
+  static constexpr const char *_type_key = "tl.PutOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(PutOpNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+  static const Op &Get();
+  TileOperator Clone() const override;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<PutOpNode>()
+        .def_ro("src_addr", &PutOpNode::src_addr)
+        .def_ro("dst_addr", &PutOpNode::dst_addr)
+        .def_ro("copy_size", &PutOpNode::copy_size)
+        .def_ro("dst_pe", &PutOpNode::dst_pe)
+        .def_ro("unroll_factor", &PutOpNode::unroll_factor)
+        .def_ro("src_buffer", &PutOpNode::src_buffer)
+        .def_ro("dst_buffer", &PutOpNode::dst_buffer)
+        .def_ro("src_indices", &PutOpNode::src_indices)
+        .def_ro("dst_indices", &PutOpNode::dst_indices)
+        .def_ro("scope", &PutOpNode::scope);
+  }
+
+  bool SEqualReduce(const PutOpNode *other, SEqualReducer equal) const {
+    return equal(src_addr, other->src_addr) &&
+           equal(dst_addr, other->dst_addr) &&
+           equal(src_offset, other->src_offset) &&
+           equal(dst_offset, other->dst_offset) &&
+           equal(copy_size, other->copy_size) && equal(dst_pe, other->dst_pe) &&
+           equal(unroll_factor, other->unroll_factor) &&
+           equal(src_buffer, other->src_buffer) &&
+           equal(dst_buffer, other->dst_buffer) &&
+           equal(src_indices, other->src_indices) &&
+           equal(dst_indices, other->dst_indices) && scope == other->scope;
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(src_addr);
+    hash_reduce(dst_addr);
+    hash_reduce(src_offset);
+    hash_reduce(dst_offset);
+    hash_reduce(copy_size);
+    hash_reduce(dst_pe);
+    hash_reduce(unroll_factor);
+    hash_reduce(src_buffer);
+    hash_reduce(dst_buffer);
+    hash_reduce(src_indices);
+    hash_reduce(dst_indices);
+    hash_reduce(scope);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  PrimExpr get_offset(const BufferLoadNode *load) const;
+
+private:
+  PrimExpr MakeAddress(const Buffer &buffer,
+                       const Array<PrimExpr> &indices) const;
+  PrimExpr MakeRemappedAddress(const LowerArgs &T, const Buffer &buffer,
+                               const Array<PrimExpr> &indices) const;
+};
+
+class PutOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(PutOp, TileOperator, PutOpNode);
+  TVM_DLL PutOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+class GetOpNode : public TileOperatorNode {
+public:
+  PrimExpr src_addr;           ///< Remote source buffer address
+  PrimExpr dst_addr;           ///< Local destination buffer address
+  PrimExpr src_offset;         ///< Byte offset within the source buffer
+  PrimExpr dst_offset;         ///< Byte offset within the destination buffer
+  PrimExpr copy_size;          ///< Number of bytes/elements to copy
+  PrimExpr src_pe;             ///< Source processing element (optional)
+  int unroll_factor;           ///< Unroll factor for warp copies
+  Buffer src_buffer;           ///< Source buffer reference
+  Buffer dst_buffer;           ///< Destination buffer reference
+  Array<PrimExpr> src_indices; ///< Source indices used for address computation
+  Array<PrimExpr>
+      dst_indices;   ///< Destination indices used for address computation
+  std::string scope; ///< Scope: {warp, block}
+  bool enable_aggressive_vectorize; ///< Whether to enable aggressive
+                                    ///< vectorization, only effctive for
+                                    ///< warp-scope
+
+  bool is_distributed() const;
+
+  static constexpr const char *_type_key = "tl.GetOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(GetOpNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+  static const Op &Get();
+  TileOperator Clone() const override;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<GetOpNode>()
+        .def_ro("src_addr", &GetOpNode::src_addr)
+        .def_ro("dst_addr", &GetOpNode::dst_addr)
+        .def_ro("copy_size", &GetOpNode::copy_size)
+        .def_ro("src_pe", &GetOpNode::src_pe)
+        .def_ro("unroll_factor", &GetOpNode::unroll_factor)
+        .def_ro("src_buffer", &GetOpNode::src_buffer)
+        .def_ro("dst_buffer", &GetOpNode::dst_buffer)
+        .def_ro("src_indices", &GetOpNode::src_indices)
+        .def_ro("dst_indices", &GetOpNode::dst_indices)
+        .def_ro("scope", &GetOpNode::scope);
+  }
+
+  bool SEqualReduce(const GetOpNode *other, SEqualReducer equal) const {
+    return equal(src_addr, other->src_addr) &&
+           equal(dst_addr, other->dst_addr) &&
+           equal(src_offset, other->src_offset) &&
+           equal(dst_offset, other->dst_offset) &&
+           equal(copy_size, other->copy_size) && equal(src_pe, other->src_pe) &&
+           equal(unroll_factor, other->unroll_factor) &&
+           equal(src_buffer, other->src_buffer) &&
+           equal(dst_buffer, other->dst_buffer) &&
+           equal(src_indices, other->src_indices) &&
+           equal(dst_indices, other->dst_indices) && scope == other->scope;
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(src_addr);
+    hash_reduce(dst_addr);
+    hash_reduce(src_offset);
+    hash_reduce(dst_offset);
+    hash_reduce(copy_size);
+    hash_reduce(src_pe);
+    hash_reduce(unroll_factor);
+    hash_reduce(src_buffer);
+    hash_reduce(dst_buffer);
+    hash_reduce(src_indices);
+    hash_reduce(dst_indices);
+    hash_reduce(scope);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  PrimExpr get_offset(const BufferLoadNode *load) const;
+
+private:
+  PrimExpr MakeAddress(const Buffer &buffer,
+                       const Array<PrimExpr> &indices) const;
+  PrimExpr MakeRemappedAddress(const LowerArgs &T, const Buffer &buffer,
+                               const Array<PrimExpr> &indices) const;
+};
+
+class GetOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(GetOp, TileOperator, GetOpNode);
+  TVM_DLL GetOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+class StOpNode : public TileOperatorNode {
+public:
+  PrimExpr dst;    ///< Destination address
+  PrimExpr value;  ///< Value to store
+  PrimExpr dst_pe; ///< Destination processing element (optional)
+  int scope;
+  int sem;
+  int na;
+
+  bool is_distributed() const;
+
+  static constexpr const char *_type_key = "tl.StOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(StOpNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+  static const Op &Get();
+  TileOperator Clone() const override;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<StOpNode>()
+        .def_ro("dst", &StOpNode::dst)
+        .def_ro("value", &StOpNode::value)
+        .def_ro("dst_pe", &StOpNode::dst_pe)
+        .def_ro("scope", &StOpNode::scope)
+        .def_ro("sem", &StOpNode::sem)
+        .def_ro("na", &StOpNode::na);
+  }
+
+  bool SEqualReduce(const StOpNode *other, SEqualReducer equal) const {
+    return equal(dst, other->dst) && equal(value, other->value) &&
+           equal(dst_pe, other->dst_pe) && scope == other->scope &&
+           sem == other->sem && na == other->na;
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(dst);
+    hash_reduce(value);
+    hash_reduce(dst_pe);
+    hash_reduce(scope);
+    hash_reduce(sem);
+    hash_reduce(na);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+};
+
+class StOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(StOp, TileOperator, StOpNode);
+  TVM_DLL StOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+class LdOpNode : public TileOperatorNode {
+public:
+  PrimExpr src;    ///< Source address
+  PrimExpr value;  ///< Value to store
+  PrimExpr src_pe; ///< Source PE (optional)
+  int scope;
+  int sem;
+  int na;
+  int nc;
+
+  bool is_distributed() const;
+
+  static constexpr const char *_type_key = "tl.LdOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(LdOpNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+  static const Op &Get();
+  TileOperator Clone() const override;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<LdOpNode>()
+        .def_ro("src", &LdOpNode::src)
+        .def_ro("value", &LdOpNode::value)
+        .def_ro("src_pe", &LdOpNode::src_pe)
+        .def_ro("scope", &LdOpNode::scope)
+        .def_ro("sem", &LdOpNode::sem)
+        .def_ro("na", &LdOpNode::na)
+        .def_ro("nc", &LdOpNode::nc);
+  }
+
+  bool SEqualReduce(const LdOpNode *other, SEqualReducer equal) const {
+    return equal(src, other->src) && equal(value, other->value) &&
+           equal(src_pe, other->src_pe) && scope == other->scope &&
+           sem == other->sem && na == other->na && nc == other->nc;
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(src);
+    hash_reduce(value);
+    hash_reduce(src_pe);
+    hash_reduce(scope);
+    hash_reduce(sem);
+    hash_reduce(na);
+    hash_reduce(nc);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+};
+
+class LdOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(LdOp, TileOperator, LdOpNode);
+  TVM_DLL LdOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+} // namespace tl
+} // namespace tvm
+
+#endif //  TVM_TL_OP_BULK_COPY_H_
diff --git a/src/op/sync.cc b/src/op/sync.cc
new file mode 100644
index 00000000..892fc222
--- /dev/null
+++ b/src/op/sync.cc
@@ -0,0 +1,217 @@
+/*!
+ * \file tl/op/sync.cc
+ * \brief Synchronization intrinsics.
+ *
+ */
+
+#include "sync.h"
+
+#include <tvm/tir/builtin.h>
+#include <tvm/tir/op.h>
+#include <tvm/tir/op_attr_types.h>
+
+#include "distributed.h"
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+PrimExpr BarrierBlocksOpNode::get_offset(const BufferLoadNode *load) const {
+  PrimExpr offset = 0;
+  PrimExpr stride = 1;
+  auto buffer_shape = load->buffer->shape;
+  for (int i = load->indices.size() - 1; i >= 0; i--) {
+    offset += load->indices[i] * stride;
+    stride *= buffer_shape[i];
+  }
+  return div(offset * load->dtype.bits(), 8);
+}
+
+#define TIR_DEFINE_TL_BUILTIN(OpName)                                          \
+  const Op &OpName() {                                                         \
+    static const Op &op = Op::Get("tl." #OpName);                              \
+    return op;                                                                 \
+  }                                                                            \
+  TVM_REGISTER_OP("tl." #OpName)                                               \
+      .set_attr<TScriptPrinterName>("TScriptPrinterName", #OpName)
+
+TIR_DEFINE_TL_BUILTIN(init_barrier_gpu)
+    .set_num_inputs(2)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(arrive_barrier_gpu)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(wait_barrier_gpu)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(wait_eq).set_num_inputs(2).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(sync_barrier_gpu)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(sync_grid).set_num_inputs(1).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+BarrierBlocksOp::BarrierBlocksOp(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<BarrierBlocksOpNode> node = make_object<BarrierBlocksOpNode>();
+  node->local_bar_addr = args[0];
+  node->need_fence = bool(args[1].as<IntImmNode>()->value);
+  const auto *call = node->local_bar_addr.as<CallNode>();
+  ICHECK(call) << "local_bar_addr must be a call node";
+  ICHECK(call->op.same_as(builtin::address_of()))
+      << "local_bar_addr must be address_of op";
+
+  const auto *load = call->args[0].as<BufferLoadNode>();
+  ICHECK(load) << "address_of argument must be a BufferLoad";
+  node->offset = node->get_offset(load);
+  node->local_bar = load->buffer;
+  node->local_indices = load->indices;
+  data_ = std::move(node);
+  (void)vmap;
+}
+
+Stmt BarrierBlocksOpNode::Lower(const LowerArgs &T,
+                                arith::Analyzer *analyzer) const {
+  (void)analyzer;
+  Array<PrimExpr> new_args;
+  std::stringstream ss;
+  ss << "tl::barrier_blocks";
+  if (!need_fence) {
+    ss << "<false>";
+  }
+  new_args.push_back(StringImm(ss.str()));
+
+  PrimExpr bar_addr = MakeLocalBarAddr(T);
+  PrimExpr rank = Call(DataType::Int(64), tl::get_rank(), {});
+  PrimExpr num_ranks = Call(DataType::Int(64), tl::get_num_ranks(), {});
+  PrimExpr local_base_ptr =
+      Call(DataType::Handle(), tl::get_remote_base_ptr(), {rank});
+  PrimExpr offset_to_base =
+      Sub(Call(DataType::Handle(), tl::get_uintptr_t(), {bar_addr}),
+          local_base_ptr);
+
+  new_args.push_back(offset_to_base);
+  new_args.push_back(rank);
+  new_args.push_back(num_ranks);
+
+  auto barrier_blocks =
+      Call(DataType::Handle(), builtin::call_extern(), new_args);
+  return Evaluate(barrier_blocks);
+}
+
+LayoutMap BarrierBlocksOpNode::InferLayout(const LayoutInferArgs &T,
+                                           InferLevel level) const {
+  (void)T;
+  (void)level;
+  return {};
+}
+
+TileOperator BarrierBlocksOpNode::Clone() const {
+  auto node = make_object<BarrierBlocksOpNode>(*this);
+  return BarrierBlocksOp(node);
+}
+
+PrimExpr BarrierBlocksOpNode::MakeLocalBarAddr(const LowerArgs &T) const {
+  const auto *call = local_bar_addr.as<CallNode>();
+  ICHECK(call && call->op.same_as(builtin::address_of()))
+      << "local_bar_addr must remain an address_of call";
+  const auto *load = call->args[0].as<BufferLoadNode>();
+  ICHECK(load) << "address_of must wrap a BufferLoad";
+  Buffer buffer = load->buffer;
+  if (T.buffer_remap.count(buffer)) {
+    buffer = T.buffer_remap[buffer];
+  }
+  return Call(DataType::Handle(), builtin::address_of(),
+              {BufferLoad(buffer, local_indices)});
+}
+
+WaitOp::WaitOp(Array<PrimExpr> args, BufferMap vmap) {
+  ObjectPtr<WaitOpNode> node = make_object<WaitOpNode>();
+  node->relation = args[0].as<IntImmNode>()->value;
+  node->addr = args[1];
+  node->expected = args[2];
+  node->peer = args[3];
+  data_ = std::move(node);
+  (void)vmap;
+}
+
+bool WaitOpNode::is_distributed() const {
+  return !(peer->IsInstance<IntImmNode>() &&
+           peer.as<IntImmNode>()->value == -1);
+}
+
+Stmt WaitOpNode::Lower(const LowerArgs &T, arith::Analyzer *analyzer) const {
+  (void)analyzer;
+  (void)T;
+  Array<PrimExpr> new_args;
+  std::stringstream ss;
+
+  // Map relation as int to literal_strings
+  const char *relation_str[] = {"eq", "ne", "ge", "le", "gt", "lt"};
+  ss << "tl::wait_" << relation_str[relation];
+
+  new_args.push_back(StringImm(ss.str()));
+  if (is_distributed()) {
+    PrimExpr local_rank = Call(DataType::Int(64), tl::get_rank(), {});
+    PrimExpr local_base_ptr =
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {local_rank});
+    PrimExpr offset_to_base = Sub(
+        Call(DataType::Handle(), tl::get_uintptr_t(), {addr}), local_base_ptr);
+    new_args.push_back(
+        Call(DataType::Handle(), tl::get_remote_base_ptr(), {peer}) +
+        offset_to_base);
+  } else {
+    new_args.push_back(addr);
+  }
+  new_args.push_back(expected);
+
+  auto wait = Call(DataType::Handle(), builtin::call_extern(), new_args);
+  return Evaluate(wait);
+}
+
+LayoutMap WaitOpNode::InferLayout(const LayoutInferArgs &T,
+                                  InferLevel level) const {
+  (void)T;
+  (void)level;
+  return {};
+}
+
+TileOperator WaitOpNode::Clone() const {
+  auto node = make_object<WaitOpNode>(*this);
+  return WaitOp(node);
+}
+
+TIR_REGISTER_TL_OP(BarrierBlocksOp, barrier_blocks)
+    .set_num_inputs(1)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_REGISTER_TL_OP(WaitOp, wait)
+    .set_num_inputs(4)
+    .set_attr<TCallEffectKind>("TCallEffectKind",
+                               Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(fence_cta).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(fence_gpu).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TIR_DEFINE_TL_BUILTIN(fence_sys).set_num_inputs(0).set_attr<TCallEffectKind>(
+    "TCallEffectKind", Integer(CallEffectKind::kOpaque));
+
+TVM_FFI_STATIC_INIT_BLOCK({ BarrierBlocksOpNode::RegisterReflection(); });
+TVM_FFI_STATIC_INIT_BLOCK({ WaitOpNode::RegisterReflection(); });
+
+} // namespace tl
+} // namespace tvm
diff --git a/src/op/sync.h b/src/op/sync.h
new file mode 100644
index 00000000..16487877
--- /dev/null
+++ b/src/op/sync.h
@@ -0,0 +1,214 @@
+/*!
+ * \file tl/op/sync.h
+ * \brief Synchronization intrinsics.
+ *
+ */
+
+#ifndef TVM_TL_OP_SYNC_H_
+#define TVM_TL_OP_SYNC_H_
+
+#include <tvm/target/target.h>
+#include <tvm/tir/stmt_functor.h>
+
+#include "operator.h"
+
+namespace tvm {
+namespace tl {
+
+using namespace tir;
+
+/*!
+ * \brief Initialize a barrier for GPU-level synchronization
+ *
+ * void init_barrier_gpu(barrier, expected)
+ */
+TVM_DLL const Op &init_barrier_gpu();
+
+/*!
+ * \brief Arrive at a barrier for GPU-level synchronization
+ *
+ * void arrive_barrier_gpu(barrier)
+ */
+TVM_DLL const Op &arrive_barrier_gpu();
+
+/*!
+ * \brief Wait at a barrier for GPU-level synchronization
+ *
+ * void wait_barrier_gpu(barrier)
+ */
+TVM_DLL const Op &wait_barrier_gpu();
+
+/*!
+ * \brief Wait until *addr == expected* for GPU-level synchronization
+ * void wait_eq(addr, expected)
+ */
+
+TVM_DLL const Op &wait_eq();
+
+/*!
+ * \brief TileOperatorNode for wait operation.
+ *
+ * WaitOpNode represents a wait primitive,
+ * which waits until a condition on a memory address is met.
+ */
+class WaitOpNode : public TileOperatorNode {
+public:
+  PrimExpr addr;     ///< The address to watch.
+  PrimExpr expected; ///< The expected value to compare against.
+  PrimExpr peer;     ///< The peer to compare against.
+  int relation;      ///< The relation to compare against.
+
+  bool is_distributed() const;
+
+  static constexpr const char *_type_key = "tl.WaitOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(WaitOpNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+  static const Op &Get();
+  TileOperator Clone() const override;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<WaitOpNode>()
+        .def_ro("addr", &WaitOpNode::addr)
+        .def_ro("expected", &WaitOpNode::expected)
+        .def_ro("peer", &WaitOpNode::peer)
+        .def_ro("relation", &WaitOpNode::relation);
+  }
+
+  bool SEqualReduce(const WaitOpNode *other, SEqualReducer equal) const {
+    return equal(addr, other->addr) && equal(expected, other->expected) &&
+           equal(peer, other->peer) && equal(relation, other->relation);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(addr);
+    hash_reduce(expected);
+    hash_reduce(peer);
+    hash_reduce(relation);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+};
+
+/*!
+ * \brief Wrapper for the WaitOp operator.
+ */
+class WaitOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(WaitOp, TileOperator, WaitOpNode);
+  TVM_DLL WaitOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+/*!
+ * \brief Synchronize at a barrier for GPU-level synchronization
+ *
+ * void sync_barrier_gpu(barrier)
+ */
+TVM_DLL const Op &sync_barrier_gpu();
+
+/*!
+ * \brief Synchronize at a barrier for GPU-level synchronization in cooperative
+ * group style
+ *
+ * void sync_grid(barrier)
+ */
+TVM_DLL const Op &sync_grid();
+
+/*!
+ * \brief Synchronize all blocks at a system-level barrier
+ *
+ * void barrier_blocks(barrier, rank, num_ranks)
+ *
+ */
+class BarrierBlocksOpNode : public TileOperatorNode {
+public:
+  PrimExpr local_bar_addr;       ///< Address expression for the local barrier
+  PrimExpr offset;               ///< Byte offset within the barrier buffer
+  Buffer local_bar;              ///< Local barrier buffer reference
+  Array<PrimExpr> local_indices; ///< Indices used to access the barrier buffer
+  bool need_fence;               ///< Whether need sys-level fence
+
+  static constexpr const char *_type_key = "tl.BarrierBlocksOp";
+  TVM_DECLARE_FINAL_OBJECT_INFO(BarrierBlocksOpNode, TileOperatorNode);
+
+  Stmt Lower(const LowerArgs &T, arith::Analyzer *analyzer) const override;
+  LayoutMap InferLayout(const LayoutInferArgs &T,
+                        InferLevel level) const override;
+  static const Op &Get();
+  TileOperator Clone() const override;
+
+  static void RegisterReflection() {
+    namespace refl = tvm::ffi::reflection;
+    refl::ObjectDef<BarrierBlocksOpNode>()
+        .def_ro("local_bar_addr", &BarrierBlocksOpNode::local_bar_addr)
+        .def_ro("offset", &BarrierBlocksOpNode::offset)
+        .def_ro("local_bar", &BarrierBlocksOpNode::local_bar)
+        .def_ro("local_indices", &BarrierBlocksOpNode::local_indices);
+  }
+
+  bool SEqualReduce(const BarrierBlocksOpNode *other,
+                    SEqualReducer equal) const {
+    return equal(local_bar_addr, other->local_bar_addr) &&
+           equal(offset, other->offset) && equal(local_bar, other->local_bar) &&
+           equal(local_indices, other->local_indices);
+  }
+
+  void SHashReduce(SHashReducer hash_reduce) const {
+    hash_reduce(local_bar_addr);
+    hash_reduce(offset);
+    hash_reduce(local_bar);
+    hash_reduce(local_indices);
+  }
+
+  static constexpr bool _type_has_method_sequal_reduce = true;
+  static constexpr bool _type_has_method_shash_reduce = true;
+
+  PrimExpr get_offset(const BufferLoadNode *load) const;
+
+private:
+  PrimExpr MakeLocalBarAddr(const LowerArgs &T) const;
+};
+
+/*!
+ * \brief Wrapper for the BarrierBlocks operator
+ */
+class BarrierBlocksOp : public TileOperator {
+public:
+  TVM_DEFINE_OBJECT_REF_METHODS(BarrierBlocksOp, TileOperator,
+                                BarrierBlocksOpNode);
+  TVM_DLL BarrierBlocksOp(Array<PrimExpr> args, BufferMap vmap);
+  static const Op &Get();
+};
+
+/*!
+ * \brief Create a memory fence at the block level (visible to all threads in
+ * the current block)
+ *
+ * void fence_cta()
+ */
+TVM_DLL const Op &fence_cta();
+
+/*!
+ * \brief Synchronize all threads at the GPU level (visible to all blocks on the
+ * current device)
+ *
+ * void fence_gpu()
+ */
+TVM_DLL const Op &fence_gpu();
+
+/*!
+ * \brief Synchronize all threads at the system level (visible in a node)
+ *
+ * void fence_sys()
+ */
+TVM_DLL const Op &fence_sys();
+
+} // namespace tl
+} // namespace tvm
+
+#endif // TVM_TL_OP_SYNC_H_
