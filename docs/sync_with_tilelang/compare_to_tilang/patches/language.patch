diff --git a/tilelang/language/__init__.py b/tilelang/language/__init__.py
index c369b101..bb182ae6 100644
--- a/tilelang/language/__init__.py
+++ b/tilelang/language/__init__.py
@@ -1,12 +1,13 @@
 """The language interface for tl programs."""
+from __future__ import annotations
 
-from typing import Optional, Callable, Dict
 # from .parser import *
 # now is fully compatible with the upstream
 # tir script
 # TODO(lei): remove this import once the
 # upstream tir script is fully compatible
 from tvm.script.parser.tir import *
+from . import overrides as _overrides  # noqa: F401
 from .tir import (
     prim_func,  # noqa: F401
 )
@@ -17,6 +18,7 @@ from .proxy import (
     make_tensor,  # noqa: F401
     Buffer,  # noqa: F401
     Tensor,  # noqa: F401
+    StridedTensor,  # noqa: F401
     FragmentBuffer,  # noqa: F401
     SharedBuffer,  # noqa: F401
     LocalBuffer,  # noqa: F401
@@ -25,6 +27,7 @@ from .parallel import Parallel  # noqa: F401
 from .pipeline import Pipelined  # noqa: F401
 from .persistent import Persistent  # noqa: F401
 from .frame import has_let_value, get_let_value  # noqa: F401
+from .math_intrinsics import *  # noqa: F401
 from .kernel import (
     Kernel,  # noqa: F401
     KernelLaunchFrame,  # noqa: F401
@@ -40,9 +43,12 @@ from .allocate import (
     alloc_shared,  # noqa: F401
     alloc_fragment,  # noqa: F401
     alloc_barrier,  # noqa: F401
+    alloc_tmem,  # noqa: F401
+    alloc_reducer,  # noqa: F401
+    alloc_descriptor,  # noqa: F401
 )
 from .copy import copy, c2d_im2col  # noqa: F401
-from .gemm import GemmWarpPolicy, gemm  # noqa: F401
+from .gemm import GemmWarpPolicy, gemm, gemm_v2  # noqa: F401
 from .experimental.gemm_sp import gemm_sp  # noqa: F401
 from .fill import fill, clear  # noqa: F401
 from .reduce import (
@@ -52,10 +58,21 @@ from .reduce import (
     reduce_sum,  # noqa: F401
     reduce_abssum,  # noqa: F401
     reduce_absmax,  # noqa: F401
+    reduce_bitand,  # noqa: F401
+    reduce_bitor,  # noqa: F401
+    reduce_bitxor,  # noqa: F401
     cumsum,  # noqa: F401
+    finalize_reducer,  # noqa: F401
+    warp_reduce_sum,  # noqa: F401
+    warp_reduce_max,  # noqa: F401
+    warp_reduce_min,  # noqa: F401
+    warp_reduce_bitand,  # noqa: F401
+    warp_reduce_bitor,  # noqa: F401
 )
 from .print import print  # noqa: F401
 from .customize import (
+    atomic_max,  # noqa: F401
+    atomic_min,  # noqa: F401
     atomic_add,  # noqa: F401
     atomic_addx2,  # noqa: F401
     atomic_addx4,  # noqa: F401
@@ -63,121 +80,24 @@ from .customize import (
     clamp,  # noqa: F401
     reshape,  # noqa: F401
     view,  # noqa: F401
+    atomic_load,  # noqa: F401
+    atomic_store,  # noqa: F401
+    loop_break,  # noqa: F401
 )
 from .logical import any_of, all_of  # noqa: F401
 from .builtin import *  # noqa: F401
+from .distributed.multi_device.nvshmem import *  # noqa: F401
+from .distributed.multi_device.cpengine import *  # noqa: F401
+from .distributed.common import *  # noqa: F401
 
-from .memscope import *  # noqa: F401
+from .utils import index_to_coordinates  # noqa: F401
 
-
-def symbolic(name: str, dtype: str = "int32"):
-    return tir.Var(name, dtype)
-
-
-def use_swizzle(panel_size: int, order: str = "row", enable: bool = True):
-    # If order is row, use rasterization2DRow, otherwise use rasterization2DColumn
-    # The panel size is the number of threads in a warp
-    # Use to improve the L2 Cache Locality
-    device_func = ("rasterization2DRow" if order == "row" else "rasterization2DColumn")
-    return attr(None, "threadblock_swizzle_pattern",
-                f"tl::{device_func}<{panel_size}>") if enable else None
-
-
-def annotate_layout(layout_map: Dict):
-    """Annotate the layout of the buffer
-
-    Args:
-        layout_map (Dict): a dictionary of buffer to layout
-
-    Returns:
-        block_attr: a block attribute
-    
-    Example:
-        @T.prim_func
-        def main(
-                A: T.Tensor((M, N), dtype),
-                B: T.Tensor((M, N), dtype),
-        ):
-            # Initialize Kernel Context
-            with T.Kernel(T.ceildiv(N, block_N), T.ceildiv(M, block_M), threads=128) as (bx, by):
-                A_shared = T.alloc_shared((block_M, block_N), dtype)
-
-                T.annotate_layout({A_shared: layout})
-                for i, j in T.Parallel(block_M, block_N):
-                    A_shared[i, j] = A[by * block_M + i, bx * block_N + j]
-
-                for i, j in T.Parallel(block_M, block_N):
-                    B[by * block_M + i, bx * block_N + j] = A_shared[i, j]
-
-        return main
-    """
-    # layout_map is a dictionary of buffer to layout
-    _layout_map = {}
-    for buffer, layout in layout_map.items():
-        if isinstance(layout, Layout):
-            _layout_map[buffer.data] = layout
-        elif isinstance(layout, Callable):
-            _layout_map[buffer.data] = Layout(buffer.shape, layout)
-        else:
-            raise ValueError(f"Invalid layout: {layout}")
-
-    return block_attr({"layout_map": _layout_map})
-
-
-def annotate_padding(padding_map: Dict):
-    """Annotate the padding of the buffer
-
-    Args:
-        padding_map (dict): a dictionary of buffer to padding value
-
-    Returns:
-        block_attr: a block attribute
-    
-    Example:
-        @T.prim_func
-        def main(
-                A: T.Tensor((M, N), dtype),
-                B: T.Tensor((M, N), dtype),
-        ):
-            # Initialize Kernel Context
-            with T.Kernel(T.ceildiv(N, block_N), T.ceildiv(M, block_M), threads=128) as (bx, by):
-                A_shared = T.alloc_shared((block_M, block_N), dtype)
-
-                T.annotate_padding({A_shared: pad_value})
-                for i, j in T.Parallel(block_M, block_N):
-                    A_shared[i, j] = A[by * block_M + i - 10, bx * block_N + j]
-
-                for i, j in T.Parallel(block_M, block_N):
-                    B[by * block_M + i, bx * block_N + j] = A_shared[i, j]
-
-        return main
-    """
-    # padding_map is a dictionary of buffer to padding value
-    _padding_map = {}
-    for buffer, padding_value in padding_map.items():
-        # assert not global
-        assert buffer.scope() != "global", "padding can not be applied to global buffers"
-        _padding_map[buffer.data] = padding_value
-    return block_attr({"padding_map": _padding_map})
-
-
-def annotate_l2_hit_ratio(l2_hit_ratio_map: Dict):
-    """Annotate the L2 hit ratio of the buffer, detailed explanation please refer to:
-    https://docs.nvidia.com/cuda/cuda-c-programming-guide/#l2-policy-for-persisting-accesses
-
-    Args:
-        l2_hit_ratio_map (dict): a dictionary of buffer to L2 hit ratio value
-    Example:
-        # 0.5 is the hit ratio
-        T.annotate_l2_hit_ratio({A: 0.5})
-    """
-    _l2_hit_ratio_map = {}
-    for buffer, hit_ratio in l2_hit_ratio_map.items():
-        assert buffer.scope() == "global", "persistent L2 can only be applied to global buffers"
-        _l2_hit_ratio_map[buffer.data] = float(hit_ratio)
-    return block_attr({"l2_hit_ratio_map": _l2_hit_ratio_map})
+from .symbolics import dynamic, symbolic  # noqa: F401
+from .annotations import (  # noqa: F401
+    use_swizzle, annotate_layout, annotate_safe_value, annotate_l2_hit_ratio,
+)
 
 
-def import_source(source: Optional[str] = None):
+def import_source(source: str | None = None):
     # source is the source code to be imported
     return block_attr({"pragma_import_c": source}) if source is not None else None
diff --git a/tilelang/language/allocate.py b/tilelang/language/allocate.py
index 24483911..cde803f0 100644
--- a/tilelang/language/allocate.py
+++ b/tilelang/language/allocate.py
@@ -13,8 +13,12 @@ Available allocation functions:
 Each function takes shape and dtype parameters and returns a TVM buffer object
 with the appropriate memory scope.
 """
+from __future__ import annotations
 
+from tilelang import tvm as tvm
 from tvm.script import tir as T
+from tvm.tir import PrimExpr
+from tvm.script.parser.tir import block_attr
 
 
 def alloc_shared(shape, dtype, scope="shared.dyn"):
@@ -63,17 +67,54 @@ def alloc_fragment(shape, dtype, scope="local.fragment"):
     return T.alloc_buffer(shape, dtype, scope=scope)
 
 
-def alloc_var(dtype, scope="local.var"):
+def alloc_var(dtype, *args, scope="local.var", init: PrimExpr | None = None):
     """Allocate a single-element variable buffer.
 
     Args:
         dtype (str): The data type of the buffer (e.g., 'float32', 'int32')
-        scope (str, optional): The memory scope. Defaults to "local.var"
+        *args: Optional positional arguments. A single positional string is treated
+            as the scope for backward compatibility. A single non-string positional
+            argument (or keyword ``init``) specifies the initializer. When two
+            positional arguments are provided, they are interpreted as
+            ``(init, scope)``.
+        scope (str, optional): The memory scope. Defaults to "local.var".
+            Use as keyword argument for clarity when also providing an initializer.
+        init (PrimExpr, optional): The optional initializer value. When provided,
+            the generated code will initialize the variable with this value instead
+            of defaulting to zero.
 
     Returns:
         T.Buffer: A TVM buffer object allocated as a single-element variable
     """
-    return T.alloc_buffer([1], dtype, scope=scope)
+    parsed_scope = scope
+    parsed_init = init
+
+    if len(args) == 1:
+        arg = args[0]
+        if isinstance(arg, str) and parsed_init is None and scope == "local.var":
+            parsed_scope = arg
+        else:
+            if parsed_init is not None:
+                raise TypeError("Initializer specified multiple times in alloc_var.")
+            parsed_init = arg
+    elif len(args) == 2:
+        if parsed_init is not None:
+            raise TypeError("Initializer specified multiple times in alloc_var.")
+        parsed_init, parsed_scope_arg = args
+        if not isinstance(parsed_scope_arg, str):
+            raise TypeError("Scope must be provided as a string in alloc_var.")
+        parsed_scope = parsed_scope_arg
+    elif len(args) > 2:
+        raise TypeError(
+            f"alloc_var expected at most 3 positional arguments but got {len(args) + 1}.")
+
+    if not isinstance(parsed_scope, str):
+        raise TypeError("Scope must be a string in alloc_var.")
+
+    buffer = T.alloc_buffer([1], dtype, scope=parsed_scope)
+    if parsed_init is not None:
+        block_attr({"tl.local_var_init": {buffer.data: parsed_init}})
+    return buffer
 
 
 def alloc_barrier(arrive_count: int):
@@ -86,3 +127,77 @@ def alloc_barrier(arrive_count: int):
         T.Buffer: A TVM buffer object allocated as a barrier
     """
     return T.alloc_buffer([arrive_count], "uint64", scope="shared.barrier")
+
+
+def alloc_tmem(shape, dtype):
+    """
+    Allocate a Tensor Memory (TMEM) buffer for use with 5th generation Tensor Core operations (e.g., TCGEN5.MMA).
+
+    TMEM is a dedicated on-chip memory introduced in Hopper GPUs, designed to reduce register pressure and enable asynchronous, single-threaded MMA operations. It is organized as a 2D array of 512 columns by 128 rows (lanes), with each cell being 32 bits. Allocation is performed in units of columns, and every lane of a column is allocated together.
+
+    Key properties and requirements:
+        - The number of columns allocated must be a power of 2 and at least 32.
+        - TMEM allocations are dynamic and must be explicitly deallocated.
+        - Both allocation and deallocation must be performed by the same warp.
+        - The base address of the TMEM allocation is stored in shared memory and used as the offset for TCGEN5.MMA accumulator tensors.
+        - Only TCGEN5.MMA and specific TMEM load/store instructions can access TMEM; all pre-processing must occur before data is loaded into TMEM, and all post-processing after data is retrieved.
+        - The number of columns allocated should not increase between any two allocations in the execution order within the CTA.
+
+    Args:
+        num_cols (int): Number of columns to allocate in TMEM. Must be a power of 2 and >= 32 but less than or equal to 512.
+
+    Returns:
+        T.Buffer: A TVM buffer object allocated in TMEM scope, suitable for use as an accumulator or operand in TCGEN5.MMA operations.
+
+    Note:
+        - TMEM is only available on supported architectures (e.g., Hopper and later).
+        - The buffer returned should be used according to TMEM access restrictions and deallocated appropriately.
+    """
+
+    assert len(shape) == 2, "shape must be a 2D tensor for TMEM allocation"
+    return T.alloc_buffer(shape, dtype, scope="shared.tmem")
+
+
+def alloc_reducer(shape, dtype, op="sum", replication=None):
+    """
+    Allocate a reducer buffer.
+
+    Modifications needs to conform with `op`,
+    such as `op="sum"` requires `reducer[...] += ...` and
+    `op="max"` requires `reducer[...] = T.max(reducer[...], ...)`.
+
+    Only after T.fill with proper initializer the reduction may begin;
+    only after T.finalize_reducer the partial results will be available.
+
+    For `op="sum"`, filled value must be 0; for min and max, the filled initializer will become max or min clamper correspondingly.
+    You may want to use `T.max_value` for min and `T.min_value` for max.
+
+    Args:
+        shape (tuple): The shape of the buffer to allocate
+        dtype (str): The data type of the buffer (e.g., 'float32', 'int32')
+        op (str): The reduce operation corresponded with the reducer
+        replication (str | None): Replication strategy, can be "all" or "none". Defaults to not specified, and the compiler will do whatever it want.
+
+    Returns:
+        T.Buffer: A TVM buffer object allocated in thread-private storage, available to reduce values in T.Parallel loops.
+    """
+
+    assert op in ["sum", "max", "min"]
+    # TODO: support automatic layout
+    if replication is None:
+        replication = "none"
+    assert replication in ["all", "none"]
+
+    reducer = T.alloc_buffer(shape, dtype, scope="local.fragment")
+    block_attr({"reducer_info": {reducer.data: {"rep": replication, "op": op}}})
+
+    return reducer
+
+
+def alloc_descriptor(dtype="uint64", scope="local.descriptor"):
+    """Allocate a descriptor buffer for wgmma and utcmma.
+
+    Returns:
+        T.Buffer: A TVM buffer object allocated as a descriptor
+    """
+    return T.alloc_buffer([1], dtype, scope=scope)
diff --git a/tilelang/language/annotations.py b/tilelang/language/annotations.py
new file mode 100644
index 00000000..12d3af4d
--- /dev/null
+++ b/tilelang/language/annotations.py
@@ -0,0 +1,53 @@
+"""Annotation helpers exposed on the TileLang language surface."""
+from __future__ import annotations
+
+from typing import Callable
+
+from tilelang.layout import Layout
+from tvm.script.parser.tir import attr, block_attr
+
+__all__ = [
+    "use_swizzle",
+    "annotate_layout",
+    "annotate_safe_value",
+    "annotate_l2_hit_ratio",
+]
+
+
+def use_swizzle(panel_size: int, order: str = "row", enable: bool = True):
+    """Annotate a kernel to use a specific threadblock swizzle pattern."""
+    device_func = "rasterization2DRow" if order == "row" else "rasterization2DColumn"
+    if not enable:
+        return None
+    return attr(None, "threadblock_swizzle_pattern", f"tl::{device_func}<{panel_size}>")
+
+
+def annotate_layout(layout_map: dict):
+    """Annotate the layout of the buffer."""
+    _layout_map = {}
+    for buffer, layout in layout_map.items():
+        if isinstance(layout, Layout):
+            _layout_map[buffer.data] = layout
+        elif isinstance(layout, Callable):
+            _layout_map[buffer.data] = Layout(buffer.shape, layout)
+        else:
+            raise ValueError(f"Invalid layout: {layout}")
+
+    return block_attr({"layout_map": _layout_map})
+
+
+def annotate_safe_value(safe_value_map: dict):
+    """Annotate the safe value of the buffer."""
+    _safe_value_map = {}
+    for buffer, safe_value in safe_value_map.items():
+        _safe_value_map[buffer.data] = safe_value
+    return block_attr({"safe_value_map": _safe_value_map})
+
+
+def annotate_l2_hit_ratio(l2_hit_ratio_map: dict):
+    """Annotate the L2 hit ratio of the buffer."""
+    _l2_hit_ratio_map = {}
+    for buffer, hit_ratio in l2_hit_ratio_map.items():
+        assert buffer.scope() == "global", "persistent L2 can only be applied to global buffers"
+        _l2_hit_ratio_map[buffer.data] = float(hit_ratio)
+    return block_attr({"l2_hit_ratio_map": _l2_hit_ratio_map})
diff --git a/tilelang/language/ast/_ffi_api.py b/tilelang/language/ast/_ffi_api.py
index 96b41de8..518d57ea 100644
--- a/tilelang/language/ast/_ffi_api.py
+++ b/tilelang/language/ast/_ffi_api.py
@@ -17,6 +17,6 @@
 # This file is modified from the original version,
 # which is part of the TVM project (https://tvm.apache.org/).
 """FFI APIs"""
-import tvm._ffi
+import tvm.ffi
 
-tvm._ffi._init_api("script.ir_builder.tir", __name__)  # pylint: disable=protected-access
+tvm.ffi._init_api("script.ir_builder.tir", __name__)  # pylint: disable=protected-access
diff --git a/tilelang/language/ast/ir.py b/tilelang/language/ast/ir.py
index 781b8f48..0948cdfa 100644
--- a/tilelang/language/ast/ir.py
+++ b/tilelang/language/ast/ir.py
@@ -1428,19 +1428,19 @@ float16x64 = func_gen(("Float16x64"))
 float32x64 = func_gen(("Float32x64"))
 float64x64 = func_gen(("Float64x64"))
 
-e4m3_float8 = func_gen(("E4M3Float8"))
-e4m3_float8x4 = func_gen(("E4M3Float8x4"))
-e4m3_float8x8 = func_gen(("E4M3Float8x8"))
-e4m3_float8x16 = func_gen(("E4M3Float8x16"))
-e4m3_float8x32 = func_gen(("E4M3Float8x32"))
-e4m3_float8x64 = func_gen(("E4M3Float8x64"))
-
-e5m2_float8 = func_gen(("E5M2Float8"))
-e5m2_float8x4 = func_gen(("E5M2Float8x4"))
-e5m2_float8x8 = func_gen(("E5M2Float8x8"))
-e5m2_float8x16 = func_gen(("E5M2Float8x16"))
-e5m2_float8x32 = func_gen(("E5M2Float8x32"))
-e5m2_float8x64 = func_gen(("E5M2Float8x64"))
+float8_e4m3 = func_gen(("E4M3Float8"))
+float8_e4m3x4 = func_gen(("E4M3Float8x4"))
+float8_e4m3x8 = func_gen(("E4M3Float8x8"))
+float8_e4m3x16 = func_gen(("E4M3Float8x16"))
+float8_e4m3x32 = func_gen(("E4M3Float8x32"))
+float8_e4m3x64 = func_gen(("E4M3Float8x64"))
+
+float8_e5m2 = func_gen(("E5M2Float8"))
+float8_e5m2x4 = func_gen(("E5M2Float8x4"))
+float8_e5m2x8 = func_gen(("E5M2Float8x8"))
+float8_e5m2x16 = func_gen(("E5M2Float8x16"))
+float8_e5m2x32 = func_gen(("E5M2Float8x32"))
+float8_e5m2x64 = func_gen(("E5M2Float8x64"))
 
 # pylint: enable=invalid-name
 
@@ -1892,6 +1892,8 @@ call_llvm_pure_intrin = _dtype_forward(_tir_op.call_llvm_pure_intrin)
 call_pure_extern = _dtype_forward(_tir_op.call_pure_extern)
 ptx_mma = _dtype_forward(_tir_op.ptx_mma)
 ptx_mma_sp = _dtype_forward(_tir_op.ptx_mma_sp)
+ptx_wgmma_ss = _dtype_forward(_tir_op.ptx_wgmma_ss)
+ptx_wgmma_rs = _dtype_forward(_tir_op.ptx_wgmma_rs)
 ptx_ldmatrix = _dtype_forward(_tir_op.ptx_ldmatrix)
 ptx_cp_async = _dtype_forward(_tir_op.ptx_cp_async)
 ptx_cp_async_bulk = _dtype_forward(_tir_op.ptx_cp_async_bulk)
@@ -1964,33 +1966,33 @@ __all__ = [
     "uint16x64",
     "uint32x64",
     "uint64x64",
-    "e4m3_float8",
-    "e5m2_float8",
+    "float8_e4m3",
+    "float8_e5m2",
     "float16",
     "float32",
     "float64",
-    "e4m3_float8x4",
-    "e5m2_float8x4",
+    "float8_e4m3x4",
+    "float8_e5m2x4",
     "float16x4",
     "float32x4",
     "float64x4",
-    "e4m3_float8x8",
-    "e5m2_float8x8",
+    "float8_e4m3x8",
+    "float8_e5m2x8",
     "float16x8",
     "float32x8",
     "float64x8",
-    "e4m3_float8x16",
-    "e5m2_float8x16",
+    "float8_e4m3x16",
+    "float8_e5m2x16",
     "float16x16",
     "float32x16",
     "float64x16",
-    "e4m3_float8x32",
-    "e5m2_float8x32",
+    "float8_e4m3x32",
+    "float8_e5m2x32",
     "float16x32",
     "float32x32",
     "float64x32",
-    "e4m3_float8x64",
-    "e5m2_float8x64",
+    "float8_e4m3x64",
+    "float8_e5m2x64",
     "float16x64",
     "float32x64",
     "float64x64",
@@ -2141,6 +2143,8 @@ __all__ = [
     "tvm_warp_activemask",
     "ptx_mma",
     "ptx_mma_sp",
+    "ptx_wgmma_ss",
+    "ptx_wgmma_rs",
     "ptx_ldmatrix",
     "ptx_cp_async",
     "ptx_cp_async_bulk",
diff --git a/tilelang/language/atomic.py b/tilelang/language/atomic.py
new file mode 100644
index 00000000..f1b37d23
--- /dev/null
+++ b/tilelang/language/atomic.py
@@ -0,0 +1,402 @@
+# Copyright (c) Tile-AI Corporation.
+# Licensed under the MIT License.
+"""Atomic operations for tilelang."""
+from __future__ import annotations
+
+import tilelang.language as T
+from tvm import ir, tir
+from tvm.tir import PrimExpr, Buffer, BufferRegion, Var, op
+from tilelang.language.utils import buffer_to_tile_region, buffer_region_to_tile_region, buffer_load_to_tile_region
+from tilelang.utils.language import get_buffer_region_from_load
+
+_MEMORY_ORDER_ID_MAP = {
+    "relaxed": 0,
+    "consume": 1,
+    "acquire": 2,
+    "release": 3,
+    "acq_rel": 4,
+    "seq_cst": 5,
+}
+
+
+def atomic_max(dst: Buffer,
+               value: PrimExpr,
+               memory_order: str | None = None,
+               return_prev: bool = False) -> PrimExpr:
+    """
+    Perform an atomic maximum on the value stored at dst with an optional memory-order.
+
+    If memory_order is None the runtime extern "AtomicMax" is called without an explicit memory-order id; otherwise the provided memory_order string is mapped to a numeric id using the module's memory-order map and passed to the extern.
+
+    Parameters:
+        dst (Buffer): Destination buffer/address to apply the atomic max.
+        value (PrimExpr): Value to compare/store atomically.
+        memory_order (Optional[str]): Optional memory-order name (e.g. "relaxed", "acquire", "seq_cst").
+            If provided, it is translated to the corresponding numeric memory-order id before the call.
+        return_prev (bool): If True, return the previous value; if False, return handle (default False).
+
+    Returns:
+        PrimExpr: A handle/expression representing the issued atomic maximum operation, or the previous value if return_prev is True.
+
+    Examples:
+        >>> # Basic atomic max operation
+        >>> counter = T.Tensor([1], "float32", name="counter")
+        >>> atomic_max(counter, 42.0)
+
+        >>> # With memory ordering
+        >>> atomic_max(counter, 100.0, memory_order="acquire")
+
+        >>> # Get the previous value
+        >>> prev_value = atomic_max(counter, 50.0, return_prev=True)
+        >>> # prev_value now contains the value that was in counter before the max operation
+
+        >>> # Use in parallel reduction to find global maximum
+        >>> @T.prim_func
+        >>> def find_max(data: T.Buffer, result: T.Buffer):
+        >>>     for i in T.thread_binding(128, "threadIdx.x"):
+        >>>         atomic_max(result, data[i])
+    """
+    func_name = "AtomicMaxRet" if return_prev else "AtomicMax"
+    return_type = dst.dtype if return_prev else "handle"
+
+    if memory_order is None:
+        return T.call_extern(return_type, func_name, dst, value)
+    else:
+        return T.call_extern(return_type, func_name, dst, value, _MEMORY_ORDER_ID_MAP[memory_order])
+
+
+def atomic_min(dst: Buffer,
+               value: PrimExpr,
+               memory_order: str | None = None,
+               return_prev: bool = False) -> PrimExpr:
+    """
+    Atomically update the value at dst to the minimum of its current value and value.
+
+    If memory_order is provided, it selects the memory-order semantic used by the underlying extern call;
+    allowed names are "relaxed", "consume", "acquire", "release", "acq_rel", and "seq_cst" (mapped internally
+    to integer IDs). If memory_order is None, the extern is invoked without an explicit memory-order argument.
+
+    Parameters:
+        dst (Buffer): Destination buffer/address to apply the atomic min.
+        value (PrimExpr): Value to compare/store atomically.
+        memory_order (Optional[str]): Optional memory-order name controlling the atomic operation's ordering.
+        return_prev (bool): If True, return the previous value; if False, return handle (default False).
+
+    Returns:
+        PrimExpr: A handle expression representing the atomic-min operation, or the previous value if return_prev is True.
+
+    Examples:
+        >>> # Basic atomic min operation
+        >>> min_val = T.Tensor([1], "int32", name="min_val")
+        >>> atomic_min(min_val, 10)
+
+        >>> # Find minimum across threads
+        >>> @T.prim_func
+        >>> def find_min(data: T.Buffer, result: T.Buffer):
+        >>>     for i in T.thread_binding(256, "threadIdx.x"):
+        >>>         atomic_min(result, data[i])
+
+        >>> # Track minimum with previous value
+        >>> threshold = T.Tensor([1], "float32", name="threshold")
+        >>> old_min = atomic_min(threshold, 3.14, return_prev=True)
+        >>> # old_min contains the previous minimum value
+
+        >>> # With relaxed memory ordering for performance
+        >>> atomic_min(min_val, 5, memory_order="relaxed")
+    """
+    func_name = "AtomicMinRet" if return_prev else "AtomicMin"
+    return_type = dst.dtype if return_prev else "handle"
+
+    if memory_order is None:
+        return T.call_extern(return_type, func_name, dst, value)
+    else:
+        return T.call_extern(return_type, func_name, dst, value, _MEMORY_ORDER_ID_MAP[memory_order])
+
+
+def atomic_add(dst: Buffer,
+               value: PrimExpr,
+               memory_order: str | None = None,
+               return_prev: bool = False,
+               use_tma: bool = False) -> PrimExpr:
+    """
+    Atomically add `value` into `dst`, returning a handle to the operation.
+
+    Supports scalar/addressed extern atomic add when neither argument exposes extents, or tile-region-based atomic add for Buffer/BufferRegion/BufferLoad inputs. If both arguments are plain Buffers their shapes must be structurally equal. If at least one side exposes extents, extents are aligned (missing dimensions are treated as size 1); an assertion is raised if extents cannot be deduced. The optional `memory_order` (one of "relaxed","consume","acquire","release","acq_rel","seq_cst") is used only for the direct extern `AtomicAdd` path when no extents are available â€” otherwise the tile-region path ignores `memory_order`.
+
+    Parameters:
+        dst (Buffer): Destination buffer/address to apply the atomic add.
+        value (PrimExpr): Value to add atomically.
+        memory_order (Optional[str]): Optional memory-order name controlling the atomic operation's ordering.
+        return_prev (bool): If True, return the previous value; if False, return handle (default False).
+        use_tma (bool): If True, use TMA (cp.reduce) to perform the atomic add. This is available only for sm90+ (default False).
+
+    Returns:
+        PrimExpr: A handle representing the atomic addition operation, or the previous value if return_prev is True.
+
+    Examples:
+        >>> # Basic atomic addition
+        >>> counter = T.Tensor([1], "int32", name="counter")
+        >>> atomic_add(counter, 1)  # Increment counter by 1
+
+        >>> # Parallel sum reduction
+        >>> @T.prim_func
+        >>> def parallel_sum(data: T.Buffer, result: T.Buffer):
+        >>>     for i in T.thread_binding(1024, "threadIdx.x"):
+        >>>         atomic_add(result, data[i])
+
+        >>> # Get previous value for debugging
+        >>> old_value = atomic_add(counter, 5, return_prev=True)
+        >>> # old_value contains the value before adding 5
+
+        >>> # Tensor-to-tensor atomic add (tile-region based)
+        >>> src_tensor = T.Tensor([128, 64], "float32", name="src")
+        >>> dst_tensor = T.Tensor([128, 64], "float32", name="dst")
+        >>> atomic_add(dst_tensor, src_tensor)  # Add entire tensors atomically
+
+        >>> # With memory ordering for scalar operations
+        >>> atomic_add(counter, 10, memory_order="acquire")
+
+        >>> # Accumulate gradients in training
+        >>> gradients = T.Tensor([1000], "float32", name="gradients")
+        >>> global_grad = T.Tensor([1000], "float32", name="global_grad")
+        >>> atomic_add(global_grad, gradients)
+    """
+
+    def get_extent(data):
+        """
+        Return the inferred extent (shape) of a buffer-like object.
+
+        If `data` is a Var bound to a let value, the let value is resolved before inspection.
+        Parameters:
+            data: A Var, Buffer, or BufferRegion to inspect.
+
+        Returns:
+            The shape/extents as a list-like of PrimExpr (Buffer.shape or list of region item extents), or None if the extent cannot be determined.
+        """
+        if isinstance(data, Var) and T.has_let_value(data):
+            data = T.get_let_value(data)
+        if isinstance(data, Buffer):
+            return data.shape
+        elif isinstance(data, BufferRegion):
+            return [x.extent for x in data.region]
+        else:
+            return None
+
+    src_extent = get_extent(value)
+    dst_extent = get_extent(dst)
+
+    if dst_extent is None and src_extent is None:
+        func_name = "AtomicAddRet" if return_prev else "AtomicAdd"
+        return_type = dst.dtype if return_prev else "handle"
+
+        if memory_order is None:
+            return T.call_extern(return_type, func_name, dst, value)
+        else:
+            return T.call_extern(return_type, func_name, dst, value,
+                                 _MEMORY_ORDER_ID_MAP[memory_order])
+
+    if isinstance(dst, Buffer) and isinstance(value, Buffer):
+        ir.assert_structural_equal(dst.shape, value.shape)
+
+    assert src_extent or dst_extent, "Can't deduce atomicadd extents from args"
+    src_extent = list(src_extent) if src_extent else [1] * len(dst_extent)
+    dst_extent = list(dst_extent) if dst_extent else [1] * len(src_extent)
+    extent = max(src_extent, dst_extent)
+
+    def _to_region(data, access_type):
+        if isinstance(data, tir.Var) and T.has_let_value(data):
+            data = T.get_let_value(data)
+        if isinstance(data, tir.Buffer):
+            return buffer_to_tile_region(data, access_type)
+        elif isinstance(data, tir.BufferRegion):
+            return buffer_region_to_tile_region(data, access_type, extent)
+        elif isinstance(data, tir.BufferLoad):
+            region = get_buffer_region_from_load(data)
+            if region is None:
+                return buffer_load_to_tile_region(data, access_type, extent)
+            return buffer_region_to_tile_region(region, access_type, extent)
+        else:
+            return buffer_load_to_tile_region(data, access_type, extent)
+
+    value = _to_region(value, "r")
+    dst = _to_region(dst, "w")
+
+    # Note: tile-region-based atomic operations don't support return_prev yet
+    # This would need to be implemented in the tile runtime
+    if return_prev:
+        raise NotImplementedError(
+            "return_prev is not supported for tile-region-based atomic operations")
+
+    if memory_order is None:
+        return T.call_intrin("handle", op.Op.get("tl.atomicadd"), value, dst, use_tma, 0)
+    else:
+        return T.call_intrin("handle", op.Op.get("tl.atomicadd"), value, dst, use_tma,
+                             _MEMORY_ORDER_ID_MAP[memory_order])
+
+
+def atomic_addx2(dst: Buffer, value: PrimExpr, return_prev: bool = False) -> PrimExpr:
+    """Perform an atomic addition operation with double-width operands.
+
+    Args:
+        dst (Buffer): Destination buffer where the atomic addition will be performed
+        value (PrimExpr): Value to be atomically added (double-width)
+        return_prev (bool): If True, return the previous value; if False, return handle (default False)
+
+    Returns:
+        PrimExpr: Handle to the double-width atomic addition operation, or the previous value if return_prev is True
+
+    Examples:
+        >>> # Atomic addition with FP16 pairs
+        >>> half_dst = T.Tensor([2], "float16", name="half_dst")
+        >>> half_val = T.Tensor([2], "float16", name="half_val")
+        >>> atomic_addx2(half_dst, half_val)
+
+        >>> # BF16 vectorized atomic add (requires CUDA Arch > 750)
+        >>> bf16_dst = T.Tensor([2], "bfloat16", name="bf16_dst")
+        >>> bf16_val = T.Tensor([2], "bfloat16", name="bf16_val")
+        >>> atomic_addx2(bf16_dst, bf16_val)
+
+        >>> # Get previous paired values
+        >>> prev_values = atomic_addx2(half_dst, half_val, return_prev=True)
+        >>> # prev_values is a half2 containing the two previous FP16 values
+
+        >>> # Efficient gradient accumulation for mixed precision training
+        >>> @T.prim_func
+        >>> def accumulate_fp16_gradients(grads: T.Buffer, global_grads: T.Buffer):
+        >>>     for i in T.thread_binding(128, "threadIdx.x"):
+        >>>         for j in range(0, grads.shape[1], 2):  # Process in pairs
+        >>>             atomic_addx2(global_grads[i, j:j+2], grads[i, j:j+2])
+    """
+    func_name = "AtomicAddx2Ret" if return_prev else "AtomicAddx2"
+    return_type = dst.dtype if return_prev else "handle"
+    return T.call_extern(return_type, func_name, T.address_of(dst), T.address_of(value))
+
+
+def atomic_addx4(dst: Buffer, value: PrimExpr, return_prev: bool = False) -> PrimExpr:
+    """Perform an atomic addition operation with quad-width operands.
+
+    Args:
+        dst (Buffer): Destination buffer where the atomic addition will be performed
+        value (PrimExpr): Value to be atomically added (quad-width)
+        return_prev (bool): If True, return the previous value; if False, return handle (default False)
+
+    Returns:
+        PrimExpr: Handle to the quad-width atomic addition operation, or the previous value if return_prev is True
+
+    Examples:
+        >>> # Atomic addition with float4 (requires CUDA Arch >= 900)
+        >>> float4_dst = T.Tensor([4], "float32", name="float4_dst")
+        >>> float4_val = T.Tensor([4], "float32", name="float4_val")
+        >>> atomic_addx4(float4_dst, float4_val)
+
+        >>> # Get previous float4 values
+        >>> prev_float4 = atomic_addx4(float4_dst, float4_val, return_prev=True)
+        >>> # prev_float4 is a float4 containing the four previous float32 values
+
+        >>> # High-throughput gradient accumulation for large models
+        >>> @T.prim_func
+        >>> def accumulate_float4_gradients(grads: T.Buffer, global_grads: T.Buffer):
+        >>>     for i in T.thread_binding(256, "threadIdx.x"):
+        >>>         for j in range(0, grads.shape[1], 4):  # Process 4 floats at once
+        >>>             atomic_addx4(global_grads[i, j:j+4], grads[i, j:j+4])
+
+        >>> # Efficient RGBA pixel blending
+        >>> rgba_dst = T.Tensor([4], "float32", name="rgba_dst")  # R, G, B, A channels
+        >>> rgba_add = T.Tensor([4], "float32", name="rgba_add")
+        >>> atomic_addx4(rgba_dst, rgba_add)  # Atomic blend of all 4 channels
+    """
+    func_name = "AtomicAddx4Ret" if return_prev else "AtomicAddx4"
+    return_type = "float4" if "float" in str(dst.dtype).lower() else "handle"
+    return T.call_extern(return_type, func_name, T.address_of(dst), T.address_of(value))
+
+
+def atomic_load(src: Buffer, memory_order: str = "seq_cst") -> PrimExpr:
+    """
+    Load a value from the given buffer using the specified atomic memory ordering.
+
+    Performs an atomic load from `src` and returns a PrimExpr representing the loaded value.
+    memory_order selects the ordering and must be one of: "relaxed", "consume", "acquire",
+    "release", "acq_rel", or "seq_cst" (default).
+    Raises KeyError if an unknown memory_order is provided.
+
+    Note: atomic_load always returns the loaded value, so no return_prev parameter is needed.
+
+    Examples:
+        >>> # Basic atomic load
+        >>> shared_var = T.Tensor([1], "int32", name="shared_var")
+        >>> value = atomic_load(shared_var)
+
+        >>> # Load with specific memory ordering
+        >>> value = atomic_load(shared_var, memory_order="acquire")
+        >>> # Ensures all subsequent memory operations happen after this load
+
+        >>> # Relaxed load for performance-critical code
+        >>> value = atomic_load(shared_var, memory_order="relaxed")
+
+        >>> # Producer-consumer pattern
+        >>> @T.prim_func
+        >>> def consumer(flag: T.Buffer, data: T.Buffer, result: T.Buffer):
+        >>>     # Wait until producer sets flag
+        >>>     while atomic_load(flag, memory_order="acquire") == 0:
+        >>>         pass  # Spin wait
+        >>>     # Now safely read data
+        >>>     result[0] = data[0]
+
+        >>> # Load counter for statistics
+        >>> counter = T.Tensor([1], "int64", name="counter")
+        >>> current_count = atomic_load(counter, memory_order="relaxed")
+    """
+    return T.call_extern(src.dtype, "AtomicLoad", src, _MEMORY_ORDER_ID_MAP[memory_order])
+
+
+def atomic_store(dst: Buffer, src: PrimExpr, memory_order: str = "seq_cst") -> PrimExpr:
+    """
+    Perform an atomic store of `src` into `dst` with the given memory ordering.
+
+    Parameters:
+        dst (Buffer): Destination buffer to store into.
+        src (PrimExpr): Value to store.
+        memory_order (str, optional): Memory ordering name; one of "relaxed", "consume",
+            "acquire", "release", "acq_rel", or "seq_cst". Defaults to "seq_cst".
+            The name is mapped to an internal numeric ID used by the underlying runtime.
+
+    Returns:
+        PrimExpr: A handle representing the issued atomic store operation.
+
+    Raises:
+        KeyError: If `memory_order` is not one of the supported names.
+
+    Note: atomic_store doesn't return a previous value, so no return_prev parameter is needed.
+
+    Examples:
+        >>> # Basic atomic store
+        >>> shared_var = T.Tensor([1], "int32", name="shared_var")
+        >>> atomic_store(shared_var, 42)
+
+        >>> # Store with release ordering to publish data
+        >>> data = T.Tensor([1000], "float32", name="data")
+        >>> ready_flag = T.Tensor([1], "int32", name="ready_flag")
+        >>> # ... fill data ...
+        >>> atomic_store(ready_flag, 1, memory_order="release")
+        >>> # Ensures all previous writes are visible before flag is set
+
+        >>> # Relaxed store for performance
+        >>> atomic_store(shared_var, 100, memory_order="relaxed")
+
+        >>> # Producer-consumer synchronization
+        >>> @T.prim_func
+        >>> def producer(data: T.Buffer, flag: T.Buffer):
+        >>>     data[0] = 3.14159  # Write data first
+        >>>     atomic_store(flag, 1, memory_order="release")
+        >>>     # Consumer can now safely read data after seeing flag == 1
+
+        >>> # Update configuration atomically
+        >>> config = T.Tensor([1], "int32", name="config")
+        >>> new_config = 0x12345678
+        >>> atomic_store(config, new_config, memory_order="seq_cst")
+
+        >>> # Thread-safe logging counter
+        >>> log_counter = T.Tensor([1], "int64", name="log_counter")
+        >>> atomic_store(log_counter, 0)  # Reset counter atomically
+    """
+    return T.call_extern("handle", "AtomicStore", dst, src, _MEMORY_ORDER_ID_MAP[memory_order])
diff --git a/tilelang/language/builtin.py b/tilelang/language/builtin.py
index 4e293c49..8db63830 100644
--- a/tilelang/language/builtin.py
+++ b/tilelang/language/builtin.py
@@ -1,11 +1,30 @@
 """The language interface for tl programs."""
+from __future__ import annotations
 
 from tilelang import tvm as tvm
-from tilelang.language import ptx_arrive_barrier, evaluate
+from tilelang.language import ptx_arrive_barrier, evaluate, address_of
 from tilelang.language.kernel import get_thread_bindings, get_block_extents
+from tilelang.utils.target import check_hip_availability
 from tvm import tir
-from typing import Union, Any
-from tvm.tir import PrimExpr, Var, Call
+from typing import Any, Literal
+import tilelang.language as T
+from tvm.tir import PrimExpr, Var, Call, Buffer, BufferLoad
+
+_IS_HIP_AVAILABLE = check_hip_availability()
+
+
+def _normalize_index_arg(value: int | PrimExpr | None) -> PrimExpr | None:
+    """
+    Normalize warp sizing arguments so both Python ints and PrimExpr values
+    are accepted uniformly.
+    """
+    if value is None:
+        return None
+    if isinstance(value, PrimExpr):
+        return value
+    if isinstance(value, int):
+        return tir.IntImm("int32", value)
+    raise TypeError(f"Expect warp sizing argument to be int or PrimExpr, but got {type(value)}.")
 
 
 def create_list_of_mbarrier(*args: Any) -> Call:
@@ -26,7 +45,7 @@ def create_list_of_mbarrier(*args: Any) -> Call:
     ------
     TypeError
         If the input is not a list or variadic arguments.
-    
+
     Examples
     --------
     >>> create_list_of_mbarrier([128, 128])
@@ -142,13 +161,31 @@ def dec_max_nreg(reg_count: int):
     return set_max_nreg(reg_count, 0)
 
 
+def annotate_producer_reg_dealloc(reg_count: int = 24):
+    """Annotate the producer reg dealloc.
+    """
+    return dec_max_nreg(reg_count)
+
+
+def annotate_consumer_reg_alloc(reg_count: int = 240):
+    """Annotate the consumer reg alloc.
+    """
+    return inc_max_nreg(reg_count)
+
+
 def no_set_max_nreg():
     """Disable the maximum register limit setting.
     """
     return tir.call_intrin("handle", tir.op.Op.get("tl.no_set_max_nreg"))
 
 
-def mbarrier_wait_parity(mbarrier: Union[int, PrimExpr, tir.Call], parity: Union[int, Var]):
+def disable_warp_group_reg_alloc():
+    """Disable the warp group reg alloc.
+    """
+    return no_set_max_nreg()
+
+
+def mbarrier_wait_parity(mbarrier: int | PrimExpr | tir.Call, parity: int | Var):
     """Wait for memory barrier parity condition.
 
     Args:
@@ -198,7 +235,7 @@ def mbarrier_wait_parity(mbarrier: Union[int, PrimExpr, tir.Call], parity: Union
     return tir.call_intrin("handle", tir.op.Op.get("tl.mbarrier_wait_parity"), mbarrier, parity)
 
 
-def mbarrier_arrive(mbarrier: Union[int, PrimExpr, tir.Call]):
+def mbarrier_arrive(mbarrier: int | PrimExpr | tir.Call):
     """Arrive at memory barrier.
 
     Args:
@@ -228,6 +265,171 @@ def mbarrier_expect_tx(*args):
     return tir.call_intrin("handle", tir.op.Op.get("tl.mbarrier_expect_tx"), *args)
 
 
+def warpgroup_arrive():
+    """Signal warpgroup readiness for subsequent WGMMA operations.
+
+    Returns:
+        tir.Call: A handle to the warpgroup arrive operation.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.warpgroup_arrive"))
+
+
+def warpgroup_commit_batch():
+    """Commit the current warpgroup batch for WGMMA operations.
+
+    Returns:
+        tir.Call: A handle to the warpgroup commit batch operation.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.warpgroup_commit_batch"))
+
+
+def warpgroup_wait(num_mma: int):
+    """Wait for completion of the specified warpgroup batch.
+
+    Args:
+        num_mma: int
+            Identifier of the warpgroup MMA batch to wait on.
+
+    Returns:
+        tir.Call: A handle to the warpgroup wait operation.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.warpgroup_wait"), num_mma)
+
+
+def get_lane_idx(warp_size: int | PrimExpr | None = None,) -> PrimExpr:
+    """Return the logical lane index of the calling thread within a warp.
+
+    Parameters
+    ----------
+    warp_size : Optional[int, PrimExpr]
+        Logical warp (or wavefront) size. Defaults to 32 on NVIDIA and 64 on AMD.
+
+    Example
+    -------
+    >>> lane = T.get_lane_idx()
+    >>> custom_lane = T.get_lane_idx(64)  # override warp size explicitly
+
+    Implementation Notes
+    --------------------
+    Lowers to the CUDA helper `tl::get_lane_idx(warp_size)` defined in
+    `src/tl_templates/cuda/intrin.h`, which computes the lane index from the
+    linear thread id using the provided `warp_size`.
+    """
+    warp_size_expr = _normalize_index_arg(warp_size)
+    if warp_size_expr is None:
+        return tir.call_intrin("int32", tir.op.Op.get("tl.get_lane_idx"))
+    return tir.call_intrin("int32", tir.op.Op.get("tl.get_lane_idx"), warp_size_expr)
+
+
+def get_warp_idx_sync(warp_size: int | PrimExpr | None = None,) -> PrimExpr:
+    """Return the canonical warp index, assuming the warp's threads are converged.
+
+    Parameters
+    ----------
+    warp_size : Optional[int, PrimExpr]
+        Logical warp size used for the index calculation.
+
+    Example
+    -------
+    >>> warp = T.get_warp_idx_sync()
+    >>> custom_warp = T.get_warp_idx_sync(64)
+
+    Implementation Notes
+    --------------------
+    Emits `tl::get_warp_idx_sync(warp_size)` which divides the block-linear
+    thread id by `warp_size`, matching the semantics of CUTLASS' canonical helpers.
+    """
+    warp_size_expr = _normalize_index_arg(warp_size)
+    if warp_size_expr is None:
+        return tir.call_intrin("int32", tir.op.Op.get("tl.get_warp_idx_sync"))
+    return tir.call_intrin("int32", tir.op.Op.get("tl.get_warp_idx_sync"), warp_size_expr)
+
+
+def get_warp_idx(warp_size: int | PrimExpr | None = None,) -> PrimExpr:
+    """Return the canonical warp index without synchronizing the warp.
+
+    Parameters
+    ----------
+    warp_size : Optional[int, PrimExpr]
+        Logical warp size used for the index calculation.
+
+    Example
+    -------
+    >>> warp = T.get_warp_idx()
+    >>> custom_warp = T.get_warp_idx(64)
+
+    Implementation Notes
+    --------------------
+    Lowers to `tl::get_warp_idx(warp_size)` which divides the block-linear
+    thread id by the provided `warp_size` without requiring warp convergence.
+    """
+    warp_size_expr = _normalize_index_arg(warp_size)
+    if warp_size_expr is None:
+        return tir.call_intrin("int32", tir.op.Op.get("tl.get_warp_idx"))
+    return tir.call_intrin("int32", tir.op.Op.get("tl.get_warp_idx"), warp_size_expr)
+
+
+def get_warp_group_idx(
+    warp_size: int | PrimExpr | None = None,
+    warps_per_group: int | PrimExpr | None = None,
+) -> PrimExpr:
+    """Return the canonical warp group index for the calling thread.
+
+    Parameters
+    ----------
+    warp_size : Optional[int, PrimExpr]
+        Logical warp size to use (defaults to 32 on NVIDIA / 64 on AMD).
+    warps_per_group : Optional[int, PrimExpr]
+        Number of warps per warp-group. Defaults to 4 on NVIDIA architectures.
+
+    Example
+    -------
+    >>> group = T.get_warp_group_idx()
+    >>> custom_group = T.get_warp_group_idx(32, 6)  # treat 6 warps as a group
+
+    Implementation Notes
+    --------------------
+    Generates `tl::get_warp_group_idx(warp_size, warps_per_group)` which
+    divides the block-linear thread id by `warp_size * warps_per_group`,
+    matching the canonical ordering while allowing architecture-specific overrides.
+    """
+    warp_size_expr = _normalize_index_arg(warp_size)
+    warps_per_group_expr = _normalize_index_arg(warps_per_group)
+    args = []
+    if warp_size_expr is not None:
+        args.append(warp_size_expr)
+    if warps_per_group_expr is not None:
+        if warp_size_expr is None:
+            raise ValueError("get_warp_group_idx expects `warp_size` when specifying "
+                             "`warps_per_group`.")
+        args.append(warps_per_group_expr)
+    return tir.call_intrin("int32", tir.op.Op.get("tl.get_warp_group_idx"), *args)
+
+
+def shuffle_elect(thread_extent: int) -> PrimExpr:
+    """Elect exactly one lane within a logical thread group.
+
+    Parameters
+    ----------
+    thread_extent : int
+        Size (in threads) of the group in which a single lane should be elected.
+        Passing 0 elects a single lane in the entire thread block.
+
+    Example
+    -------
+    >>> is_leader = T.shuffle_elect(64)
+    >>> T.if_then_else(is_leader, do_leader_work(), T.evaluate(0))
+
+    Implementation Notes
+    --------------------
+    Lowered to the CUDA helper `tl::tl_shuffle_elect<thread_extent>()` defined in
+    `src/tl_templates/cuda/intrin.h`, which relies on
+    `cutlass::canonical_warp_idx_sync()` and `cute::elect_one_sync()` (or
+    `__shfl_sync`) to pick one lane per group.
+    """
+    return tir.call_intrin("bool", tir.op.Op.get("tl.tl_shuffle_elect"), thread_extent)
+
+
 def wait_wgmma(id: int):
     """Wait for WGMMA (Warp Group Matrix Multiply-Accumulate) operations to complete.
 
@@ -241,7 +443,7 @@ def wait_wgmma(id: int):
     return tir.call_intrin("handle", tir.op.Op.get("tl.wait_wgmma"), id)
 
 
-def barrier_wait(barrier_id: Union[int, PrimExpr, tir.Call], parity: Union[int, Var, None] = None):
+def barrier_wait(barrier_id: int | PrimExpr | tir.Call, parity: int | Var | None = None):
     """Wait for a memory barrier to complete.
 
     Args:
@@ -256,7 +458,7 @@ def barrier_wait(barrier_id: Union[int, PrimExpr, tir.Call], parity: Union[int,
     return mbarrier_wait_parity(barrier_id, parity)
 
 
-def barrier_arrive(barrier_id: Union[int, PrimExpr, tir.Call]):
+def barrier_arrive(barrier_id: int | PrimExpr | tir.Call):
     """Arrive at a memory barrier.
 
     Args:
@@ -266,7 +468,7 @@ def barrier_arrive(barrier_id: Union[int, PrimExpr, tir.Call]):
     return mbarrier_arrive(barrier_id)
 
 
-def shfl_xor(value: Union[int, PrimExpr, tir.Call], offset: Union[int, PrimExpr, tir.Call]):
+def shfl_xor(value: int | PrimExpr | tir.Call, offset: int | PrimExpr | tir.Call):
     """Perform a shuffle operation with XOR offset.
 
     Args:
@@ -277,50 +479,51 @@ def shfl_xor(value: Union[int, PrimExpr, tir.Call], offset: Union[int, PrimExpr,
     Returns:
         tir.Call: A handle to the shuffle operation
     """
-    return tir.call_extern(value.dtype, "__shfl_xor_sync", 0xffffffff, value, offset)
+    if _IS_HIP_AVAILABLE:
+        return tir.call_extern(value.dtype, "__shfl_xor", value, offset)
+    else:
+        return tir.call_extern(value.dtype, "__shfl_xor_sync", 0xffffffff, value, offset)
 
 
-def shfl_down(value: Union[int, PrimExpr, tir.Call], offset: Union[int, PrimExpr, tir.Call]):
+def shfl_down(value: int | PrimExpr | tir.Call, offset: int | PrimExpr | tir.Call):
     """Perform a shuffle operation with down offset.
 
     Args:
         value: Optional[int, PrimExpr]
             The value to shuffle
     """
-    return tir.call_extern(value.dtype, "__shfl_down_sync", 0xffffffff, value, offset)
+    if _IS_HIP_AVAILABLE:
+        return tir.call_extern(value.dtype, "__shfl_down", value, offset)
+    else:
+        return tir.call_extern(value.dtype, "__shfl_down_sync", 0xffffffff, value, offset)
 
 
-def shfl_up(value: Union[int, PrimExpr, tir.Call], offset: Union[int, PrimExpr, tir.Call]):
+def shfl_up(value: int | PrimExpr | tir.Call, offset: int | PrimExpr | tir.Call):
     """Perform a shuffle operation with up offset.
 
     Args:
         value: Optional[int, PrimExpr]
             The value to shuffle
     """
-    return tir.call_extern(value.dtype, "__shfl_up_sync", 0xffffffff, value, offset)
-
-
-def sync_threads():
-    """Synchronize all threads in a warp.
-    """
-    return tir.op.tvm_storage_sync("shared")
-
+    if _IS_HIP_AVAILABLE:
+        return tir.call_extern(value.dtype, "__shfl_up", value, offset)
+    else:
+        return tir.call_extern(value.dtype, "__shfl_up_sync", 0xffffffff, value, offset)
 
-def sync_thread_partial(barrier_id: Union[int, PrimExpr, tir.Call]):
-    """Synchronize threads within a warp.
 
-    Args:
-        barrier_id: Optional[int, PrimExpr]
-            The memory barrier to synchronize
-
-    Returns:
-        tir.Call: A handle to the synchronization operation
+def sync_threads(barrier_id: int | PrimExpr = None, arrive_count: int = None):
+    """Synchronize all threads in a block.
     """
-    return tir.call_intrin("handle", tir.op.Op.get("tl.sync_thread_partial"), barrier_id)
+    args = []
+    if barrier_id is not None:
+        args.append(barrier_id)
+    if arrive_count is not None:
+        args.append(arrive_count)
+    return tir.call_intrin("int32", "tir.tvm_storage_sync", "shared", *args)
 
 
 def sync_global():
-    """Synchronize all threads in a block.
+    """Synchronize all threads in the entire grid.
     """
     tx, ty, tz = get_thread_bindings()
     ex, ey, ez = get_block_extents()
@@ -329,7 +532,318 @@ def sync_global():
     return evaluate(tir.Call("handle", "tir.tvm_storage_sync", args))
 
 
-def sync_grid():
+def sync_grid_cg():
     """Synchronize all threads in a grid.
     """
-    return tir.call_intrin("handle", tir.op.Op.get("tl.sync_grid"))
+    return tir.call_intrin("handle", tir.op.Op.get("tl.sync_grid_cg"))
+
+
+def copy_unrolled(dst: PrimExpr, src: PrimExpr, size: int, unroll_factor: int = 4):
+    """Copy between two global memory buffers with unrolled loop.
+
+    Args:
+        dst: tir.Buffer
+            The destination buffer
+        src: tir.Buffer
+            The source buffer
+        unroll_factor: int
+            The unroll factor
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.copy_unrolled"), dst, src, size,
+                           unroll_factor)
+
+
+# Device-level barrier synchronization
+
+
+def alloc_barrier_gpu():
+    """Allocate a barrier for GPU-level synchronization.
+
+    Returns:
+        T.Buffer: A single-element TVM buffer object allocated as a barrier
+    """
+    return T.alloc_buffer([1], "uint32", scope="global")
+
+
+def init_barrier_gpu(barrier: PrimExpr, expected: int):
+    """Initialize a barrier for GPU-level synchronization.
+
+    Args:
+        barrier: The barrier to initialize
+        expected (int): The number of threads that need to arrive at the barrier.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.init_barrier_gpu"), address_of(barrier),
+                           expected)
+
+
+def arrive_barrier_gpu(barrier: PrimExpr):
+    """Arrive at a barrier for GPU-level synchronization.
+
+    Args:
+        barrier: The barrier to arrive at
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.arrive_barrier_gpu"), address_of(barrier))
+
+
+def wait_barrier_gpu(barrier: PrimExpr):
+    """Wait at a barrier for GPU-level synchronization.
+
+    Args:
+        barrier: The barrier to wait at
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.wait_barrier_gpu"), address_of(barrier))
+
+
+def sync_barrier_gpu(barrier: PrimExpr):
+    """Synchronize at a barrier for GPU-level synchronization.
+
+    Args:
+        barrier: The barrier to synchronize at
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.sync_barrier_gpu"), address_of(barrier))
+
+
+def sync_grid(barrier: PrimExpr):
+    """Synchronize at a barrier for GPU-level synchronization in cooperative group style.
+
+    Args:
+        barrier: The barrier to synchronize at
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.sync_grid"), address_of(barrier))
+
+
+def barrier_blocks(barrier: PrimExpr):
+    """Barrier all blocks at a system-level barrier.
+    Compare to sync_blocks, barrier_blocks have an extra system-level fence effect
+
+    Args:
+        barrier: The barrier to synchronize at, should be [num_ranks] of int32
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.barrier_blocks"), address_of(barrier),
+                           1)  # whether need fence
+
+
+def sync_blocks(barrier: PrimExpr):
+    """Synchronize all blocks at a system-level barrier.
+
+    Args:
+        barrier: The barrier to synchronize at, should be [num_ranks] of int32
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.barrier_blocks"), address_of(barrier),
+                           0)  # whether need fence
+
+
+def fence_cta():
+    """Create a memory fence at the block level (visible to all threads in the current block)."""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.fence_cta"))
+
+
+def fence_gpu():
+    """Synchronize all threads at the GPU level (visible to all blocks on the current device)."""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.fence_gpu"))
+
+
+def fence_sys():
+    """Synchronize all threads at the system level (visible in a node)."""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.fence_sys"))
+
+
+def get_clock():
+    """Get the current clock cycle count.
+
+    Returns:
+        tir.Call: A handle to the clock cycle count operation
+    """
+    return tir.call_intrin("int64", tir.op.Op.get("tl.get_clock"))
+
+
+def initialize_descriptor(descriptor: Buffer,
+                          start_address: PrimExpr,
+                          layout_type_: int = 0,
+                          leading_byte_offset: int = 0,
+                          stride_byte_offset: int = 0) -> PrimExpr:
+    """
+    Initialize a memory descriptor with the given parameters.
+
+    Parameters:
+        descriptor (Buffer): The memory descriptor to initialize.
+        start_address (PrimExpr): The starting address of the memory region.
+        layout_type_ (int, optional): Layout type identifier. Defaults to 0.
+        leading_byte_offset (int, optional): Leading byte offset. Defaults to 0.
+        stride_byte_offset (int, optional): Stride byte offset. Defaults to 0.
+
+    Returns:
+        PrimExpr: A handle representing the initialized descriptor.
+    """
+
+    if not isinstance(descriptor, (BufferLoad, Buffer)):
+        raise TypeError("Descriptor must be a tvm.tir.Buffer or tvm.tir.BufferLoad.")
+
+    if isinstance(descriptor, Buffer) and len(descriptor.shape) != 1 or descriptor.shape[0] != 1:
+        raise ValueError("Descriptor must be a 1D buffer of size 1.")
+
+    descriptor = descriptor if isinstance(descriptor, BufferLoad) else tir.BufferLoad(
+        descriptor, [0])
+
+    return evaluate(
+        tir.call_intrin("handle", tir.op.Op.get("tl.initialize_descriptor"), descriptor,
+                        start_address, layout_type_, int(leading_byte_offset),
+                        int(stride_byte_offset)))
+
+
+def increase_descriptor_offset(descriptor: PrimExpr, offset: PrimExpr) -> PrimExpr:
+    """
+    Increase the offset of a memory descriptor.
+
+    Parameters:
+        descriptor (PrimExpr): The memory descriptor to modify.
+        offset (PrimExpr): The offset value to increase.
+
+    Returns:
+        PrimExpr: A handle representing the modified descriptor.
+    """
+    if not isinstance(descriptor, (BufferLoad, Buffer)):
+        raise TypeError("Descriptor must be a tvm.tir.Buffer or tvm.tir.BufferLoad.")
+
+    if isinstance(descriptor, Buffer) and len(descriptor.shape) != 1 or descriptor.shape[0] != 1:
+        raise ValueError("Descriptor must be a 1D buffer of size 1.")
+
+    descriptor = descriptor if isinstance(descriptor, BufferLoad) else tir.BufferLoad(
+        descriptor, [0])
+
+    return evaluate(
+        tir.call_intrin("handle", tir.op.Op.get("tl.increase_descriptor_offset"), descriptor,
+                        offset))
+
+
+def loop_break():
+    """Break out of the innermost loop.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.loop_break"))
+
+
+def cp_async_barrier_noinc(barrier_id: int | PrimExpr | tir.Call):
+    """Perform a ptx async copy barrier using cp.async.mbarrier.arrive.noinc.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.ptx_cp_async_barrier_noinc"), barrier_id)
+
+
+def atom_add(barrier: PrimExpr, value: PrimExpr, scope: str = "gpu", sem: str = "relaxed"):
+    """Perform a ptx async copy barrier using cp.async.mbarrier.arrive.noinc.
+    """
+    assert scope in ["gpu", "sys"], "Scope must be one of 'gpu', or 'sys'."
+    assert sem in ["relaxed", "acquire", "release", "acq_rel"
+                  ], "Semantic must be one of 'relaxed', 'acquire', 'release', or 'acq_rel'."
+    return tir.call_intrin("uint32", tir.op.Op.get("tl.atom_add"), address_of(barrier), value, sem,
+                           scope)
+
+
+def ld(
+    src: PrimExpr,
+    value: PrimExpr,
+    scope: Literal["cta", "gpu", "sys"] = "gpu",
+    sem: Literal["weak", "volatile", "acquire", "release", "relaxed"] = "weak",
+    na: bool = False,
+    nc: bool = False,
+    src_pe: tir.PrimExpr | tir.IntImm | None = -1,
+):
+    """Load a value from a given address with specified scope, semantic, and optional destination PE.
+
+    Args:
+        src: The source address to load from.
+        value: The value to load.
+        scope: The memory scope.
+        sem: The memory semantic.
+        na: Whether to use no-allocate L1 policy.
+        nc: Whether to use non-coherent cache.
+        src_pe: The source processing element (PE) identifier.
+                Use -1 (default) for local PE, or a non-negative integer to target a remote PE.
+
+    Returns:
+        tir.Call: A handle to the load operation.
+    """
+    assert scope in ["cta", "gpu", "sys"], "Scope must be one of 'cta', 'gpu', or 'sys'."
+    assert sem in [
+        "weak", "volatile", "acquire", "relaxed"
+    ], "Semantic must be one of 'weak', 'volatile', 'acquire', 'release', or 'relaxed'."
+    scope = {"cta": 0, "gpu": 1, "sys": 2}[scope]
+    sem = {"weak": 0, "volatile": 1, "acquire": 2, "release": 3, "relaxed": 4}[sem]
+    na = 1 if na else 0
+    nc = 1 if nc else 0
+    return tir.call_intrin("handle", tir.op.Op.get("tl.ld"), address_of(src), value, sem, scope, na,
+                           nc, src_pe)
+
+
+def st(
+    dst: PrimExpr,
+    value: PrimExpr,
+    scope: Literal["cta", "gpu", "sys"] = "gpu",
+    sem: Literal["weak", "volatile", "release", "relaxed"] = "weak",
+    na: bool = False,
+    dst_pe: tir.PrimExpr | tir.IntImm | None = -1,
+):
+    """Store a value to a given address with specified scope, semantic, and optional destination PE.
+
+    Args:
+        dst: The destination to store the value to.
+        value: The value to store.
+        scope: The memory scope.
+        sem: The memory semantic.
+        na: Whether to use no-allocate L1 policy.
+        dst_pe: The destination processing element (PE) identifier.
+                Use -1 (default) for local PE, or a non-negative integer to target a remote PE.
+
+    Returns:
+        tir.Call: A handle to the store operation.
+    """
+    assert scope in ["cta", "gpu", "sys"], "Scope must be one of 'cta', 'gpu', or 'sys'."
+    assert sem in ["weak", "volatile", "release", "relaxed"
+                  ], "Semantic must be one of 'weak', 'volatile', 'release', or 'relaxed'."
+
+    # convert to int
+    scope = {"cta": 0, "gpu": 1, "sys": 2}[scope]
+    sem = {"weak": 0, "volatile": 1, "acquire": 2, "release": 3, "relaxed": 4}[sem]
+    na = 1 if na else 0
+    return tir.call_intrin("handle", tir.op.Op.get("tl.st"), address_of(dst), value, sem, scope, na,
+                           dst_pe)
+
+
+def elect_one_sync():
+    """Efficiently elect exactly one lane within a warp."""
+    return tir.call_intrin("bool", tir.op.Op.get("tl.elect_one_sync"))
+
+
+def sync_warp():
+    """Synchronize all threads in a warp."""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.sync_warp"))
+
+
+def loop_continue():
+    """Continue the innermost loop."""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.loop_continue"))
+
+
+def warp_any(value, mask=-1):
+    """Check if any lane in the warp has a true value.
+
+    Args:
+        value (int): The value to vote.
+        mask (uint32): The mask to use, default is 0xFFFFFFFF, which means all lanes.
+
+    Returns:
+        result (int): The result of the vote.
+    """
+    return tir.call_intrin("int32", tir.op.Op.get("tl.warp_any"), value, mask)
+
+
+def warp_all(value, mask=-1):
+    """Check if all lane in the warp have a true value.
+
+    Args:
+        value (int): The value to vote.
+        mask (uint32): The mask to use, default is 0xFFFFFFFF(-1), which means all lanes.
+
+    Returns:
+        result (int): The result of the vote.
+    """
+    return tir.call_intrin("int32", tir.op.Op.get("tl.warp_all"), value, mask)
diff --git a/tilelang/language/copy.py b/tilelang/language/copy.py
index f492c1bc..84444b8c 100644
--- a/tilelang/language/copy.py
+++ b/tilelang/language/copy.py
@@ -1,91 +1,18 @@
 """The language interface for tl programs."""
+from __future__ import annotations
 
-from typing import Union, List, Optional
+from typing import Literal
 from tilelang import language as T
+from tilelang.utils.language import get_buffer_region_from_load
 from tvm import ir, tir
+from tilelang.language.utils import buffer_to_tile_region, buffer_region_to_tile_region, buffer_load_to_tile_region
 
 
-def region(buffer: tir.BufferLoad, access_type: str, *args: tir.PrimExpr):
-    """Create a memory region descriptor for tile operations.
-
-    Args:
-        buffer (tir.BufferLoad): The buffer to create a region for
-        access_type (str): Type of access - 'r' for read, 'w' for write, 'rw' for read-write
-        *args (tir.PrimExpr): Extent expressions defining the region size
-
-    Returns:
-        tir.Call: A region descriptor for tile operations
-    """
-    access_type = {"r": 1, "w": 2, "rw": 3}[access_type]
-    return tir.call_intrin("handle", tir.op.Op.get("tl.region"), buffer, access_type, *args)
-
-
-def buffer_to_tile_region(buffer: tir.Buffer, access_type: str):
-    """Convert a TVM buffer to a tile region descriptor.
-
-    Args:
-        buffer (tir.Buffer): The buffer to convert
-        access_type (str): Type of access - 'r' for read, 'w' for write, 'rw' for read-write
-
-    Returns:
-        tir.Call: A region descriptor covering the entire buffer
-    """
-    mins = [0 for _ in buffer.shape]
-    extents = [x for x in buffer.shape]
-    return region(T.BufferLoad(buffer, mins), access_type, *extents)
-
-
-def buffer_load_to_tile_region(load: tir.BufferLoad, access_type: str, extents: List[tir.PrimExpr]):
-    """Convert a buffer load operation to a tile region descriptor.
-
-    Args:
-        load (tir.BufferLoad): The buffer load operation
-        access_type (str): Type of access - 'r' for read, 'w' for write, 'rw' for read-write
-        extents (List[tir.PrimExpr]): List of expressions defining the region size
-
-    Returns:
-        tir.Call: A region descriptor for the loaded area
-    """
-    indices = load.indices
-    if len(indices) > len(extents):
-        # (f"mismatch between indices and extents for buffer load {load}: indices = {indices}, extents = {extents}, "
-        # f"region will be expanded in the last 2 dimensions")
-        new_extents = []
-        for _ in range(len(indices) - len(extents)):
-            new_extents.append(1)
-        for extent in extents:
-            new_extents.append(extent)
-        extents = new_extents
-    assert len(indices) == len(extents), f"indices = {indices}, extents = {extents}"
-    return region(load, access_type, *extents)
-
-
-def buffer_region_to_tile_region(buffer_region: tir.BufferRegion, access_type: str,
-                                 extents: List[tir.PrimExpr]):
-    """Convert a buffer region to a tile region descriptor.
-
-    Args:
-        buffer_region (tir.BufferRegion): The buffer region to convert
-        access_type (str): Type of access - 'r' for read, 'w' for write, 'rw' for read-write
-
-    Returns:
-        tir.Call: A region descriptor for the specified buffer region
-    """
-    mins = [x.min for x in buffer_region.region]
-    region_extents = [x.extent for x in buffer_region.region]
-    assert len(region_extents) >= len(
-        extents
-    ), f"region_extents must be >= extents, region_extents = {region_extents}, extents = {extents}"
-
-    return region(T.BufferLoad(buffer_region.buffer, mins), access_type, *region_extents)
-
-
-def copy(
-    src: Union[tir.Buffer, tir.BufferLoad, tir.BufferRegion],
-    dst: Union[tir.Buffer, tir.BufferLoad],
-    coalesced_width: Optional[int] = None,
-    disable_tma: bool = False,
-):
+def copy(src: tir.Buffer | tir.BufferLoad | tir.BufferRegion,
+         dst: tir.Buffer | tir.BufferLoad,
+         coalesced_width: int | None = None,
+         disable_tma: bool = False,
+         eviction_policy: Literal["evict_normal", "evict_first", "evict_last"] | None = None):
     """Copy data between memory regions.
 
     Args:
@@ -109,11 +36,24 @@ def copy(
             return data.shape
         elif isinstance(data, tir.BufferRegion):
             return [x.extent for x in data.region]
+        elif isinstance(data, tir.BufferLoad):
+            region = get_buffer_region_from_load(data)
+            if region is None:
+                return None
+            return [x.extent for x in region.region]
         else:
             return None
 
     src_extent = get_extent(src)
     dst_extent = get_extent(dst)
+    # Combine the nested if statements into a single if statement as suggested by SIM102
+    if (src_extent is None and dst_extent is None and isinstance(src, tir.BufferLoad) and
+            isinstance(dst, tir.BufferLoad)):
+        # check if the case is like this:
+        # copy(buffer_a[i], buffer_b[i]) where both are BufferLoad nodes
+        # In this case, lower it to a simple BufferStore: buffer_b[i] = buffer_a[i]
+        return tir.BufferStore(dst.buffer, src, dst.indices)
+
     assert src_extent or dst_extent, "Can't deduce copy extents from args"
     src_extent = list(src_extent) if src_extent else [1] * len(dst_extent)
     dst_extent = list(dst_extent) if dst_extent else [1] * len(src_extent)
@@ -126,6 +66,11 @@ def copy(
             return buffer_to_tile_region(data, access_type)
         elif isinstance(data, tir.BufferRegion):
             return buffer_region_to_tile_region(data, access_type, extent)
+        elif isinstance(data, tir.BufferLoad):
+            region = get_buffer_region_from_load(data)
+            if region is None:
+                return buffer_load_to_tile_region(data, access_type, extent)
+            return buffer_region_to_tile_region(region, access_type, extent)
         else:
             return buffer_load_to_tile_region(data, access_type, extent)
 
@@ -134,20 +79,23 @@ def copy(
 
     if coalesced_width is None:
         coalesced_width = -1  # PrimExpr can not be None
+    if eviction_policy is None:
+        eviction_policy = 0
+    else:
+        eviction_policy = {"evict_normal": 0, "evict_first": 1, "evict_last": 2}[eviction_policy]
     return tir.call_intrin("handle", tir.op.Op.get("tl.copy"), src, dst, coalesced_width,
-                           disable_tma)
-
-
-def c2d_im2col(
-    img: tir.Buffer,
-    col: tir.Buffer,
-    nhw_step: tir.PrimExpr,
-    c_step: tir.PrimExpr,
-    kernel: int,
-    stride: int,
-    dilation: int,
-    pad: int,
-):
+                           disable_tma, eviction_policy)
+
+
+def c2d_im2col(img: tir.Buffer,
+               col: tir.Buffer,
+               nhw_step: tir.PrimExpr,
+               c_step: tir.PrimExpr,
+               kernel: int,
+               stride: int,
+               dilation: int,
+               pad: int,
+               eviction_policy: Literal["evict_normal", "evict_first", "evict_last"] | None = None):
     """Perform im2col transformation for 2D convolution.
 
     Args:
@@ -163,15 +111,10 @@ def c2d_im2col(
     Returns:
         tir.Call: A handle to the im2col operation
     """
-    return tir.call_intrin(
-        "handle",
-        tir.op.Op.get("tl.c2d_im2col"),
-        img.access_ptr("r"),
-        col.access_ptr("w"),
-        nhw_step,
-        c_step,
-        kernel,
-        stride,
-        dilation,
-        pad,
-    )
+    if eviction_policy is None:
+        eviction_policy = 0
+    else:
+        eviction_policy = {"evict_normal": 0, "evict_first": 1, "evict_last": 2}[eviction_policy]
+    return tir.call_intrin("handle", tir.op.Op.get("tl.c2d_im2col"), img.access_ptr("r"),
+                           col.access_ptr("w"), nhw_step, c_step, kernel, stride, dilation, pad,
+                           eviction_policy)
diff --git a/tilelang/language/customize.py b/tilelang/language/customize.py
index 1e87a70b..9f7d3587 100644
--- a/tilelang/language/customize.py
+++ b/tilelang/language/customize.py
@@ -1,47 +1,11 @@
+# Copyright (c) Tile-AI Corporation.
+# Licensed under the MIT License.
 """The language interface for tl programs."""
+from __future__ import annotations
 
 import tilelang.language as T
-from tvm.tir import PrimExpr, Buffer
-from typing import List, Union
-
-
-def atomic_add(dst: Buffer, value: PrimExpr) -> PrimExpr:
-    """Perform an atomic addition operation.
-
-    Args:
-        dst (Buffer): Destination buffer where the atomic addition will be performed
-        value (PrimExpr): Value to be atomically added
-
-    Returns:
-        PrimExpr: Handle to the atomic addition operation
-    """
-    return T.call_extern("handle", "AtomicAdd", T.address_of(dst), value)
-
-
-def atomic_addx2(dst: Buffer, value: PrimExpr) -> PrimExpr:
-    """Perform an atomic addition operation with double-width operands.
-
-    Args:
-        dst (Buffer): Destination buffer where the atomic addition will be performed
-        value (PrimExpr): Value to be atomically added (double-width)
-
-    Returns:
-        PrimExpr: Handle to the double-width atomic addition operation
-    """
-    return T.call_extern("handle", "AtomicAddx2", T.address_of(dst), T.address_of(value))
-
-
-def atomic_addx4(dst: Buffer, value: PrimExpr) -> PrimExpr:
-    """Perform an atomic addition operation with double-width operands.
-
-    Args:
-        dst (Buffer): Destination buffer where the atomic addition will be performed
-        value (PrimExpr): Value to be atomically added (double-width)
-
-    Returns:
-        PrimExpr: Handle to the double-width atomic addition operation
-    """
-    return T.call_extern("handle", "AtomicAddx4", T.address_of(dst), T.address_of(value))
+from tvm.tir import PrimExpr, Buffer, op
+from .atomic import atomic_max, atomic_min, atomic_add, atomic_addx2, atomic_addx4, atomic_load, atomic_store  # noqa: F401
 
 
 def dp4a(A: Buffer, B: Buffer, C: Buffer) -> PrimExpr:
@@ -60,12 +24,12 @@ def dp4a(A: Buffer, B: Buffer, C: Buffer) -> PrimExpr:
 
 def clamp(dst: PrimExpr, min_val: PrimExpr, max_val: PrimExpr) -> PrimExpr:
     """Clamps the input value dst between [min_val, max_val]
-    
+
     Args:
         dst: Input value to be clamped
         min_val: Minimum value
         max_val: Maximum value
-    
+
     Returns:
         Value clamped to the specified range
     """
@@ -74,9 +38,9 @@ def clamp(dst: PrimExpr, min_val: PrimExpr, max_val: PrimExpr) -> PrimExpr:
     return dst
 
 
-def reshape(src: Buffer, shape: List[PrimExpr]) -> Buffer:
+def reshape(src: Buffer, shape: list[PrimExpr]) -> Buffer:
     """Reshapes the input buffer to the specified shape.
-    
+
     Args:
         src (Buffer): Input buffer to be reshaped
         shape (List[PrimExpr]): New shape for the buffer
@@ -87,21 +51,23 @@ def reshape(src: Buffer, shape: List[PrimExpr]) -> Buffer:
     return T.Tensor(shape, src.dtype, src.data)
 
 
-def view(src: Buffer,
-         shape: Union[List[PrimExpr], None] = None,
-         dtype: Union[str, None] = None) -> Buffer:
-    """Views the input buffer with optionally modified shape and dtype.
-    
-    Args:
-        src (Buffer): Input buffer to be viewed
-        shape (Union[List[PrimExpr], None], optional): New shape for the buffer. Defaults to None.
-        dtype (Union[str, None], optional): New dtype for the buffer. Defaults to None.
-
-    Returns:
-        Buffer: A new buffer view with the specified shape and dtype
+def view(src: Buffer, shape: list[PrimExpr] | None = None, dtype: str | None = None) -> Buffer:
     """
+         Return a Tensor view of the input buffer with an optional new shape and dtype.
+
+         If `shape` is None the source buffer's shape is used; if `dtype` is None the source buffer's dtype is used. The returned buffer shares the same underlying data as `src` (no copy).
+         """
     if shape is None:
         shape = src.shape
     if dtype is None:
         dtype = src.dtype
     return T.Tensor(shape, dtype, src.data)
+
+
+def loop_break():
+    """Break out of the current loop.
+
+    Returns:
+        tir.Call: A call to the `tl.loop_break` intrinsic.
+    """
+    return T.call_intrin("handle", op.Op.get("tl.loop_break"))
diff --git a/tilelang/language/distributed/__init__.py b/tilelang/language/distributed/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/tilelang/language/distributed/common.py b/tilelang/language/distributed/common.py
new file mode 100644
index 00000000..adb559e9
--- /dev/null
+++ b/tilelang/language/distributed/common.py
@@ -0,0 +1,162 @@
+"""The language interface for tl programs."""
+from __future__ import annotations
+
+from tvm import tir
+from tvm.tir import address_of
+from tvm.tir import PrimExpr, IntImm
+from enum import Enum
+
+
+def get_rank():
+    """Get the rank of the current process.
+    """
+    return tir.call_intrin("uint64", tir.op.Op.get("tl.get_rank"))
+
+
+def get_num_ranks():
+    """Get the number of processes.
+    """
+    return tir.call_intrin("uint64", tir.op.Op.get("tl.get_num_ranks"))
+
+
+def put_warp(src: PrimExpr,
+             dst: PrimExpr,
+             size: PrimExpr,
+             dst_pe: PrimExpr | IntImm | None = -1,
+             unroll_factor: int = 4,
+             enable_aggressive_vectorize: bool = False):
+    """Put to a remote buffer with unrolled loop.
+
+    Args:
+        src: PrimExpr
+            The source address.
+        dst: PrimExpr
+            The destination address.
+        size: PrimExpr
+            The size of the put in elements.
+        dst_pe: PrimExpr | None
+            The PE index of the destination.
+            -1 by default, which means local copy.
+        unroll_factor: int
+            The unroll factor
+        enable_aggressive_vectorize: bool
+            Whether to enable aggressive vectorization.
+            If True, the compiler with try to vectorize the copy via int4.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.put"), src, dst, size, dst_pe, unroll_factor,
+                           "warp", enable_aggressive_vectorize)
+
+
+def get_warp(src: PrimExpr,
+             dst: PrimExpr,
+             size: PrimExpr,
+             src_pe: PrimExpr | IntImm | None = -1,
+             unroll_factor: int = 4,
+             enable_aggressive_vectorize: bool = False):
+    """Get from a remote buffer with unrolled loop.
+
+    Args:
+        src: PrimExpr
+            The source address.
+        dst: PrimExpr
+            The destination address.
+        size: PrimExpr
+            The size of the get in elements.
+        src_pe: PrimExpr | None
+            The PE index of the source.
+            -1 by default, which means local copy.
+        unroll_factor: int
+            The unroll factor
+        enable_aggressive_vectorize: bool
+            Whether to enable aggressive vectorization.
+            If True, the compiler with try to vectorize the copy via int4.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.get"), src, dst, size, src_pe, unroll_factor,
+                           "warp", enable_aggressive_vectorize)
+
+
+def put_block(src: PrimExpr, dst: PrimExpr, size: PrimExpr, dst_pe: PrimExpr | IntImm | None = -1):
+    """Put to a remote buffer.
+
+    Args:
+        src: PrimExpr
+            The source address.
+        dst: PrimExpr
+            The destination address.
+        size: PrimExpr
+            The size of the put in elements.
+        dst_pe: PrimExpr | None
+            The PE index of the destination.
+            -1 by default, which means local copy.
+    """
+    return tir.call_intrin(
+        "handle", tir.op.Op.get("tl.put"), src, dst, size, dst_pe, 0, "block", True
+    )  # NOTE: unroll_factor is not needed because currently we implement block-level comm based on NVSHMEM-style copy
+
+
+def get_block(src: PrimExpr, dst: PrimExpr, size: PrimExpr, src_pe: PrimExpr | IntImm | None = -1):
+    """Get from a remote buffer.
+
+    Args:
+        src: PrimExpr
+            The source address.
+        dst: PrimExpr
+            The destination address.
+        size: PrimExpr
+            The size of the get in elements.
+        src_pe: PrimExpr | None
+            The PE index of the source.
+            -1 by default, which means local copy.
+    """
+    return tir.call_intrin(
+        "handle", tir.op.Op.get("tl.get"), src, dst, size, src_pe, 0, "block", True
+    )  # NOTE: unroll_factor is not needed because currently we implement block-level comm based on NVSHMEM-style copy
+
+
+class BinaryRelation(Enum):
+    EQ = 0
+    NE = 1
+    GE = 2
+    LE = 3
+    GT = 4
+    LT = 5
+
+
+def wait_eq(barrier: PrimExpr, expected: PrimExpr):
+    """Wait until *barrier == expected* for GPU-level synchronization.
+    # todo: have different semantic compared to 3 fns below currently
+    Args:
+        barrier: The barrier to wait at
+        expected: The expected value to wait for
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.wait_eq"), address_of(barrier), expected)
+
+
+def wait_ne(ptr: PrimExpr, expected: PrimExpr, peer: PrimExpr | None = -1):
+    """Wait until *ptr != expected"""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.wait"), BinaryRelation.NE.value,
+                           address_of(ptr), expected, peer)
+
+
+def wait_ge(ptr: PrimExpr, expected: PrimExpr, peer: PrimExpr | None = -1):
+    """Wait until *ptr >= expected"""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.wait"), BinaryRelation.GE.value,
+                           address_of(ptr), expected, peer)
+
+
+def wait_le(ptr: PrimExpr, expected: PrimExpr, peer: PrimExpr | None = -1):
+    """Wait until *ptr <= expected"""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.wait"), BinaryRelation.LE.value,
+                           address_of(ptr), expected, peer)
+
+
+def wait_gt(ptr: PrimExpr, expected: PrimExpr, peer: PrimExpr | None = -1):
+    """Wait until *ptr > expected"""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.wait"), BinaryRelation.GT.value,
+                           address_of(ptr), expected, peer)
+
+
+def wait_lt(ptr: PrimExpr, expected: PrimExpr, peer: PrimExpr | None = -1):
+    """Wait until *ptr < expected"""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.wait"), BinaryRelation.LT.value,
+                           address_of(ptr), expected, peer)
diff --git a/tilelang/language/distributed/multi_device/__init__.py b/tilelang/language/distributed/multi_device/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/tilelang/language/distributed/multi_device/cpengine.py b/tilelang/language/distributed/multi_device/cpengine.py
new file mode 100644
index 00000000..f5617d77
--- /dev/null
+++ b/tilelang/language/distributed/multi_device/cpengine.py
@@ -0,0 +1,7 @@
+"""The language interface for tl programs."""
+
+from tvm import tir
+
+
+def cpengine_cpasync(*args):
+    return tir.call_intrin("int32", tir.op.Op.get("tl.CpengineCpAsync"), *args)
diff --git a/tilelang/language/distributed/multi_device/nvshmem.py b/tilelang/language/distributed/multi_device/nvshmem.py
new file mode 100644
index 00000000..186a5d99
--- /dev/null
+++ b/tilelang/language/distributed/multi_device/nvshmem.py
@@ -0,0 +1,218 @@
+"""The language interface for tl programs."""
+
+from tvm import tir
+
+
+def get_pe():
+    """Get the processing element (PE) ID."""
+    return tir.call_intrin("int32", tir.op.Op.get("tl.GetPE"))
+
+
+def get_pe_num():
+    """Get the total number of processing elements (PEs)."""
+    return tir.call_intrin("int32", tir.op.Op.get("tl.GetPENum"))
+
+
+def int_p(dest, value, pe):
+    """Put a single integer value to a remote PE with a very low latency.
+    Args:
+        dest: Symmetric address of the destination data object.
+        value: The value to be transferred to dest.
+        pe: The PE ID of the destination PE.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.IntPE"), dest, value, pe)
+
+
+def barrier_all():
+    """Synchronizes all processing elements (PEs),
+    ensuring completion of all previously issued memory stores and remote memory updates."""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.BarrierAll"))
+
+
+def barrier_all_block(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.BarrierAllBlock"), *args)
+
+
+def barrier_all_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.BarrierAllWarp"), *args)
+
+
+def sync_all():
+    """Synchronizes all processing elements (PEs).
+    In contrast with `barrier_all`,
+    `sync_all` only ensures completion and visibility of previously issued memory stores,
+    and does not ensure completion of remote memory updates issued via NVSHMEM routines."""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.SyncAll"))
+
+
+def sync_all_block(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.SyncAllBlock"), *args)
+
+
+def sync_all_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.SyncAllWarp"), *args)
+
+
+def quiet():
+    """Ensures completion of all operations on symmetric data objects issued by the calling PE."""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.Quiet"))
+
+
+def fence():
+    """Ensures ordering of delivery of operations on symmetric data objects."""
+    return tir.call_intrin("handle", tir.op.Op.get("tl.Fence"))
+
+
+def getmem_nbi_block(dest, src, nelems, pe):
+    """Get data from remote memory to local memory at block granularity without blocking.
+    Args:
+        dest: Symmetric address of the destination data object.
+        src: Symmetric address of the object containing the data to be copied.
+        nelems: Number of elements to be transferred (in bytes).
+        pe: The PE ID of the source PE.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.GetmemNbiBlock"), dest, src, nelems, pe)
+
+
+def getmem_block(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.GetmemBlock"), *args)
+
+
+def getmem_nbi_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.GetmemNbiWarp"), *args)
+
+
+def getmem_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.GetmemWarp"), *args)
+
+
+def getmem_nbi(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.GetmemNbi"), *args)
+
+
+def getmem(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.Getmem"), *args)
+
+
+def putmem_block(*args):
+    """Put data from local memory to remote memory at block granularity.
+    Args:
+        dest: Symmetric address of the destination data object.
+        src: Symmetric address of the object containing the data to be copied.
+        nelems: Number of elements to be transferred (in bytes).
+        pe: The PE ID of the destination PE.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemBlock"), *args)
+
+
+def putmem_nbi_block(dest, src, nelems, pe):
+    """Put data from local memory to remote memory at block granularity without blocking.
+    Args:
+        dest: Symmetric address of the destination data object.
+        src: Symmetric address of the object containing the data to be copied.
+        nelems: Number of elements to be transferred (in bytes).
+        pe: The PE ID of the destination PE.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemNbiBlock"), dest, src, nelems, pe)
+
+
+def putmem_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemWarp"), *args)
+
+
+def putmem_nbi_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemNbiWarp"), *args)
+
+
+def putmem(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.Putmem"), *args)
+
+
+def putmem_nbi(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemNbi"), *args)
+
+
+def putmem_signal(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemSignal"), *args)
+
+
+def putmem_signal_nbi(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemSignalNbi"), *args)
+
+
+def putmem_signal_block(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemSignalBlock"), *args)
+
+
+def putmem_signal_nbi_block(dest, src, nelems, sig_addr, signal, sig_op, pe):
+    """Put data from local memory to remote memory at block granularity without blocking,
+    and update a remote flag on delivery.
+    Args:
+        dest: Symmetric address of the destination data object.
+        src: Symmetric address of the object containing the data to be copied.
+        nelems: Number of elements to be transferred (in bytes).
+        sig_addr: Symmetric address of the remote flag to be updated.
+        signal: The value used for updating the remote signal data object.
+        sig_op: The type of update to be performed on the remote signal data object.
+        pe: The PE ID of the destination PE.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemSignalNbiBlock"), dest, src, nelems,
+                           sig_addr, signal, sig_op, pe)
+
+
+def putmem_signal_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemSignalWarp"), *args)
+
+
+def putmem_signal_nbi_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.PutmemSignalNbiWarp"), *args)
+
+
+def signal_op(sig_addr, signal, sig_op, pe):
+    """Atomically updates `sig_addr` with `signal` using operation `sig_op` on the specified PE.
+    Args:
+        sig_addr: Symmetric address of the signal word to be updated.
+        signal: The value used for updating the remote signal data object.
+        sig_op: The type of update to be performed on the remote signal data object.
+        pe: The PE ID of the destination PE.
+    """
+    return tir.call_intrin("handle", tir.op.Op.get("tl.SignalOp"), sig_addr, signal, sig_op, pe)
+
+
+def signal_wait_until(sig_addr, cmp, cmp_val):
+    """Waits until the signal at `sig_addr` reaches the specified `signal` value on the specified PE.
+    Args:
+        sig_addr (uint64_t*): Symmetric address of the signal word to be waited on.
+        cmp: The comparison operation to be performed on the signal value.
+        cmp_val (uint64_t): The value to compare against the signal value.
+    """
+    # Actually nvshmem_signal_wait_until returns a uint64_t* value, but we simply ignore it
+    return tir.call_intrin("handle", tir.op.Op.get("tl.SignalWaitUntil"), sig_addr, cmp, cmp_val)
+
+
+def broadcast(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.Broadcast"), *args)
+
+
+def broadcast_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.BroadcastWarp"), *args)
+
+
+def broadcast_block(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.BroadcastBlock"), *args)
+
+
+def broadcastmem_block(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.BroadcastmemBlock"), *args)
+
+
+def fcollect(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.Fcollect"), *args)
+
+
+def fcollect_warp(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.FcollectWarp"), *args)
+
+
+def fcollect_block(*args):
+    return tir.call_intrin("handle", tir.op.Op.get("tl.FcollectBlock"), *args)
diff --git a/tilelang/language/experimental/gemm_sp.py b/tilelang/language/experimental/gemm_sp.py
index 5cb6eb83..fc511c00 100644
--- a/tilelang/language/experimental/gemm_sp.py
+++ b/tilelang/language/experimental/gemm_sp.py
@@ -1,16 +1,16 @@
 """The language interface for tl programs."""
+from __future__ import annotations
 
 from tilelang.primitives.gemm.base import GemmWarpPolicy
 import tilelang.language as T
 from tvm import tir
-from typing import Union
 
 
 def gemm_sp(
-    A_sparse: Union[tir.Buffer, tir.Var],
-    E: Union[tir.Buffer, tir.Var],
-    B: Union[tir.Buffer, tir.Var],
-    C: Union[tir.Buffer, tir.Var],
+    A_sparse: tir.Buffer | tir.Var,
+    E: tir.Buffer | tir.Var,
+    B: tir.Buffer | tir.Var,
+    C: tir.Buffer | tir.Var,
     transpose_A: bool = False,
     transpose_B: bool = False,
     policy: GemmWarpPolicy = GemmWarpPolicy.Square,
@@ -42,7 +42,7 @@ def gemm_sp(
         AssertionError: If the K dimensions of matrices A and B don't match
     """
 
-    def legalize_arguments(arg: Union[tir.Buffer, tir.Var]):
+    def legalize_arguments(arg: tir.Buffer | tir.Var):
         """Convert let-bound variables to their corresponding buffers.
 
         Args:
diff --git a/tilelang/language/fastmath.py b/tilelang/language/fastmath.py
new file mode 100644
index 00000000..0146f53a
--- /dev/null
+++ b/tilelang/language/fastmath.py
@@ -0,0 +1,149 @@
+from tvm import tir
+
+
+def __log(x):
+    """Calculate log(x) with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__log"), x)
+
+
+def __log2(x):
+    """Calculate log2(x) with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__log2"), x)
+
+
+def __log10(x):
+    """Calculate log10(x) with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__log10"), x)
+
+
+def __tan(x):
+    """Calculate tan(x) with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__tan"), x)
+
+
+def __cos(x):
+    """Calculate cos(x) with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__cos"), x)
+
+
+def __sin(x):
+    """Calculate sin(x) with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__sin"), x)
+
+
+def __exp10(x):
+    """Calculate 10**x with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__exp10"), x)
+
+
+def __exp(x):
+    """Calculate 2**x with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__exp"), x)
+
+
+__all__ = [
+    "__log",  # noqa: F401
+    "__log2",  # noqa: F401
+    "__log10",  # noqa: F401
+    "__tan",  # noqa: F401
+    "__cos",  # noqa: F401
+    "__sin",  # noqa: F401
+    "__exp10",  # noqa: F401
+    "__exp",  # noqa: F401
+]
diff --git a/tilelang/language/fill.py b/tilelang/language/fill.py
index 123c9026..95ef2674 100644
--- a/tilelang/language/fill.py
+++ b/tilelang/language/fill.py
@@ -1,17 +1,18 @@
 """The language interface for tl programs."""
+from __future__ import annotations
 
 from tvm import tir
-from typing import Union
 from tilelang.language import has_let_value, get_let_value
+from tilelang.utils.language import get_buffer_region_from_load
 
 
-def fill(buffer: Union[tir.Buffer, tir.BufferRegion], value: tir.PrimExpr):
+def fill(buffer: tir.Buffer | tir.BufferRegion, value: tir.PrimExpr):
     """Fill a buffer or buffer region with a specified value.
-    
+
     Args:
         buffer: Either a TVM buffer or buffer region to be filled
         value: The value to fill the buffer with
-    
+
     Returns:
         A TVM intrinsic call that performs the fill operation
     """
@@ -20,15 +21,15 @@ def fill(buffer: Union[tir.Buffer, tir.BufferRegion], value: tir.PrimExpr):
     return tir.call_intrin("handle", tir.op.Op.get("tl.fill"), buffer, value)
 
 
-def clear(buffer: Union[tir.Buffer, tir.Var]):
+def clear(buffer: tir.Buffer | tir.Var):
     """Clear a buffer by filling it with zeros.
-    
+
     Args:
         buffer: Either a TVM buffer or a variable that contains a buffer region
-    
+
     Returns:
         A fill operation that sets the buffer contents to zero
-        
+
     Raises:
         ValueError: If the buffer variable contains an invalid buffer region
     """
@@ -36,6 +37,12 @@ def clear(buffer: Union[tir.Buffer, tir.Var]):
         buffer_region = get_let_value(buffer)  # Get the actual buffer region from variable
         if isinstance(buffer_region, tir.BufferRegion):
             return fill(buffer_region, 0)
+        elif isinstance(buffer_region, tir.BufferLoad):
+            region = get_buffer_region_from_load(buffer_region)
+            if region is None:
+                raise ValueError(
+                    f"Invalid buffer region: {buffer_region}, type: {type(buffer_region)}")
+            return fill(region, 0)
         else:
-            raise ValueError(f"Invalid buffer region: {buffer_region}")
+            raise ValueError(f"Invalid buffer region: {buffer_region}, type: {type(buffer_region)}")
     return fill(buffer, 0)
diff --git a/tilelang/language/frame.py b/tilelang/language/frame.py
index ebc2ee67..8e6d5926 100644
--- a/tilelang/language/frame.py
+++ b/tilelang/language/frame.py
@@ -1,12 +1,12 @@
 """Override the LetFrame to print a message when entering the frame."""
+from __future__ import annotations
 
-from tvm._ffi import register_object as _register_object
+from tvm.ffi import register_object as _register_object
 from tvm.tir import Var, PrimExpr, BufferLoad, BufferRegion
 from tvm.ir import Range
 from tvm import DataType
 from tvm.script.ir_builder.tir.frame import TIRFrame
 from collections import deque
-from typing import Optional
 import threading
 
 
@@ -150,7 +150,7 @@ class LetFrame(TIRFrame):
         super().__exit__(ptype, value, trace)
 
     @classmethod
-    def Current(cls) -> "LetFrame":
+    def Current(cls) -> LetFrame:
         """Get the current (topmost) let frame.
 
         Returns:
@@ -198,7 +198,7 @@ def has_let_value(var: Var) -> bool:
     return _get_let_stack().has_value(var)
 
 
-def get_let_value(var: Var) -> Optional[PrimExpr]:
+def get_let_value(var: Var) -> PrimExpr | None:
     """Get the value bound to a variable in the current let frame stack.
 
     Args:
diff --git a/tilelang/language/gemm.py b/tilelang/language/gemm.py
index 209aac47..bb8dc6ce 100644
--- a/tilelang/language/gemm.py
+++ b/tilelang/language/gemm.py
@@ -1,21 +1,23 @@
 """The language interface for tl programs."""
+from __future__ import annotations
 
 from tilelang.primitives.gemm.base import GemmWarpPolicy
 import tilelang.language as T
 from tvm import tir
-from typing import Union, List
+from tilelang.utils.language import get_buffer_region_from_load
 
 
 def gemm(
-    A: Union[tir.Buffer, tir.Var],
-    B: Union[tir.Buffer, tir.Var],
-    C: Union[tir.Buffer, tir.Var],
+    A: tir.Buffer | tir.Var,
+    B: tir.Buffer | tir.Var,
+    C: tir.Buffer | tir.Var,
     transpose_A: bool = False,
     transpose_B: bool = False,
     policy: GemmWarpPolicy = GemmWarpPolicy.Square,
     clear_accum: bool = False,
     k_pack: int = 1,
     wg_wait: int = 0,
+    mbar: tir.Buffer | None = None,
 ):
     """Perform a General Matrix Multiplication (GEMM) operation.
 
@@ -32,6 +34,9 @@ def gemm(
         clear_accum (bool, optional): Whether to clear accumulator before computation. Defaults to False.
         k_pack (int, optional): Number of k dimensions packed into a single warp. Defaults to 1.
         wg_wait (int, optional): Warp group wait count. Defaults to 0.
+            On hopper it is equivalent to `wgmma.wait_group.sync.aligned <wg_wait>` if wg_wait is not -1
+            On sm100, `wg_wait` can only be 0 or -1. `mbarrier_wait(TCGEN5MMA barrier)` will be appended if wg_wait is 0.
+        mbar (tir.Buffer, optional): mbarrier for TCGEN5MMA synchronization
 
     Returns:
         tir.Call: A handle to the GEMM operation
@@ -40,7 +45,7 @@ def gemm(
         AssertionError: If the K dimensions of matrices A and B don't match
     """
 
-    def legalize_arguments(arg: Union[tir.Buffer, tir.Var]):
+    def legalize_arguments(arg: tir.Buffer | tir.Var):
         """Convert let-bound variables to their corresponding buffers.
 
         Args:
@@ -56,8 +61,9 @@ def gemm(
     A = legalize_arguments(A)
     B = legalize_arguments(B)
     C = legalize_arguments(C)
+    mbar = legalize_arguments(mbar) if mbar is not None else None
 
-    def retrieve_shape(object: Union[tir.Buffer, tir.BufferRegion]) -> List[int]:
+    def retrieve_shape(object: tir.Buffer | tir.BufferRegion) -> list[int]:
         if isinstance(object, tir.Buffer):
             return object.shape
         elif isinstance(object, tir.BufferRegion):
@@ -66,13 +72,51 @@ def gemm(
             for r in region:
                 shape.append(r.extent)
             return shape
+        elif isinstance(object, tir.BufferLoad):
+            region = get_buffer_region_from_load(object).region
+            shape = []
+            for r in region:
+                shape.append(r.extent)
+            return shape
+        else:
+            raise ValueError(
+                f"Unsupported retrieve_shape argument type: {type(object)} for buffer {object}")
+
+    def retrieve_stride(object: tir.Buffer | tir.BufferRegion) -> list[int]:
+        if isinstance(object, tir.Buffer):
+            strides = []
+            stride = 1
+            for s in reversed(object.shape):
+                strides.insert(0, stride)
+                stride *= s
+            return strides
+        elif isinstance(object, tir.BufferRegion):
+            buffer, _ = object.buffer, object.region
+            strides = []
+            stride = 1
+            for s in reversed(buffer.shape):
+                strides.insert(0, stride)
+                stride *= s
+            return strides
+        elif isinstance(object, tir.BufferLoad):
+            buffer = object.buffer
+            strides = []
+            stride = 1
+            for s in reversed(buffer.shape):
+                strides.insert(0, stride)
+                stride *= s
+            return strides
         else:
-            raise ValueError(f"Unsupported argument type: {type(object)} for buffer {object}")
+            raise ValueError(
+                f"Unsupported retrieve_stride argument type: {type(object)} for buffer {object}")
 
     A_shape = retrieve_shape(A)
     B_shape = retrieve_shape(B)
     C_shape = retrieve_shape(C)
 
+    A_stride = retrieve_stride(A)
+    B_stride = retrieve_stride(B)
+
     assert len(C_shape) == 2, "current only support C as a 2D tensor"
     assert len(A_shape) >= 2, "current only support A as a 2D or higher-order tensor"
     assert len(B_shape) >= 2, "current only support B as a 2D or higher-order tensor"
@@ -90,8 +134,10 @@ def gemm(
     K_B = B_shape[-1] if transpose_B else B_shape[-2]
     assert K == K_B, f"T.gemm K shape check failed: K_A = {K}, K_B = {K_B}"
 
-    def retrieve_ptr(object: Union[tir.Buffer, tir.BufferRegion],
-                     access_type: str = "r") -> tir.PrimExpr:
+    stride_a = A_stride[-2]
+    stride_b = B_stride[-2]
+
+    def retrieve_ptr(object: tir.Buffer | tir.BufferRegion, access_type: str = "r") -> tir.PrimExpr:
         if isinstance(object, tir.Buffer):
             return object.access_ptr(access_type)
         elif isinstance(object, tir.BufferRegion):
@@ -105,18 +151,262 @@ def gemm(
                 strides.insert(0, stride)
                 stride *= s
             offset = 0
-            for i in range(len(indices)):
+            # not offset the last two dimension
+            for i in range(len(indices) - 2):
                 offset += indices[i] * strides[i]
             return buffer.access_ptr(access_mask=access_type, offset=offset)
+        elif isinstance(object, tir.BufferLoad):
+            buffer = object.buffer
+            region = get_buffer_region_from_load(object).region
+            indices = []
+            for r in region:
+                indices.append(r.min)
+            strides = []
+            stride = 1
+            for s in reversed(buffer.shape):
+                strides.insert(0, stride)
+                stride *= s
+            offset = 0
+            for i in range(len(indices) - 2):
+                offset += indices[i] * strides[i]
+            return buffer.access_ptr(access_mask=access_type, offset=offset)
+        else:
+            raise ValueError(
+                f"Unsupported retrieve_ptr argument type: {type(object)} for buffer {object}")
+
+    def retrieve_offset(object: tir.Buffer | tir.BufferRegion) -> tir.PrimExpr:
+        """Retrieve the offset of the buffer or buffer region."""
+        if isinstance(object, tir.Buffer):
+            return [0] * len(object.shape)
+        elif isinstance(object, tir.BufferRegion):
+            _, region = object.buffer, object.region
+            indices = []
+            for r in region:
+                indices.append(r.min)
+            return indices
+        elif isinstance(object, tir.BufferLoad):
+            region = get_buffer_region_from_load(object).region
+            indices = []
+            for r in region:
+                indices.append(r.min)
+            return indices
         else:
-            raise ValueError(f"Unsupported argument type: {type(object)} for buffer {object}")
+            raise ValueError(
+                f"Unsupported retrieve_offset argument type: {type(object)} for buffer {object}")
+
+    A_offset = retrieve_offset(A)
+    B_offset = retrieve_offset(B)
+    assert A_offset[-2] == 0, "The offset of the first dimension of A must be 0"
+    assert B_offset[-2] == 0, "The offset of the first dimension of B must be 0"
+    offset_a = A_offset[-1]
+    offset_b = B_offset[-1]
+
+    Aptr = retrieve_ptr(A, "r")
+    Bptr = retrieve_ptr(B, "r")
+    Cptr = retrieve_ptr(C, "rw")
+    mbarptr = retrieve_ptr(mbar, "rw") if mbar is not None else tir.const(0, "uint32")
+    C_coords = [r.min for r in C.region] if isinstance(C, tir.BufferRegion) else [0, 0]
+    return tir.call_intrin("handle", tir.op.Op.get("tl.gemm"), Aptr, Bptr, Cptr, transpose_A,
+                           transpose_B, M, N, K, policy, clear_accum, stride_a, stride_b, offset_a,
+                           offset_b, k_pack, wg_wait, mbarptr, C_coords[0], C_coords[1])
+
+
+# experimental currently, for fast compilation
+def gemm_v2(
+    A: tir.Buffer | tir.Var,
+    B: tir.Buffer | tir.Var,
+    C: tir.Buffer | tir.Var,
+    transpose_A: bool = False,
+    transpose_B: bool = False,
+    policy: GemmWarpPolicy = GemmWarpPolicy.Square,
+    clear_accum: bool = False,
+    k_pack: int = 1,
+    wg_wait: int = 0,
+):
+    """Perform a General Matrix Multiplication (GEMM) operation.
+
+    This function computes C = A @ B where A and B can optionally be transposed.
+    The operation supports various warp policies and accumulation modes.
+
+    Args:
+        A (Union[tir.Buffer, tir.Var]): First input matrix
+        B (Union[tir.Buffer, tir.Var]): Second input matrix
+        C (Union[tir.Buffer, tir.Var]): Output matrix for results
+        transpose_A (bool, optional): Whether to transpose matrix A. Defaults to False.
+        transpose_B (bool, optional): Whether to transpose matrix B. Defaults to False.
+        policy (GemmWarpPolicy, optional): Warp execution policy. Defaults to GemmWarpPolicy.Square.
+        clear_accum (bool, optional): Whether to clear accumulator before computation. Defaults to False.
+        k_pack (int, optional): Number of k dimensions packed into a single warp. Defaults to 1.
+        wg_wait (int, optional): Warp group wait count. Defaults to 0.
+
+    Returns:
+        tir.Call: A handle to the GEMM operation
+
+    Raises:
+        AssertionError: If the K dimensions of matrices A and B don't match
+    """
+
+    def legalize_arguments(arg: tir.Buffer | tir.Var):
+        """Convert let-bound variables to their corresponding buffers.
+
+        Args:
+            arg (Union[tir.Buffer, tir.Var]): Input argument to legalize
+
+        Returns:
+            Union[tir.Buffer, tir.Var]: The legalized argument
+        """
+        if isinstance(arg, tir.Var) and T.has_let_value(arg):
+            return T.get_let_value(arg).buffer
+        return arg
+
+    A = legalize_arguments(A)
+    B = legalize_arguments(B)
+    C = legalize_arguments(C)
+
+    def retrieve_shape(object: tir.Buffer | tir.BufferRegion) -> list[int]:
+        if isinstance(object, tir.Buffer):
+            return object.shape
+        elif isinstance(object, tir.BufferRegion):
+            region = object.region
+            shape = []
+            for r in region:
+                shape.append(r.extent)
+            return shape
+        elif isinstance(object, tir.BufferLoad):
+            region = get_buffer_region_from_load(object).region
+            shape = []
+            for r in region:
+                shape.append(r.extent)
+            return shape
+        else:
+            raise ValueError(
+                f"Unsupported retrieve_shape argument type: {type(object)} for buffer {object}")
+
+    def retrieve_stride(object: tir.Buffer | tir.BufferRegion) -> list[int]:
+        if isinstance(object, tir.Buffer):
+            strides = []
+            stride = 1
+            for s in reversed(object.shape):
+                strides.insert(0, stride)
+                stride *= s
+            return strides
+        elif isinstance(object, tir.BufferRegion):
+            buffer, _ = object.buffer, object.region
+            strides = []
+            stride = 1
+            for s in reversed(buffer.shape):
+                strides.insert(0, stride)
+                stride *= s
+            return strides
+        elif isinstance(object, tir.BufferLoad):
+            buffer = object.buffer
+            strides = []
+            stride = 1
+            for s in reversed(buffer.shape):
+                strides.insert(0, stride)
+                stride *= s
+            return strides
+        else:
+            raise ValueError(
+                f"Unsupported retrieve_stride argument type: {type(object)} for buffer {object}")
+
+    A_shape = retrieve_shape(A)
+    B_shape = retrieve_shape(B)
+    C_shape = retrieve_shape(C)
+
+    A_stride = retrieve_stride(A)
+    B_stride = retrieve_stride(B)
+
+    assert len(C_shape) == 2, "current only support C as a 2D tensor"
+    assert len(A_shape) >= 2, "current only support A as a 2D or higher-order tensor"
+    assert len(B_shape) >= 2, "current only support B as a 2D or higher-order tensor"
+    if len(A_shape) > 2:
+        for i in range(len(A_shape) - 2):
+            assert A_shape[i] == 1, \
+                "current only support A as a 2D or higher-order tensor with the last two dimensions being the matrix dimensions"
+    if len(B_shape) > 2:
+        for i in range(len(B_shape) - 2):
+            assert B_shape[i] == 1, \
+                "current only support B as a 2D or higher-order tensor with the last two dimensions being the matrix dimensions"
+
+    M, N = C_shape
+    K = A_shape[-2] if transpose_A else A_shape[-1]
+    K_B = B_shape[-1] if transpose_B else B_shape[-2]
+    assert K == K_B, f"T.gemm K shape check failed: K_A = {K}, K_B = {K_B}"
+
+    stride_a = A_stride[-2]
+    stride_b = B_stride[-2]
+
+    def retrieve_ptr(object: tir.Buffer | tir.BufferRegion, access_type: str = "r") -> tir.PrimExpr:
+        if isinstance(object, tir.Buffer):
+            return object.access_ptr(access_type)
+        elif isinstance(object, tir.BufferRegion):
+            buffer, region = object.buffer, object.region
+            indices = []
+            for r in region:
+                indices.append(r.min)
+            strides = []
+            stride = 1
+            for s in reversed(buffer.shape):
+                strides.insert(0, stride)
+                stride *= s
+            offset = 0
+            # not offset the last two dimension
+            for i in range(len(indices) - 2):
+                offset += indices[i] * strides[i]
+            return buffer.access_ptr(access_mask=access_type, offset=offset)
+        elif isinstance(object, tir.BufferLoad):
+            buffer = object.buffer
+            region = get_buffer_region_from_load(object).region
+            indices = []
+            for r in region:
+                indices.append(r.min)
+            strides = []
+            stride = 1
+            for s in reversed(buffer.shape):
+                strides.insert(0, stride)
+                stride *= s
+            offset = 0
+            for i in range(len(indices) - 2):
+                offset += indices[i] * strides[i]
+            return buffer.access_ptr(access_mask=access_type, offset=offset)
+        else:
+            raise ValueError(
+                f"Unsupported retrieve_ptr argument type: {type(object)} for buffer {object}")
+
+    def retrieve_offset(object: tir.Buffer | tir.BufferRegion) -> tir.PrimExpr:
+        """Retrieve the offset of the buffer or buffer region."""
+        if isinstance(object, tir.Buffer):
+            return [0] * len(object.shape)
+        elif isinstance(object, tir.BufferRegion):
+            _, region = object.buffer, object.region
+            indices = []
+            for r in region:
+                indices.append(r.min)
+            return indices
+        elif isinstance(object, tir.BufferLoad):
+            region = get_buffer_region_from_load(object).region
+            indices = []
+            for r in region:
+                indices.append(r.min)
+            return indices
+        else:
+            raise ValueError(
+                f"Unsupported retrieve_offset argument type: {type(object)} for buffer {object}")
+
+    A_offset = retrieve_offset(A)
+    B_offset = retrieve_offset(B)
+    assert A_offset[-2] == 0, "The offset of the first dimension of A must be 0"
+    assert B_offset[-2] == 0, "The offset of the first dimension of B must be 0"
+    offset_a = A_offset[-1]
+    offset_b = B_offset[-1]
 
     Aptr = retrieve_ptr(A, "r")
     Bptr = retrieve_ptr(B, "r")
     Cptr = retrieve_ptr(C, "rw")
     return tir.call_intrin(
         "handle",
-        tir.op.Op.get("tl.gemm"),
+        tir.op.Op.get("tl.gemm_py"),
         Aptr,
         Bptr,
         Cptr,
@@ -127,6 +417,10 @@ def gemm(
         K,
         policy,
         clear_accum,
+        stride_a,
+        stride_b,
+        offset_a,
+        offset_b,
         k_pack,
         wg_wait,
     )
diff --git a/tilelang/language/kernel.py b/tilelang/language/kernel.py
index deddfb4c..54b78d3d 100644
--- a/tilelang/language/kernel.py
+++ b/tilelang/language/kernel.py
@@ -1,14 +1,26 @@
 """The language interface for tl programs."""
+from __future__ import annotations
 
-from typing import Union, List, Tuple, Optional
 from collections import deque
 from tvm import tir
 from tvm.tir import Var
 from tvm.script.ir_builder.tir.frame import TIRFrame, BlockFrame
-from tvm._ffi import register_object
+from tvm.ffi import register_object
 from tilelang import _ffi_api
 import threading
 
+# Ensure single-dimension kernel bindings can be unpacked like iterables.
+# especially for issue https://github.com/tile-ai/tilelang/issues/830
+if not hasattr(Var, "__iter__"):
+
+    def _var_iter(self):
+        yield self
+
+    Var.__iter__ = _var_iter  # type: ignore[attr-defined]
+
+if not hasattr(Var, "__len__"):
+    Var.__len__ = lambda self: 1  # type: ignore[attr-defined]
+
 
 class FrameStack:
     """
@@ -68,6 +80,17 @@ def _get_current_stack() -> FrameStack:
     return _local.kernel_launch_frame_stack
 
 
+def _normalize_bindings(bindings: list[Var]) -> Var | list[Var]:
+    """
+    Return a bare Var when we only have a single binding so that users may write either
+    `with T.Kernel(...) as pid:` or `with T.Kernel(...) as (pid,)`.
+    Otherwise, keep the list semantics for multi-dimensional launches.
+    """
+    if len(bindings) == 1:
+        return bindings[0]
+    return bindings
+
+
 @register_object("tl.KernelLaunchFrame")
 class KernelLaunchFrame(TIRFrame):
     """
@@ -75,7 +98,7 @@ class KernelLaunchFrame(TIRFrame):
     and handles the entry and exit of the kernel launch scope.
     """
 
-    def __enter__(self) -> Union[Var, List[Var]]:
+    def __enter__(self) -> Var | list[Var]:
         """
         Enters the KernelLaunchFrame scope and pushes this frame onto the stack.
         Returns one Var if we detect exactly 5 frames (meaning there is a single
@@ -83,9 +106,6 @@ class KernelLaunchFrame(TIRFrame):
         """
         super().__enter__()
         _get_current_stack().push(self)
-        # If we have exactly 5 frames, return the single iter_var.var.
-        if len(self.frames) == 5:
-            return self.frames[0].iter_var.var
 
         last_block_frame = self.frames[-1]
         assert isinstance(last_block_frame,
@@ -95,11 +115,11 @@ class KernelLaunchFrame(TIRFrame):
 
         if maybe_cpu:
             # CPU kernel frame, return a list of for frame items.
-            return [frame.vars[0] for frame in self.frames[0:-1]]
+            return _normalize_bindings([frame.vars[0] for frame in self.frames[0:-1]])
         else:
             # Otherwise, return a list of iter_var.var objects (excluding the last 4 frames).
             # As 4 frames for threadIdx.x, threadIdx.y, threadIdx.z and block frame with attributes
-            return [frame.iter_var.var for frame in self.frames[0:-4]]
+            return _normalize_bindings([frame.iter_var.var for frame in self.frames[0:-4]])
 
     def __exit__(self, ptype, value, trace):
         """
@@ -112,7 +132,7 @@ class KernelLaunchFrame(TIRFrame):
         super().__exit__(ptype, value, trace)
 
     @classmethod
-    def Current(cls) -> Optional["KernelLaunchFrame"]:
+    def Current(cls) -> KernelLaunchFrame | None:
         """
         Returns the topmost (current) KernelLaunchFrame from the stack if it exists,
         or None if the stack is empty.
@@ -128,7 +148,7 @@ class KernelLaunchFrame(TIRFrame):
         iter_var = self.frames[dim].iter_var
         return int(iter_var.dom.extent)
 
-    def get_block_extents(self) -> List[int]:
+    def get_block_extents(self) -> list[int]:
         """
         Returns the block extents for all three dimensions.
         """
@@ -142,7 +162,7 @@ class KernelLaunchFrame(TIRFrame):
         iter_var = self.frames[-4 + dim].iter_var
         return int(iter_var.dom.extent)
 
-    def get_thread_extents(self) -> List[int]:
+    def get_thread_extents(self) -> list[int]:
         """
         Returns the thread extents for all three dimensions.
         """
@@ -155,7 +175,7 @@ class KernelLaunchFrame(TIRFrame):
         """
         return self.frames[-4 + dim].iter_var.var
 
-    def get_thread_bindings(self) -> List[Var]:
+    def get_thread_bindings(self) -> list[Var]:
         """
         Returns the thread binding for the given dimension.
         dim=0 corresponds to threadIdx.x, dim=1 to threadIdx.y, and dim=2 to threadIdx.z.
@@ -178,21 +198,21 @@ class KernelLaunchFrame(TIRFrame):
         """
         return self.frames[dim].iter_var.var
 
-    def get_block_bindings(self) -> List[Var]:
+    def get_block_bindings(self) -> list[Var]:
         """
         Returns all three block bindings.
         """
         return [frame.iter_var.var for frame in self.frames[0:-4]]
 
     @property
-    def blocks(self) -> List[Var]:
+    def blocks(self) -> list[Var]:
         """
         Returns the block indices from the topmost frame.
         """
         return [frame.iter_var.var for frame in self.frames[0:-4]]
 
     @property
-    def threads(self) -> List[Var]:
+    def threads(self) -> list[Var]:
         """
         Returns the thread indices from the topmost frame.
         """
@@ -207,10 +227,10 @@ class KernelLaunchFrame(TIRFrame):
 
 
 def Kernel(
-    *blocks: List[tir.PrimExpr],
-    threads: Optional[Union[int, List[int], Tuple]] = None,
+    *blocks: list[tir.PrimExpr],
+    threads: int | list[int] | tuple | None = None,
     is_cpu: bool = False,
-    prelude: Optional[str] = None,
+    prelude: str | None = None,
 ):
     """Tools to quickly construct a GPU kernel launch frame.
 
@@ -234,6 +254,31 @@ def Kernel(
     -------
     res : Tuple[frame.LaunchThreadFrame]
         The result LaunchThreadFrame.
+
+    Examples
+    --------
+    Create a 1-D CUDA kernel launch and unpack the single block index:
+
+    .. code-block:: python
+
+        with T.Kernel(T.ceildiv(N, 128), threads=128) as bx:
+            # bx is the blockIdx.x binding (also iterable as (bx,))
+            ...
+
+    Launch a 2-D grid while requesting two thread dimensions:
+
+    .. code-block:: python
+
+        with T.Kernel(grid_x, grid_y, threads=(64, 2)) as (bx, by):
+            tx, ty = T.get_thread_bindings()
+            ...
+
+    Emit a CPU kernel where thread bindings are skipped:
+
+    .. code-block:: python
+
+        with T.Kernel(loop_extent, is_cpu=True) as (i,):
+            ...
     """
     attrs: dict = {}
 
@@ -261,46 +306,54 @@ def Kernel(
 def get_thread_binding(dim: int = 0) -> Var:
     """Returns the thread binding for the given dimension.
     """
+    assert KernelLaunchFrame.Current() is not None, "KernelLaunchFrame is not initialized"
     return KernelLaunchFrame.Current().get_thread_binding(dim)
 
 
-def get_thread_bindings() -> List[Var]:
+def get_thread_bindings() -> list[Var]:
     """Returns all three thread bindings.
     """
+    assert KernelLaunchFrame.Current() is not None, "KernelLaunchFrame is not initialized"
     return KernelLaunchFrame.Current().get_thread_bindings()
 
 
 def get_block_binding(dim: int = 0) -> Var:
     """Returns the block binding for the given dimension.
     """
+    assert KernelLaunchFrame.Current() is not None, "KernelLaunchFrame is not initialized"
     return KernelLaunchFrame.Current().get_block_binding(dim)
 
 
-def get_block_bindings() -> List[Var]:
+def get_block_bindings() -> list[Var]:
     """Returns all three block bindings.
     """
+    assert KernelLaunchFrame.Current() is not None, "KernelLaunchFrame is not initialized"
     return KernelLaunchFrame.Current().get_block_bindings()
 
 
 def get_thread_extent(dim: int = 0) -> int:
     """Returns the thread extent for the given dimension.
     """
+    assert KernelLaunchFrame.Current() is not None, "KernelLaunchFrame is not initialized"
     return KernelLaunchFrame.Current().get_thread_extent(dim)
 
 
-def get_thread_extents() -> List[int]:
+def get_thread_extents() -> list[int]:
     """Returns all three thread extents.
     """
+    assert KernelLaunchFrame.Current() is not None, "KernelLaunchFrame is not initialized"
     return KernelLaunchFrame.Current().get_thread_extents()
 
 
 def get_block_extent(dim: int = 0) -> int:
     """Returns the block extent for the given dimension.
     """
+    assert KernelLaunchFrame.Current() is not None, "KernelLaunchFrame is not initialized"
     return KernelLaunchFrame.Current().get_block_extent(dim)
 
 
-def get_block_extents() -> List[int]:
+def get_block_extents() -> list[int]:
     """Returns all three block extents.
     """
+    assert KernelLaunchFrame.Current() is not None, "KernelLaunchFrame is not initialized"
     return KernelLaunchFrame.Current().get_block_extents()
diff --git a/tilelang/language/logical.py b/tilelang/language/logical.py
index 1af6f04c..a09088e6 100644
--- a/tilelang/language/logical.py
+++ b/tilelang/language/logical.py
@@ -1,19 +1,18 @@
 """The language interface for tl programs."""
+from __future__ import annotations
 
 from tilelang import language as T
-from tvm.tir import Buffer, BufferRegion
-from tvm.ir import Range
+from tvm.tir import Buffer, BufferRegion, BufferLoad
 from tvm import tir
-from typing import Union
 from tilelang.utils.language import get_buffer_elems
 
 
-def any_of(buffer: Union[T.Tensor, BufferRegion]):
+def any_of(buffer: T.Tensor | BufferRegion):
     """Check if any element in the buffer is true.
-    
+
     Args:
         buffer: Either a TVM buffer or buffer region to be checked
-    
+
     Returns:
         A TVM intrinsic call that performs the any operation
     """
@@ -28,26 +27,27 @@ def any_of(buffer: Union[T.Tensor, BufferRegion]):
         for i, r in enumerate(region):
             extent = r.extent
             if extent == 1:
-                new_region.append(r)
+                new_region.append(r.min)
             else:
                 # check the idx is the last dimension
                 if i != len(region) - 1:
                     raise ValueError(
                         "Only support the last dimension to be for T.any currently, please contact us if you need this feature"
                     )
-                new_region.append(Range(r.min, 1))
-        buffer = BufferRegion(buffer, new_region)
-        return T.call_intrin(return_type, tir.op.Op.get("tl.any_of"), T.address_of(buffer), extent)
+                new_region.append(r.min)
+        buffer_load = BufferLoad(buffer, new_region)
+        return T.call_intrin(return_type, tir.op.Op.get("tl.any_of"), T.address_of(buffer_load),
+                             extent)
     else:
         raise ValueError(f"Invalid buffer type: {type(buffer)}")
 
 
-def all_of(buffer: Union[T.Tensor, BufferRegion]):
+def all_of(buffer: T.Tensor | BufferRegion):
     """Check if all elements in the buffer are true.
-    
+
     Args:
         buffer: Either a TVM buffer or buffer region to be checked
-    
+
     Returns:
         A TVM intrinsic call that performs the any operation
     """
@@ -62,15 +62,16 @@ def all_of(buffer: Union[T.Tensor, BufferRegion]):
         for i, r in enumerate(region):
             extent = r.extent
             if extent == 1:
-                new_region.append(r)
+                new_region.append(r.min)
             else:
                 # check the idx is the last dimension
                 if i != len(region) - 1:
                     raise ValueError(
                         "Only support the last dimension to be for T.any currently, please contact us if you need this feature"
                     )
-                new_region.append(Range(r.min, 1))
-        buffer = BufferRegion(buffer, new_region)
-        return T.call_intrin(return_type, tir.op.Op.get("tl.all_of"), T.address_of(buffer), extent)
+                new_region.append(r.min)
+        buffer_load = BufferLoad(buffer, new_region)
+        return T.call_intrin(return_type, tir.op.Op.get("tl.all_of"), T.address_of(buffer_load),
+                             extent)
     else:
         raise ValueError(f"Invalid buffer type: {type(buffer)}")
diff --git a/tilelang/language/math_intrinsics.py b/tilelang/language/math_intrinsics.py
new file mode 100644
index 00000000..39cab27a
--- /dev/null
+++ b/tilelang/language/math_intrinsics.py
@@ -0,0 +1,350 @@
+from tvm import tir
+
+
+def _validate_rounding_mode(rounding_mode):
+    """Validate that the rounding mode is one of the supported IEEE modes"""
+    valid_modes = {'rn', 'rz', 'ru', 'rd'}
+    if isinstance(rounding_mode, str) and rounding_mode in valid_modes:
+        return
+    raise ValueError(f"Invalid rounding mode '{rounding_mode}'. Must be one of: {valid_modes}")
+
+
+def __log(x):
+    """Calculate log(x) with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__log"), x)
+
+
+def __log2(x):
+    """Calculate log2(x) with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__log2"), x)
+
+
+def __log10(x):
+    """Calculate log10(x) with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__log10"), x)
+
+
+def __tan(x):
+    """Calculate tan(x) with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__tan"), x)
+
+
+def __cos(x):
+    """Calculate cos(x) with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__cos"), x)
+
+
+def __sin(x):
+    """Calculate sin(x) with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__sin"), x)
+
+
+def __exp10(x):
+    """Calculate 10**x with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__exp10"), x)
+
+
+def __exp(x):
+    """Calculate 2**x with fast math
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input argument.
+
+    Returns
+    -------
+    y : PrimExpr
+        The result.
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.__exp"), x)
+
+
+# IEEE-compliant operations
+def ieee_add(x, y, rounding_mode="rn"):
+    """IEEE-compliant addition with specified rounding mode
+
+    Parameters
+    ----------
+    x : PrimExpr
+        First operand.
+    y : PrimExpr
+        Second operand.
+    rounding_mode : str, optional
+        Rounding mode: 'rn' (round to nearest), 'rz' (round toward zero),
+        'ru' (round toward positive infinity), 'rd' (round toward negative infinity).
+        Default is 'rn'.
+
+    Returns
+    -------
+    result : PrimExpr
+        The result.
+    """
+    _validate_rounding_mode(rounding_mode)
+    x = tir.convert(x)
+    y = tir.convert(y)
+    rounding_mode = tir.convert(rounding_mode)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.ieee_add"), x, y, rounding_mode)
+
+
+def ieee_sub(x, y, rounding_mode="rn"):
+    """IEEE-compliant subtraction with specified rounding mode
+
+    Parameters
+    ----------
+    x : PrimExpr
+        First operand.
+    y : PrimExpr
+        Second operand.
+    rounding_mode : str, optional
+        Rounding mode: 'rn', 'rz', 'ru', 'rd'. Default is 'rn'.
+
+    Returns
+    -------
+    result : PrimExpr
+        The result.
+    """
+    _validate_rounding_mode(rounding_mode)
+    x = tir.convert(x)
+    y = tir.convert(y)
+    rounding_mode = tir.convert(rounding_mode)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.ieee_sub"), x, y, rounding_mode)
+
+
+def ieee_mul(x, y, rounding_mode="rn"):
+    """IEEE-compliant multiplication with specified rounding mode
+
+    Parameters
+    ----------
+    x : PrimExpr
+        First operand.
+    y : PrimExpr
+        Second operand.
+    rounding_mode : str, optional
+        Rounding mode: 'rn', 'rz', 'ru', 'rd'. Default is 'rn'.
+
+    Returns
+    -------
+    result : PrimExpr
+        The result.
+    """
+    _validate_rounding_mode(rounding_mode)
+    x = tir.convert(x)
+    y = tir.convert(y)
+    rounding_mode = tir.convert(rounding_mode)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.ieee_mul"), x, y, rounding_mode)
+
+
+def ieee_fmaf(x, y, z, rounding_mode="rn"):
+    """IEEE-compliant fused multiply-add with specified rounding mode
+
+    Parameters
+    ----------
+    x : PrimExpr
+        First operand.
+    y : PrimExpr
+        Second operand.
+    z : PrimExpr
+        Third operand (addend).
+    rounding_mode : str, optional
+        Rounding mode: 'rn', 'rz', 'ru', 'rd'. Default is 'rn'.
+
+    Returns
+    -------
+    result : PrimExpr
+        The result of x * y + z.
+    """
+    _validate_rounding_mode(rounding_mode)
+    x = tir.convert(x)
+    y = tir.convert(y)
+    z = tir.convert(z)
+    rounding_mode = tir.convert(rounding_mode)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.ieee_fmaf"), x, y, z, rounding_mode)
+
+
+def ieee_frcp(x, rounding_mode="rn"):
+    """IEEE-compliant reciprocal with specified rounding mode
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input operand.
+    rounding_mode : str, optional
+        Rounding mode: 'rn', 'rz', 'ru', 'rd'. Default is 'rn'.
+
+    Returns
+    -------
+    result : PrimExpr
+        The result of 1/x.
+    """
+    _validate_rounding_mode(rounding_mode)
+    x = tir.convert(x)
+    rounding_mode = tir.convert(rounding_mode)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.ieee_frcp"), x, rounding_mode)
+
+
+def ieee_fsqrt(x, rounding_mode="rn"):
+    """IEEE-compliant square root with specified rounding mode
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input operand.
+    rounding_mode : str, optional
+        Rounding mode: 'rn', 'rz', 'ru', 'rd'. Default is 'rn'.
+
+    Returns
+    -------
+    result : PrimExpr
+        The result of sqrt(x).
+    """
+    _validate_rounding_mode(rounding_mode)
+    x = tir.convert(x)
+    rounding_mode = tir.convert(rounding_mode)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.ieee_fsqrt"), x, rounding_mode)
+
+
+def ieee_frsqrt(x):
+    """IEEE-compliant reciprocal square root (round to nearest only)
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Input operand.
+
+    Returns
+    -------
+    result : PrimExpr
+        The result of 1/sqrt(x).
+    """
+    x = tir.convert(x)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.ieee_frsqrt"), x)
+
+
+def ieee_fdiv(x, y, rounding_mode="rn"):
+    """IEEE-compliant division with specified rounding mode
+
+    Parameters
+    ----------
+    x : PrimExpr
+        Dividend.
+    y : PrimExpr
+        Divisor.
+    rounding_mode : str, optional
+        Rounding mode: 'rn', 'rz', 'ru', 'rd'. Default is 'rn'.
+
+    Returns
+    -------
+    result : PrimExpr
+        The result of x/y.
+    """
+    _validate_rounding_mode(rounding_mode)
+    x = tir.convert(x)
+    y = tir.convert(y)
+    rounding_mode = tir.convert(rounding_mode)
+    return tir.call_intrin(x.dtype, tir.op.Op.get("tl.ieee_fdiv"), x, y, rounding_mode)
+
+
+__all__ = [
+    "__log",  # noqa: F401
+    "__log2",  # noqa: F401
+    "__log10",  # noqa: F401
+    "__tan",  # noqa: F401
+    "__cos",  # noqa: F401
+    "__sin",  # noqa: F401
+    "__exp10",  # noqa: F401
+    "__exp",  # noqa: F401
+    "ieee_add",  # noqa: F401
+    "ieee_sub",  # noqa: F401
+    "ieee_mul",  # noqa: F401
+    "ieee_fmaf",  # noqa: F401
+    "ieee_frcp",  # noqa: F401
+    "ieee_fsqrt",  # noqa: F401
+    "ieee_frsqrt",  # noqa: F401
+    "ieee_fdiv",  # noqa: F401
+]
diff --git a/tilelang/language/memscope.py b/tilelang/language/memscope.py
deleted file mode 100644
index 15535388..00000000
--- a/tilelang/language/memscope.py
+++ /dev/null
@@ -1,18 +0,0 @@
-from tvm._ffi.registry import register_func
-from tvm.ir import make_node
-
-
-@register_func("tvm.info.mem.local.var")
-def mem_info_local_var():
-    """Get memory information for local variable memory.
-
-    Returns:
-        tvm.ir.make_node: A node containing memory information
-    """
-    return make_node(
-        "MemoryInfo",
-        unit_bits=8,
-        max_num_bits=64,
-        max_simd_bits=128,
-        head_address=None,
-    )
diff --git a/tilelang/language/overrides/__init__.py b/tilelang/language/overrides/__init__.py
new file mode 100644
index 00000000..c900642f
--- /dev/null
+++ b/tilelang/language/overrides/__init__.py
@@ -0,0 +1,8 @@
+"""TileLang-specific runtime overrides.
+
+Importing this package registers custom handlers that extend or override
+behavior from upstream TVMScript for TileLang semantics.
+"""
+
+# Register parser overrides upon import.
+from . import parser  # noqa: F401
diff --git a/tilelang/language/overrides/parser.py b/tilelang/language/overrides/parser.py
new file mode 100644
index 00000000..28cb9d55
--- /dev/null
+++ b/tilelang/language/overrides/parser.py
@@ -0,0 +1,206 @@
+"""TVMScript parser overrides tailored for TileLang."""
+from __future__ import annotations
+
+from functools import partial
+
+from tvm.script.ir_builder import tir as T
+from tvm.script.parser._core import dispatch, doc
+from tvm.tir import BufferLoad, Var
+
+from tvm.script.parser.tir import parser as tvm_tir_parser
+from tilelang.language.tir.ir import SerialStepSpec
+
+
+def _get_node_span(node: doc.AST) -> tuple[int, int, int, int]:
+    """Return the span (lineno, col, end_lineno, end_col) for a doc node."""
+    return (node.lineno, node.col_offset, node.end_lineno, node.end_col_offset)
+
+
+# Original implementation located at
+# 3rdparty/tvm/python/tvm/script/parser/tir/parser.py (visit_assign).
+@dispatch.register(token="tir", type_name="Assign")
+def tilelang_visit_assign(self, node: doc.Assign) -> None:  # pylint: disable=unused-argument
+    """Override `Assign` to support chained writes and `local.var` buffers."""
+    if not node.targets:
+        self.report_error(node, "Assignment must have at least one target.")
+
+    if isinstance(node.value, doc.Subscript):
+        check_slices = []
+        if isinstance(node.value.slice, doc.Slice):
+            check_slices = [node.value.slice]
+        elif isinstance(node.value.slice, doc.Tuple):
+            for part in node.value.slice.elts:
+                if isinstance(part, doc.Slice):
+                    check_slices.append(part)
+        for slice_node in check_slices:
+            if not slice_node.step and slice_node.upper and slice_node.lower:
+                slice_node.step = doc.Constant(
+                    1,
+                    None,
+                    1,
+                    1,
+                    slice_node.upper.lineno,
+                    slice_node.upper.end_col_offset + 1,
+                    slice_node.upper.lineno,
+                    slice_node.upper.end_col_offset + 2,
+                )
+
+    rhs = self.eval_expr(node.value)
+    for lhs in node.targets:
+        if isinstance(lhs, doc.Subscript):
+            if isinstance(lhs.slice, doc.Tuple):
+                indices = [self.eval_expr(index) for index in lhs.slice.elts]
+            else:
+                indices = self.eval_expr(lhs.slice)
+            T.buffer_store(self.eval_expr(lhs.value), rhs, indices)
+            continue
+
+        if isinstance(lhs, doc.Name) and lhs.id in self.var_table.get():
+            load_ctx = doc.Load()
+            store_ctx = doc.Store()
+            lhs.ctx = load_ctx
+            lhs_value = self.eval_expr(lhs)
+            lhs.ctx = store_ctx
+            if (isinstance(lhs_value, BufferLoad) and lhs_value.buffer.scope() == "local.var" and
+                    len(lhs_value.indices) == 1 and lhs_value.indices[0] == 0):
+                T.buffer_store(lhs_value.buffer, rhs, indices=[0])
+                continue
+
+        self.eval_assign(target=lhs, source=rhs, bind_value=tvm_tir_parser.bind_assign_value)
+
+
+# Original implementation located at
+# 3rdparty/tvm/python/tvm/script/parser/tir/parser.py (visit_aug_assign).
+@dispatch.register(token="tir", type_name="AugAssign")
+def tilelang_visit_aug_assign(self, node: doc.AugAssign) -> None:  # pylint: disable=unused-argument
+    """Override `AugAssign` to support writes into `local.var` buffers."""
+    lhs_pos = _get_node_span(node.target)
+    rhs_pos = _get_node_span(node.value)
+
+    node.target.ctx = doc.Load()
+    with self.var_table.with_frame():
+        lhs_name = "__tvm_tmp_value_aug_assign_lhs"
+        rhs_name = "__tvm_tmp_value_aug_assign_rhs"
+        lhs_expr = self.eval_expr(node.target)
+        rhs_expr = self.eval_expr(node.value)
+        self.var_table.add(lhs_name, lhs_expr)
+        self.var_table.add(rhs_name, rhs_expr)
+        op = doc.BinOp(
+            doc.Name(lhs_name, doc.Load(), *lhs_pos),
+            node.op,
+            doc.Name(rhs_name, doc.Load(), *rhs_pos),
+            *lhs_pos,
+        )
+        rhs = self.eval_expr(op)
+
+    lhs = node.target
+    lhs.ctx = doc.Store()
+    if isinstance(lhs, doc.Subscript):
+        if isinstance(lhs.slice, doc.Tuple):
+            indices = [self.eval_expr(index) for index in lhs.slice.elts]
+        else:
+            indices = [self.eval_expr(lhs.slice)]
+        T.buffer_store(self.eval_expr(lhs.value), rhs, indices)
+        return
+
+    if isinstance(lhs, doc.Name) and lhs.id in self.var_table.get():
+        load_ctx = doc.Load()
+        store_ctx = doc.Store()
+        lhs.ctx = load_ctx
+        lhs_value = self.eval_expr(lhs)
+        lhs.ctx = store_ctx
+        if (isinstance(lhs_value, BufferLoad) and lhs_value.buffer.scope() == "local.var" and
+                len(lhs_value.indices) == 1 and lhs_value.indices[0] == 0):
+            T.buffer_store(lhs_value.buffer, rhs, indices=[0])
+            return
+
+    self.eval_assign(target=lhs, source=rhs, bind_value=tvm_tir_parser.bind_assign_value)
+
+
+# Original implementation located at
+# 3rdparty/tvm/python/tvm/script/parser/tir/parser.py (visit_ann_assign).
+@dispatch.register(token="tir", type_name="AnnAssign")
+def tilelang_visit_ann_assign(self, node: doc.AnnAssign) -> None:  # pylint: disable=unused-argument
+    """Override `AnnAssign` to support writes into `local.var` buffers."""
+    lhs = node.target
+    rhs = self.eval_expr(node.value)
+    ann_var = self.visit_tvm_annotation(node.annotation)
+    if not isinstance(ann_var, Var):
+        self.report_error(node.annotation, "Annotation should be Var")
+
+    if isinstance(lhs, doc.Name) and lhs.id in self.var_table.get():
+        load_ctx = doc.Load()
+        store_ctx = doc.Store()
+        lhs.ctx = load_ctx
+        lhs_value = self.eval_expr(lhs)
+        lhs.ctx = store_ctx
+        if (isinstance(lhs_value, BufferLoad) and lhs_value.buffer.scope() == "local.var" and
+                len(lhs_value.indices) == 1 and lhs_value.indices[0] == 0):
+            T.buffer_store(lhs_value.buffer, rhs, indices=[0])
+            return
+
+    self.eval_assign(target=lhs, source=ann_var, bind_value=tvm_tir_parser.bind_assign_value)
+    frame = T.LetStmt(rhs, var=ann_var)
+    frame.add_callback(partial(frame.__exit__, None, None, None))
+    frame.__enter__()
+
+
+# Override For to support stepped serial: T.serial(start, end, step)
+@dispatch.register(token="tir", type_name="For")
+def tilelang_visit_for(self, node: doc.For) -> None:  # pylint: disable=unused-argument
+    """Override `For` to add support for T.serial(start, end, step).
+
+    When the iterable is a SerialStepSpec, lower it to a unit-step loop over
+    t in [0, floor_div(|end-start|, step)] and bind the loop variable using a
+    Let to `start + t*step` (inclusive semantics).
+    """
+    iter_val = self.eval_expr(node.iter)
+
+    # Fast path: fall back to TVM default behavior when not a SerialStepSpec
+    if not isinstance(iter_val, SerialStepSpec):
+        if not isinstance(iter_val, T.frame.ForFrame):
+            self.report_error(
+                node.iter,
+                "Expect the for loop to be one of the following: "
+                "range, T.serial, T.grid, T.parallel, T.vectorized, T.unroll, T.thread_binding",
+            )
+        with self.var_table.with_frame(), iter_val as iters:
+            self.eval_assign(
+                target=node.target, source=iters, bind_value=tvm_tir_parser.bind_for_value)
+            self.visit_body(node.body)
+        return
+
+    # Stepped inclusive serial: require positive integer step
+    start = iter_val.start
+    end = iter_val.stop
+    step = iter_val.step
+    annotations = iter_val.annotations
+
+    # Normalize step to Python int if possible, otherwise expect IntImm-like
+    if isinstance(step, int):
+        step_val = step
+    else:
+        step_val = getattr(step, "value", None)
+        if step_val is None:
+            self.report_error(node.iter, "T.serial step must be an integer or IntImm")
+            return
+
+    if step_val <= 0:
+        self.report_error(node.iter, "T.serial step must be a positive integer")
+        return
+
+    # Use tvm.tir.floordiv via builder ops from tilelang.tir.ir if available
+    # Avoid importing op wrappers; compute using arithmetic to keep it simple.
+    # We construct: T.ceildiv((end - start), step)
+    extent = T.ceildiv(end - start, step_val)  # type: ignore[operator]
+
+    for_frame = T.serial(0, extent, annotations=annotations)
+    with self.var_table.with_frame(), for_frame as t:
+        # Bind loop target as Let var: i = start + t * step
+        stepped_index = start + t * step_val  # type: ignore[operator]
+        self.eval_assign(
+            target=node.target,
+            source=stepped_index,
+            bind_value=tvm_tir_parser.bind_assign_value,
+        )
+        self.visit_body(node.body)
diff --git a/tilelang/language/parallel.py b/tilelang/language/parallel.py
index a70846a6..8173675a 100644
--- a/tilelang/language/parallel.py
+++ b/tilelang/language/parallel.py
@@ -1,11 +1,12 @@
 """The language interface for tl programs."""
+from __future__ import annotations
 
-from typing import Optional, Dict, Any
+from typing import Any
 from tvm import tir
 from tilelang import _ffi_api
 
 
-def Parallel(*extents: tir.PrimExpr, coalesced_width: Optional[int] = None):
+def Parallel(*extents: tir.PrimExpr, coalesced_width: int | None = None):
     """Tools to construct nested parallel for loop.
        This can be used to create element-wise tensor expression.
 
@@ -22,7 +23,7 @@ def Parallel(*extents: tir.PrimExpr, coalesced_width: Optional[int] = None):
     res : frame.ForFrame
         The ForFrame.
     """
-    annotations: Dict[str, Any] = {}
+    annotations: dict[str, Any] = {}
     if coalesced_width is not None:
         annotations.update({"coalesced_width": coalesced_width})
     return _ffi_api.Parallel(extents, annotations)  # type: ignore[attr-defined] # pylint: disable=no-member
diff --git a/tilelang/language/parser/operation.py b/tilelang/language/parser/operation.py
index 9b5a67a7..43774947 100644
--- a/tilelang/language/parser/operation.py
+++ b/tilelang/language/parser/operation.py
@@ -17,18 +17,17 @@
 # This file is modified from the original version,
 # which is part of the TVM project (https://tvm.apache.org/).
 """The tir expression operation registration"""
-
-from typing import Type
+from __future__ import annotations
 
 from tvm import tir
-from tvm._ffi.runtime_ctypes import DataType, DataTypeCode
+from tvm.ffi.runtime_ctypes import DataType, DataTypeCode
 from tvm.tir import IntImm
 from tvm.tir.expr import FloatImm
 
 from tvm.script.parser._core import OpMethod, doc, register_op
 
 
-def _register_expr_op(ty: Type):  # pylint: disable=invalid-name
+def _register_expr_op(ty: type):  # pylint: disable=invalid-name
     ty._dispatch_type = ty  # pylint: disable=protected-access
 
     def _and(a, b):
@@ -88,10 +87,10 @@ def _register_expr_op(ty: Type):  # pylint: disable=invalid-name
 
         if DataType(a.dtype).lanes == DataType(b.dtype).lanes:
             return op(a, b)
-        elif DataType(a.dtype).lanes == 1 and DataType(a.dtype).lanes != DataType(b.dtype).lanes:
+        elif (DataType(a.dtype).lanes == 1 and DataType(a.dtype).lanes != DataType(b.dtype).lanes):
             broadcast_a = tir.Broadcast(a, DataType(b.dtype).lanes)
             return op(broadcast_a, b)
-        elif DataType(b.dtype).lanes == 1 and DataType(a.dtype).lanes != DataType(b.dtype).lanes:
+        elif (DataType(b.dtype).lanes == 1 and DataType(a.dtype).lanes != DataType(b.dtype).lanes):
             broadcast_b = tir.Broadcast(b, DataType(a.dtype).lanes)
             return op(a, broadcast_b)
         else:
@@ -115,7 +114,7 @@ def _register_expr_op(ty: Type):  # pylint: disable=invalid-name
     def _ge(a, b):
         return _auto_broadcast(a, b, tir.GE)
 
-    def r(op: Type, i: int, m: OpMethod):  # pylint: disable=invalid-name
+    def r(op: type, i: int, m: OpMethod):  # pylint: disable=invalid-name
         register_op(ty, op, i)(m)
 
     for i in [0, 1]:
diff --git a/tilelang/language/persistent.py b/tilelang/language/persistent.py
index 1761cfa5..0ee7f112 100644
--- a/tilelang/language/persistent.py
+++ b/tilelang/language/persistent.py
@@ -1,15 +1,15 @@
 """The language interface for tl programs."""
+from __future__ import annotations
 
-from typing import List, Optional
 from tvm import tir
 from tilelang import _ffi_api
 
 
 def Persistent(
-    domain: List[tir.PrimExpr],
+    domain: list[tir.PrimExpr],
     wave_size: tir.PrimExpr,
     index: tir.PrimExpr,
-    group_size: Optional[tir.PrimExpr] = 8,
+    group_size: tir.PrimExpr | None = 8,
 ):
     """Tools to construct persistent for loop.
 
diff --git a/tilelang/language/pipeline.py b/tilelang/language/pipeline.py
index 85fd90cc..895ed914 100644
--- a/tilelang/language/pipeline.py
+++ b/tilelang/language/pipeline.py
@@ -1,6 +1,6 @@
 """The language interface for tl programs."""
+from __future__ import annotations
 
-from typing import List, Optional
 from tvm import tir
 from tvm.tir import IntImm
 from tilelang import _ffi_api
@@ -10,10 +10,10 @@ def Pipelined(
     start: tir.PrimExpr,
     stop: tir.PrimExpr = None,
     num_stages: int = 0,
-    order: Optional[List[int]] = None,
-    stage: Optional[List[int]] = None,
-    sync: Optional[List[List[int]]] = None,
-    group: Optional[List[List[int]]] = None,
+    order: list[int] | None = None,
+    stage: list[int] | None = None,
+    sync: list[list[int]] | None = None,
+    group: list[list[int]] | None = None,
 ):
     """Tools to construct pipelined for loop.
 
diff --git a/tilelang/language/print.py b/tilelang/language/print.py
index fde480e5..6c473aa1 100644
--- a/tilelang/language/print.py
+++ b/tilelang/language/print.py
@@ -7,17 +7,17 @@ from tvm import tir
 from typing import Any
 from tilelang.language.kernel import get_thread_bindings
 from tilelang.language import copy, macro, serial, alloc_shared
-from tilelang.intrinsics.utils import index_to_coordinates
+from tilelang.language.utils import index_to_coordinates
 
 
 @macro
 def print_var(var: tir.PrimExpr, msg: str = "") -> tir.PrimExpr:
     """
     Prints the value of a TIR primitive expression (PrimExpr) for debugging purposes.
-    
+
     Parameters:
         var (tir.PrimExpr): The variable or expression to be printed.
-        
+
     Returns:
         tir.PrimExpr: The TIR expression for the debug print operation.
     """
@@ -30,11 +30,11 @@ def print_var_with_condition(condition: tir.PrimExpr,
                              msg: str = "") -> tir.PrimExpr:
     """
     Conditionally prints a TIR primitive expression (PrimExpr) if a given condition is True.
-    
+
     Parameters:
         condition (tir.PrimExpr): A TIR expression representing the condition to check.
         var (tir.PrimExpr): The variable or expression to be printed.
-        
+
     Returns:
         tir.PrimExpr: The TIR expression for the debug print operation, if the condition is True.
     """
@@ -67,12 +67,12 @@ def print_shared_buffer_with_condition(condition: tir.PrimExpr,
                                        msg: str = "") -> tir.PrimExpr:
     """
     Conditionally prints the values of a flattened TIR buffer if the condition is True.
-    
+
     Parameters:
         condition (tir.PrimExpr): A TIR expression representing the condition to check.
         buffer (tir.Buffer): The buffer whose values need to be printed.
         elems (int): The number of elements in the buffer to print.
-        
+
     Returns:
         tir.PrimExpr: The TIR expression for the debug print operation.
     """
@@ -91,12 +91,12 @@ def print_fragment_buffer_with_condition(condition: tir.PrimExpr,
                                          msg: str = "") -> tir.PrimExpr:
     """
     Conditionally prints the values of a flattened TIR buffer if the condition is True.
-    
+
     Parameters:
         condition (tir.PrimExpr): A TIR expression representing the condition to check.
         buffer (tir.Buffer): The buffer whose values need to be printed.
         elems (int): The number of elements in the buffer to print.
-        
+
     Returns:
         tir.PrimExpr: The TIR expression for the debug print operation.
     """
@@ -116,12 +116,12 @@ def print_local_buffer_with_condition(condition: tir.PrimExpr,
                                       msg: str = "") -> tir.PrimExpr:
     """
     Conditionally prints the values of a flattened TIR buffer if the condition is True.
-    
+
     Parameters:
         condition (tir.PrimExpr): A TIR expression representing the condition to check.
         buffer (tir.Buffer): The buffer whose values need to be printed.
         elems (int): The number of elements in the buffer to print.
-        
+
     Returns:
         tir.PrimExpr: The TIR expression for the debug print operation.
     """
@@ -133,23 +133,31 @@ def print_local_buffer_with_condition(condition: tir.PrimExpr,
                             buffer[coords])
 
 
-def print(obj: Any, msg: str = "", warp_group_id: int = 0, warp_id: int = 0) -> tir.PrimExpr:
+@macro
+def print_msg(msg: str) -> tir.PrimExpr:
+    """
+    Prints a message for debugging purposes.
+    """
+    tir.call_extern("handle", "debug_print_msg", msg)
+
+
+def print(obj: Any = None, msg: str = "", warp_group_id: int = 0, warp_id: int = 0) -> tir.PrimExpr:
     """
     A generic print function that handles both TIR buffers and primitive expressions.
-    
+
     - If the input is a TIR buffer, it prints its values, but only on the first thread (tx=0, ty=0, tz=0).
     - If the input is a TIR primitive expression, it prints its value directly.
-    
+
     Parameters:
-        obj (Any): The object to print. It can be either a tir.Buffer or tir.PrimExpr.
+        obj (Any): The object to print. It can be either a tir.Buffer, tir.PrimExpr or None.
         msg (str): An optional message to include in the print statement.
         warp_group_id (int): The warp group id to print.
         warp_id (int): The warp id to print.
         print thread will be warp_group_id * warp_group_size + warp_id.
-        
+
     Returns:
         tir.PrimExpr: The TIR expression for the debug print operation.
-        
+
     Raises:
         ValueError: If the input object type is unsupported.
     """
@@ -210,7 +218,10 @@ def print(obj: Any, msg: str = "", warp_group_id: int = 0, warp_id: int = 0) ->
         # Directly print primitive expressions.
         return print_var(obj, msg)
 
+    elif obj is None:
+        return print_msg(msg)
+
     else:
         # Unsupported object type.
         raise ValueError(
-            f"Unexpected type: {type(obj)}. Supported types are tir.Buffer and tir.PrimExpr.")
+            f"Unexpected type: {type(obj)}. Supported types are tir.Buffer, tir.PrimExpr and None")
diff --git a/tilelang/language/proxy.py b/tilelang/language/proxy.py
index d6559f49..539c1d94 100644
--- a/tilelang/language/proxy.py
+++ b/tilelang/language/proxy.py
@@ -1,7 +1,7 @@
 """The language interface for tl programs."""
 
 from __future__ import annotations
-from typing import Any, Optional, Sequence, SupportsIndex, TYPE_CHECKING
+from typing import Any, Sequence, SupportsIndex, TYPE_CHECKING
 from typing_extensions import Self
 
 from tvm import tir
@@ -53,7 +53,8 @@ class BufferProxy:
     def from_ptr(self,
                  pointer_var: Var,
                  shape: tuple[PrimExpr, ...],
-                 dtype: str = "float32") -> Buffer:
+                 dtype: str = "float32",
+                 strides: tuple[PrimExpr, ...] = None) -> Buffer:
         """Create a buffer from a pointer, shape, and data type.
 
         Args:
@@ -64,12 +65,12 @@ class BufferProxy:
         Returns:
             A buffer created from the given parameters
         """
-        return match_buffer(pointer_var, shape, dtype=dtype)
+        return match_buffer(pointer_var, shape, dtype=dtype, strides=strides)
 
 
 class BaseTensorProxy:
     """Base proxy class for tensor types with configurable defaults.
-    
+
     This class serves as a foundation for different tensor proxy types, providing
     customizable default values for scope, alignment, and offset factors. It implements
     the core functionality for creating TIR buffers with specific memory configurations.
@@ -110,16 +111,17 @@ class BaseTensorProxy:
         )
 
     def __getitem__(self, keys) -> tir.Buffer:
-        if not isinstance(keys, tuple):
-            return self(keys)
-        if len(keys) >= 2 and not isinstance(keys[1], str):
-            return self(keys)
+        assert isinstance(keys, tuple)
+        # Single argument (the shape)
+        if all([type(s) not in (tuple, str, list) for s in keys]):
+            keys = (keys,)
         return self(*keys)
 
     def from_ptr(self,
                  pointer_var: Var,
                  shape: tuple[PrimExpr, ...],
-                 dtype: str = "float32") -> tir.Buffer:
+                 dtype: str = "float32",
+                 strides: tuple[PrimExpr, ...] = None) -> tir.Buffer:
         """Create a buffer from a pointer, shape, and data type.
 
         Args:
@@ -130,20 +132,58 @@ class BaseTensorProxy:
         Returns:
             A buffer created from the given parameters
         """
-        return match_buffer(pointer_var, shape, dtype=dtype)
+        return match_buffer(pointer_var, shape, dtype=dtype, strides=strides)
 
 
 class TensorProxy(BaseTensorProxy):
     """Main tensor proxy class for global scope buffers.
-    
+
     This class implements the default tensor proxy with global memory scope,
-    inheriting all functionality from BaseTensorProxy without modifications.
+    the tensor should be by default contiguous.
+    """
+
+    @staticmethod
+    def _construct_strides(shape: tuple[Any]):
+        s, strides = 1, [1]
+        for dim in shape[:0:-1]:
+            s *= dim
+            strides.append(s)
+        return tuple(reversed(strides))
+
+    def __call__(self,
+                 shape: tuple[Any] | PrimExpr | int,
+                 dtype: str = "float32",
+                 data=None,
+                 scope=None) -> tir.Buffer:
+        if isinstance(shape, (int, PrimExpr)):
+            shape = (shape,)
+        return super().__call__(
+            shape,
+            dtype=dtype,
+            strides=TensorProxy._construct_strides(shape),
+            data=data,
+            scope=scope)
+
+
+class StridedTensorProxy(BaseTensorProxy):
+    """Main tensor proxy class for global scope buffers, with strides supported.
+
+    This class implements the default tensor proxy with global memory scope, with the stride information required.
     """
 
+    def __call__(self,
+                 shape: tuple[Any],
+                 strides: tuple[Any],
+                 dtype: str = "float32",
+                 scope=None) -> tir.Buffer:
+        if len(shape) != len(strides):
+            raise ValueError("Invalid shape/strides' dimensions")
+        return super().__call__(shape, dtype=dtype, strides=strides, scope=scope)
+
 
 class FragmentBufferProxy(BaseTensorProxy):
     """Proxy class for fragment memory buffers.
-    
+
     This class represents tensor proxies specifically for local fragment memory,
     typically used in GPU tensor core operations.
     """
@@ -152,7 +192,7 @@ class FragmentBufferProxy(BaseTensorProxy):
 
 class SharedBufferProxy(BaseTensorProxy):
     """Proxy class for shared memory buffers.
-    
+
     This class represents tensor proxies for dynamic shared memory,
     commonly used in GPU shared memory operations.
     """
@@ -161,7 +201,7 @@ class SharedBufferProxy(BaseTensorProxy):
 
 class LocalBufferProxy(BaseTensorProxy):
     """Proxy class for local memory buffers.
-    
+
     This class represents tensor proxies for local memory scope,
     typically used for temporary computations in GPU kernels.
     """
@@ -204,12 +244,16 @@ if TYPE_CHECKING:
         def from_ptr(cls,
                      pointer_var: Var,
                      shape: Sequence[PrimExpr, ...],
-                     dtype: str = "float32") -> Self:
+                     dtype: str = "float32",
+                     strides: tuple[PrimExpr, ...] = None) -> Self:
             ...
 
     class Tensor(BaseTensor):
         ...
 
+    class StridedTensor(BaseTensor):
+        ...
+
     class FragmentBuffer(BaseTensor):
         ...
 
@@ -220,12 +264,13 @@ if TYPE_CHECKING:
         ...
 else:
     Tensor = TensorProxy()  # pylint: disable=invalid-name
+    StridedTensor = StridedTensorProxy()  # pylint: disable=invalid-name
     FragmentBuffer = FragmentBufferProxy()  # pylint: disable=invalid-name
     SharedBuffer = SharedBufferProxy()  # pylint: disable=invalid-name
     LocalBuffer = LocalBufferProxy()  # pylint: disable=invalid-name
 
 
-def ptr(dtype: Optional[str] = None,
+def ptr(dtype: str | None = None,
         storage_scope: str = "global",
         *,
         is_size_var: bool = False) -> Var:
@@ -250,5 +295,8 @@ def ptr(dtype: Optional[str] = None,
     return handle(dtype=dtype, storage_scope=storage_scope, is_size_var=is_size_var)
 
 
-def make_tensor(ptr: Var, shape: tuple[PrimExpr, ...], dtype: str = "float32") -> tir.Buffer:
-    return Tensor.from_ptr(ptr, shape, dtype)
+def make_tensor(ptr: Var,
+                shape: tuple[PrimExpr, ...],
+                dtype: str = "float32",
+                strides: tuple[PrimExpr, ...] = None) -> tir.Buffer:
+    return Tensor.from_ptr(ptr, shape, dtype, strides)
diff --git a/tilelang/language/reduce.py b/tilelang/language/reduce.py
index fcc01b5a..23167bdb 100644
--- a/tilelang/language/reduce.py
+++ b/tilelang/language/reduce.py
@@ -1,7 +1,7 @@
 """The language interface for tl programs."""
+from __future__ import annotations
 
 from tvm import tir
-from typing import Optional
 from tilelang.language import copy, macro, alloc_shared
 
 
@@ -24,6 +24,16 @@ def reduce(buffer: tir.Buffer, out: tir.Buffer, reduce_type: str, dim: int, clea
     Returns:
         tir.Call: Handle to the reduction operation
     """
+    # input shape: [X, d, Y], expected output shape: [X, Y] or [X, 1, Y]
+    expected_shapes = [
+        buffer.shape[:dim] + buffer.shape[dim + 1:],
+        buffer.shape[:dim] + [1] + buffer.shape[dim + 1:]
+    ]
+    if list(out.shape) not in expected_shapes:
+        expected_shapes_str = ' or '.join(map(str, expected_shapes))
+        raise ValueError(
+            f"Invalid reduce output shape, buffer shape is {buffer.shape}, dim is {dim}, "
+            f"output shape is {out.shape}, expected shapes are {expected_shapes_str}")
     buffer = buffer.access_ptr("r")
     out = out.access_ptr("w")
     return tir.call_intrin(
@@ -84,8 +94,8 @@ def reduce_sum(buffer: tir.Buffer, out: tir.Buffer, dim: int = -1, clear: bool =
         clear (bool, optional): If True, output buffer will be cleared before reduction.
                               If False, results will be accumulated on existing values.
                               Defaults to True.
-    Note: When clear=True, reduce_sum will not compute directly on the output buffer. This is because 
-          during warp reduction, the same value would be accumulated multiple times (number of threads 
+    Note: When clear=True, reduce_sum will not compute directly on the output buffer. This is because
+          during warp reduction, the same value would be accumulated multiple times (number of threads
           in the warp). Therefore, the implementation with clear=True follows these steps:
         1. create a temp buffer with same shape and dtype as out
         2. copy out to temp buffer
@@ -129,6 +139,51 @@ def reduce_absmax(buffer: tir.Buffer, out: tir.Buffer, dim: int = -1, clear: boo
     return reduce(buffer, out, "absmax", dim, clear)
 
 
+def reduce_bitand(buffer: tir.Buffer, out: tir.Buffer, dim: int = -1, clear: bool = True):
+    """Perform reduce bitwise-and on input buffer, store the result to output buffer.
+
+    Args:
+        buffer (tir.Buffer): The input buffer
+        out (tir.Buffer): The output buffer
+        dim (int): The dimension to perform reduce on
+
+    Returns:
+        tir.Call: Handle to the reduction operation
+    """
+    dim = _legalize_dim(buffer, dim)
+    return reduce(buffer, out, "bitand", dim, clear)
+
+
+def reduce_bitor(buffer: tir.Buffer, out: tir.Buffer, dim: int = -1, clear: bool = True):
+    """Perform reduce bitwise-or on input buffer, store the result to output buffer.
+
+    Args:
+        buffer (tir.Buffer): The input buffer
+        out (tir.Buffer): The output buffer
+        dim (int): The dimension to perform reduce on
+
+    Returns:
+        tir.Call: Handle to the reduction operation
+    """
+    dim = _legalize_dim(buffer, dim)
+    return reduce(buffer, out, "bitor", dim, clear)
+
+
+def reduce_bitxor(buffer: tir.Buffer, out: tir.Buffer, dim: int = -1, clear: bool = True):
+    """Perform reduce bitwise-xor on input buffer, store the result to output buffer.
+
+    Args:
+        buffer (tir.Buffer): The input buffer
+        out (tir.Buffer): The output buffer
+        dim (int): The dimension to perform reduce on
+
+    Returns:
+        tir.Call: Handle to the reduction operation
+    """
+    dim = _legalize_dim(buffer, dim)
+    return reduce(buffer, out, "bitxor", dim, clear)
+
+
 @macro
 def cumsum_fragment(src: tir.Buffer, dst: tir.Buffer, dim: int, reverse: bool) -> tir.PrimExpr:
     cumsum_smem = alloc_shared(src.shape, src.dtype, "shared.dyn")
@@ -144,17 +199,37 @@ def cumsum_fragment(src: tir.Buffer, dst: tir.Buffer, dim: int, reverse: bool) -
     copy(cumsum_smem, dst)
 
 
-def cumsum(src: tir.Buffer, dst: Optional[tir.Buffer] = None, dim: int = 0, reverse: bool = False):
-    """Perform cumulative sum on input buffer, store the result to output buffer.
+def cumsum(src: tir.Buffer, dst: tir.Buffer | None = None, dim: int = 0, reverse: bool = False):
+    """
+    Compute the cumulative sum of `src` along `dim`, writing results to `dst`.
 
-    Args:
-        src (tir.Buffer): The input buffer
-        dst (tir.Buffer, optional): The output buffer. Defaults to None.
-        dim (int, optional): The dimension to perform cumulative sum on. Defaults to 0.
-        reverse (bool, optional): Whether to perform reverse cumulative sum. Defaults to False.
+    Negative `dim` indices are normalized (Python-style). If `dst` is None, the operation is performed in-place into `src`. Raises ValueError when `dim` is out of bounds for `src.shape`. When `src.scope() == "local.fragment"`, this delegates to `cumsum_fragment`; otherwise it emits the `tl.cumsum` intrinsic.
+
+    Examples:
+        A 1D inclusive scan that writes the result into a separate shared-memory buffer:
+
+        >>> import tilelang.language as T
+        >>> @T.prim_func
+        ... def kernel(A: T.Tensor((128,), "float32"), B: T.Tensor((128,), "float32")):
+        ...     with T.Kernel(1, threads=128):
+        ...         A_shared = T.alloc_shared((128,), "float32")
+        ...         T.copy(A, A_shared)
+        ...         T.cumsum(src=A_shared, dst=A_shared, dim=0)
+        ...         T.copy(A_shared, B)
+
+        A 2D prefix sum along the last dimension with reverse accumulation:
+
+        >>> import tilelang.language as T
+        >>> @T.prim_func
+        ... def kernel2d(A: T.Tensor((64, 64), "float16"), B: T.Tensor((64, 64), "float16")):
+        ...     with T.Kernel(1, 1, threads=256):
+        ...         tile = T.alloc_shared((64, 64), "float16")
+        ...         T.copy(A, tile)
+        ...         T.cumsum(src=tile, dim=1, reverse=True)
+        ...         T.copy(tile, B)
 
     Returns:
-        tir.Call: Handle to the cumulative sum operation
+        tir.Call: A handle to the emitted cumulative-sum operation.
     """
 
     shape = src.shape
@@ -175,3 +250,106 @@ def cumsum(src: tir.Buffer, dst: Optional[tir.Buffer] = None, dim: int = 0, reve
         dim,
         reverse,
     )
+
+
+def finalize_reducer(reducer: tir.Buffer):
+    """
+    Finalize a reducer buffer by emitting the `tl.finalize_reducer` intrinsic.
+
+    This returns a TVM `tir.Call` handle that finalizes the given reducer using its writable pointer.
+    The call does not modify Python objects directly; it produces the low-level intrinsic call used by the IR.
+
+    Parameters:
+        reducer (tir.Buffer): Reducer buffer whose writable pointer will be finalized.
+
+    Returns:
+        tir.Call: Handle to the finalize reducer intrinsic call.
+    """
+    return tir.call_intrin(
+        "handle",
+        tir.op.Op.get("tl.finalize_reducer"),
+        reducer.access_ptr("w"),
+    )
+
+
+# TileScale extra
+
+
+def warp_reduce_sum(value: tir.PrimExpr):
+    """Perform warp reduction sum on a register value.
+
+    This function reduces a value across all threads in a warp using shuffle operations.
+    Each thread provides a  register `value`, and after the reduction, all threads
+    will have the sum of all values across the warp.
+
+    Args:
+        x (tir.PrimExpr): The input register value to reduce
+
+    Returns:
+        tir.PrimExpr: The reduced sum value (same on all threads in the warp)
+    """
+    return tir.call_intrin(value.dtype, tir.op.Op.get("tl.warp_reduce_sum"), value)
+
+
+def warp_reduce_max(value: tir.PrimExpr):
+    """Perform warp reduction max on a register value.
+
+    This function reduces a value across all threads in a warp using shuffle operations.
+    Each thread provides a  register `value`, and after the reduction, all threads
+    will have the max of all values across the warp.
+
+    Args:
+        value (tir.PrimExpr): The input register value to reduce
+
+    Returns:
+        tir.PrimExpr: The reduced max value (same on all threads in the warp)
+    """
+    return tir.call_intrin(value.dtype, tir.op.Op.get("tl.warp_reduce_max"), value)
+
+
+def warp_reduce_min(value: tir.PrimExpr):
+    """Perform warp reduction min on a register value.
+
+    This function reduces a value across all threads in a warp using shuffle operations.
+    Each thread provides a  register `value`, and after the reduction, all threads
+    will have the min of all values across the warp.
+
+    Args:
+        value (tir.PrimExpr): The input register value to reduce
+
+    Returns:
+        tir.PrimExpr: The reduced min value (same on all threads in the warp)
+    """
+    return tir.call_intrin(value.dtype, tir.op.Op.get("tl.warp_reduce_min"), value)
+
+
+def warp_reduce_bitand(value: tir.PrimExpr):
+    """Perform warp reduction bitwise-and on a register value.
+
+    This function reduces a value across all threads in a warp using shuffle operations.
+    Each thread provides a  register `value`, and after the reduction, all threads
+    will have the bitwise-and of all values across the warp.
+
+    Args:
+        value (tir.PrimExpr): The input register value to reduce
+
+    Returns:
+        tir.PrimExpr: The reduced bitwise-and value (same on all threads in the warp)
+    """
+    return tir.call_intrin(value.dtype, tir.op.Op.get("tl.warp_reduce_bitand"), value)
+
+
+def warp_reduce_bitor(value: tir.PrimExpr):
+    """Perform warp reduction bitwise-or on a register value.
+
+    This function reduces a value across all threads in a warp using shuffle operations.
+    Each thread provides a  register `value`, and after the reduction, all threads
+    will have the bitwise-or of all values across the warp.
+
+    Args:
+        value (tir.PrimExpr): The input register value to reduce
+
+    Returns:
+        tir.PrimExpr: The reduced bitwise-or value (same on all threads in the warp)
+    """
+    return tir.call_intrin(value.dtype, tir.op.Op.get("tl.warp_reduce_bitor"), value)
diff --git a/tilelang/language/symbolics.py b/tilelang/language/symbolics.py
new file mode 100644
index 00000000..92b9d5ba
--- /dev/null
+++ b/tilelang/language/symbolics.py
@@ -0,0 +1,28 @@
+"""Symbolic variable helpers exposed on the TileLang language surface."""
+
+from tvm import tir
+
+from tilelang.utils import deprecated
+
+__all__ = ["dynamic", "symbolic"]
+
+
+@deprecated("T.dynamic(...)", "tir.Var(...)", "v0.1.9")
+def dynamic(name: str, dtype: str = "int32"):
+    """
+    Create a TIR dynamic symbolic variable.
+
+    Parameters:
+        name (str): Identifier for the variable in generated TIR.
+        dtype (str): Data type string for the variable (e.g., "int32"). Defaults to "int32".
+
+    Returns:
+        tir.Var: A TIR variable with the given name and dtype for use in TIR/TensorIR kernels.
+    """
+    return tir.Var(name, dtype)
+
+
+@deprecated("T.symbolic(...)", "T.dynamic(...)")
+def symbolic(name: str, dtype: str = "int32"):
+    """Deprecated alias for `T.dynamic`."""
+    return tir.Var(name, dtype)
diff --git a/tilelang/language/tir/entry.py b/tilelang/language/tir/entry.py
index d663ee11..22702ae4 100644
--- a/tilelang/language/tir/entry.py
+++ b/tilelang/language/tir/entry.py
@@ -1,14 +1,15 @@
-from typing import Callable, Optional, Union
+from __future__ import annotations
+import inspect
+from typing import Callable
 
-from tvm.tir.function import PrimFunc
 import tvm.script.parser.tir.entry as _tir_entry
-import inspect
+from tvm.tir.function import PrimFunc
 from tvm.script.parser._core import parse, scan_macro, utils
 
 
-def prim_func(func: Optional[Callable] = None,
+def prim_func(func: Callable | None = None,
               private: bool = False,
-              check_well_formed=True) -> Union[PrimFunc, Callable]:
+              check_well_formed: bool = False) -> PrimFunc | Callable:
     """The parsing method for tir prim func, by using `@prim_func` as decorator.
 
     Parameters
@@ -40,8 +41,11 @@ def prim_func(func: Optional[Callable] = None,
     def decorator_wrapper(func):
         if not inspect.isfunction(func):
             raise TypeError(f"Expect a function, but got: {func}")
+        nonlocal outer_stack
         if utils.is_defined_in_class(outer_stack, func):
+            outer_stack = None
             return func
+        outer_stack = None
         f = parse(func, utils.inspect_function_capture(func), check_well_formed=check_well_formed)
         setattr(f, "__name__", func.__name__)  # noqa: B010
         return f
diff --git a/tilelang/language/tir/ir.py b/tilelang/language/tir/ir.py
index cbce46f2..977e6503 100644
--- a/tilelang/language/tir/ir.py
+++ b/tilelang/language/tir/ir.py
@@ -1,15 +1,32 @@
+from __future__ import annotations
 import tvm.script.ir_builder.tir.ir as _ir
 from tvm.script.ir_builder.tir import frame
 from tvm.tir import PrimExpr
-from typing import Any, Dict
+from typing import Any
 import tilelang.language.tir.op as _tir_op
 import functools
 
 
+class SerialStepSpec:
+    """A lightweight spec object for stepped serial loops.
+
+    This is consumed by the TileLang TIR parser override to realize
+    inclusive stepped loops like T.serial(start, end, step).
+    """
+
+    def __init__(self, start: PrimExpr, stop: PrimExpr, step: PrimExpr | int,
+                 annotations: dict[str, Any] | None):
+        self.start = start
+        self.stop = stop
+        self.step = step
+        self.annotations = annotations
+
+
 def serial(start: PrimExpr,
-           stop: PrimExpr = None,
+           stop: PrimExpr | None = None,
+           step: PrimExpr | int | None = None,
            *,
-           annotations: Dict[str, Any] = None) -> frame.ForFrame:
+           annotations: dict[str, Any] = None) -> frame.ForFrame | SerialStepSpec:
     """The serial For statement.
 
     Parameters
@@ -20,6 +37,14 @@ def serial(start: PrimExpr,
     stop : PrimExpr
         The maximum value of iteration.
 
+    step : PrimExpr | int | None
+        Optional step size of iteration. When provided as the third positional
+        argument (or keyword), the loop iterates inclusively with stride `step`:
+        i = start, start+step, ..., <= end. If `end-start` is not divisible by
+        `step`, the last value will be the largest `start + k*step` such that
+        it does not exceed `end` (for positive step). Negative steps are not
+        currently supported.
+
     annotations : Dict[str, Any]
         The optional annotations of the For statement.
 
@@ -28,13 +53,23 @@ def serial(start: PrimExpr,
     res : frame.ForFrame
         The ForFrame.
     """
-    return _ir.serial(start=start, stop=stop, annotations=annotations)
+    # If no step is provided, delegate to the upstream builder (supports
+    # both one-arg and two-arg forms).
+    if step is None:
+        return _ir.serial(start=start, stop=stop, annotations=annotations)
+
+    # Step provided: return a spec for the parser override to lower into an
+    # inclusive stepped loop. Require `stop` to be provided explicitly.
+    if stop is None:
+        raise TypeError("T.serial(start, end, step): `end` must be provided when `step` is set")
+
+    return SerialStepSpec(start=start, stop=stop, step=step, annotations=annotations)
 
 
 def parallel(start: PrimExpr,
              stop: PrimExpr = None,
              *,
-             annotations: Dict[str, Any] = None) -> frame.ForFrame:
+             annotations: dict[str, Any] = None) -> frame.ForFrame:
     """The parallel For statement.
 
     Parameters
@@ -59,7 +94,7 @@ def parallel(start: PrimExpr,
 def vectorized(start: PrimExpr,
                stop: PrimExpr = None,
                *,
-               annotations: Dict[str, Any] = None) -> frame.ForFrame:
+               annotations: dict[str, Any] = None) -> frame.ForFrame:
     """The vectorized For statement.
 
     Parameters
@@ -84,7 +119,7 @@ def vectorized(start: PrimExpr,
 def unroll(start: PrimExpr,
            stop: PrimExpr = None,
            *,
-           annotations: Dict[str, Any] = None) -> frame.ForFrame:
+           annotations: dict[str, Any] = None) -> frame.ForFrame:
     """The unrolled For statement.
 
     Parameters
@@ -111,7 +146,7 @@ def thread_binding(
     stop: PrimExpr = None,
     thread: str = None,
     *,
-    annotations: Dict[str, Any] = None,
+    annotations: dict[str, Any] = None,
 ) -> frame.ForFrame:
     """The thread-binding For statement.
 
@@ -291,6 +326,8 @@ call_llvm_pure_intrin = _dtype_forward(_tir_op.call_llvm_pure_intrin)
 call_pure_extern = _dtype_forward(_tir_op.call_pure_extern)
 ptx_mma = _dtype_forward(_tir_op.ptx_mma)
 ptx_mma_sp = _dtype_forward(_tir_op.ptx_mma_sp)
+ptx_wgmma_ss = _dtype_forward(_tir_op.ptx_wgmma_ss)
+ptx_wgmma_rs = _dtype_forward(_tir_op.ptx_wgmma_rs)
 ptx_ldmatrix = _dtype_forward(_tir_op.ptx_ldmatrix)
 ptx_cp_async = _dtype_forward(_tir_op.ptx_cp_async)
 ptx_cp_async_bulk = _dtype_forward(_tir_op.ptx_cp_async_bulk)
@@ -303,3 +340,65 @@ tvm_mfma = _dtype_forward(_tir_op.tvm_mfma)
 tvm_mfma_store = _dtype_forward(_tir_op.tvm_mfma_store)
 tvm_rdna_wmma = _dtype_forward(_tir_op.tvm_rdna_wmma)
 tvm_rdna_wmma_store = _dtype_forward(_tir_op.tvm_rdna_wmma_store)
+
+### Distributed enums ###
+import sys
+from enum import IntEnum
+
+
+class Team(IntEnum):
+    INVALID = -1
+    WORLD = 0
+    WORLD_INDEX = 0
+    SHARED = 1
+    SHARED_INDEX = 1
+    NODE = 2
+    NODE_INDEX = 2
+    SAME_MYPE_NODE = 3
+    SAME_MYPE_NODE_INDEX = 3
+    SAME_GPU = 4
+    SAME_GPU_INDEX = 4
+    GPU_LEADERS = 5
+    GPU_LEADERS_INDEX = 5
+    TEAMS_MIN = 6
+    TEAM_INDEX_MAX = sys.maxsize
+
+
+class CmpType(IntEnum):
+    EQ = 0
+    NE = 1
+    GT = 2
+    LE = 3
+    LT = 4
+    GE = 5
+    SENTINEL = sys.maxsize
+
+
+class Amo(IntEnum):
+    """Atomic Memory Operation (AMO) types.
+    Note: Signal ops (AMO_SIGNAL_SET and AMO_SIGNAL_ADD) are
+    included as a part of the AMO operations.
+    """
+
+    AMO_ACK = 1
+    AMO_INC = 2
+    AMO_SET = 3
+    AMO_ADD = 4
+    AMO_AND = 5
+    AMO_OR = 6
+    AMO_XOR = 7
+    AMO_SIGNAL = 8
+    SIGNAL_SET = 9
+    SIGNAL_ADD = 10
+    AMO_SIGNAL_SET = 9  # the same as SIGNAL_SET
+    AMO_SIGNAL_ADD = 10  # the same as SIGNAL_ADD
+    AMO_END_OF_NONFETCH = 11  # end of nonfetch atomics
+    AMO_FETCH = 12
+    AMO_FETCH_INC = 13
+    AMO_FETCH_ADD = 14
+    AMO_FETCH_AND = 15
+    AMO_FETCH_OR = 16
+    AMO_FETCH_XOR = 17
+    AMO_SWAP = 18
+    AMO_COMPARE_SWAP = 19
+    AMO_OP_SENTINEL = sys.maxsize
diff --git a/tilelang/language/tir/op.py b/tilelang/language/tir/op.py
index 77be7e12..92566560 100644
--- a/tilelang/language/tir/op.py
+++ b/tilelang/language/tir/op.py
@@ -1,4 +1,5 @@
-from typing import Any, Optional
+from __future__ import annotations
+from typing import Any
 import tvm
 from tvm.ir import PrimExpr
 from tvm.ir.base import Span
@@ -1061,6 +1062,88 @@ def ptx_mma_sp(
     )
 
 
+def ptx_wgmma_ss(
+    dtype,
+    wgmma_prefix,
+    a_is_k_major,
+    b_is_k_major,
+    a_dtype_abbrv,
+    b_dtype_abbrv,
+    accum_dtype_abbrv,
+    A_desc,
+    A_offset,
+    B_desc,
+    B_offset,
+    C_data,
+    C_offset,
+    scale_out,
+    scale_in_a,
+    scale_in_b,
+):
+    """TVM intrinsic for ptx tensor core wmma instructions
+    https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-for-wmma
+    """
+    return call_intrin(
+        dtype,
+        _tvm_op.Op.get("tl.ptx_wgmma_ss"),
+        wgmma_prefix,
+        a_is_k_major,
+        b_is_k_major,
+        a_dtype_abbrv,
+        b_dtype_abbrv,
+        accum_dtype_abbrv,
+        A_desc,
+        A_offset,
+        B_desc,
+        B_offset,
+        C_data,
+        C_offset,
+        scale_out,
+        scale_in_a,
+        scale_in_b,
+    )
+
+
+def ptx_wgmma_rs(
+    dtype,
+    wgmma_prefix,
+    a_is_k_major,
+    b_is_k_major,
+    a_dtype_abbrv,
+    b_dtype_abbrv,
+    accum_dtype_abbrv,
+    A_buf,
+    A_offset,
+    B_desc,
+    B_offset,
+    C_data,
+    C_offset,
+    scale_out,
+    scale_in_a,
+    scale_in_b,
+):
+
+    return call_intrin(
+        dtype,
+        _tvm_op.Op.get("tl.ptx_wgmma_rs"),
+        wgmma_prefix,
+        a_is_k_major,
+        b_is_k_major,
+        a_dtype_abbrv,
+        b_dtype_abbrv,
+        accum_dtype_abbrv,
+        A_buf,
+        A_offset,
+        B_desc,
+        B_offset,
+        C_data,
+        C_offset,
+        scale_out,
+        scale_in_a,
+        scale_in_b,
+    )
+
+
 def mma_store(dtype, m, n, dst_ptr, src_ptr, src_offset, dst_stride):
     """TVM intrinsic for storing the result of PTX MMA into a destination pointer
 
@@ -1321,8 +1404,9 @@ def tvm_mfma(
     call : PrimExpr
         The call expression.
     """
-    return _tvm_op.tvm_mfma(
+    return call_intrin(
         dtype,
+        _tvm_op.Op.get("tl.tvm_mfma"),
         shape,
         A_layout,
         B_layout,
@@ -1369,7 +1453,16 @@ def tvm_mfma_store(dtype, m, n, dst_ptr, src_ptr, src_offset, dst_stride):
     call : PrimExpr
         The call expression.
     """
-    return _tvm_op.tvm_mfma_store(dtype, m, n, dst_ptr, src_ptr, src_offset, dst_stride)
+    return call_intrin(
+        dtype,
+        _tvm_op.Op.get("tl.tvm_mfma_store"),
+        m,
+        n,
+        dst_ptr,
+        src_ptr,
+        src_offset,
+        dst_stride,
+    )
 
 
 def tvm_rdna_wmma(
@@ -1436,8 +1529,9 @@ def tvm_rdna_wmma(
     call : PrimExpr
         The call expression.
     """
-    return _tvm_op.tvm_rdna_wmma(
+    return call_intrin(
         dtype,
+        _tvm_op.Op.get("tl.tvm_rdna_wmma"),
         shape,
         A_layout,
         B_layout,
@@ -1484,7 +1578,16 @@ def tvm_rdna_wmma_store(dtype, m, n, dst_ptr, src_ptr, src_offset, dst_stride):
     call : PrimExpr
         The call expression.
     """
-    return _tvm_op.tvm_rdna_wmma_store(dtype, m, n, dst_ptr, src_ptr, src_offset, dst_stride)
+    return call_intrin(
+        dtype,
+        _tvm_op.Op.get("tl.tvm_rdna_wmma_store"),
+        m,
+        n,
+        dst_ptr,
+        src_ptr,
+        src_offset,
+        dst_stride,
+    )
 
 
 def ptx_cp_async_barrier(barrier_id):
@@ -1755,7 +1858,7 @@ def min_value(dtype, span=None):
     return _tvm_op.min_value(dtype, span)
 
 
-def max_value(dtype: str, span: Optional[Span] = None) -> Any:
+def max_value(dtype: str, span: Span | None = None) -> Any:
     """maximum value of dtype
 
     Parameters
@@ -1774,7 +1877,7 @@ def max_value(dtype: str, span: Optional[Span] = None) -> Any:
     return _tvm_op.max_value(dtype, span)
 
 
-def infinity(dtype: str, span: Optional[Span] = None) -> Any:
+def infinity(dtype: str, span: Span | None = None) -> Any:
     """infinity value of dtype
 
     Parameters
@@ -1793,7 +1896,7 @@ def infinity(dtype: str, span: Optional[Span] = None) -> Any:
     return _tvm_op.infinity(dtype, span)
 
 
-def reinterpret(dtype, value, span: Optional[Span] = None) -> Any:
+def reinterpret(dtype, value, span: Span | None = None) -> Any:
     """infinity value of dtype
 
     Parameters
@@ -2602,7 +2705,7 @@ def isinf(x, span=None):
 
 def pow_of_int(x: PrimExpr, y: int) -> PrimExpr:
     """Fast power operation than pow(float, float).
-    
+
     Args:
         x (PrimExpr): Base value
         y (int): Exponent value
diff --git a/tilelang/language/utils.py b/tilelang/language/utils.py
new file mode 100644
index 00000000..161a09c4
--- /dev/null
+++ b/tilelang/language/utils.py
@@ -0,0 +1,160 @@
+from __future__ import annotations
+
+from tilelang import tvm as tvm
+from tvm import tir
+from tvm.tir import PrimExpr, Buffer, BufferLoad, op
+from tilelang import language as T
+
+
+def region(buffer: BufferLoad, access_type: str, *args: PrimExpr):
+    """
+    Create a tile memory-region descriptor for a BufferLoad.
+
+    Maps access_type ('r', 'w', 'rw') to the numeric codes expected by the `tl.region` intrinsic
+    (1, 2, 3 respectively) and returns a tir.Call representing the region with the provided extents.
+
+    Parameters:
+        buffer (tir.BufferLoad): The BufferLoad that identifies the underlying buffer and indices.
+        access_type (str): One of 'r', 'w', or 'rw' indicating read, write, or read-write access.
+        *args (tir.PrimExpr): Extent expressions for each region dimension.
+
+    Returns:
+        tir.Call: A call to the `tl.region` intrinsic describing the memory region.
+
+    Raises:
+        KeyError: If access_type is not one of 'r', 'w', or 'rw'.
+    """
+    access_type = {"r": 1, "w": 2, "rw": 3}[access_type]
+    return T.call_intrin("handle", op.Op.get("tl.region"), buffer, access_type, *args)
+
+
+def buffer_to_tile_region(buffer: Buffer, access_type: str):
+    """Convert a TVM buffer to a tile region descriptor.
+
+    Args:
+        buffer (tir.Buffer): The buffer to convert
+        access_type (str): Type of access - 'r' for read, 'w' for write, 'rw' for read-write
+
+    Returns:
+        tir.Call: A region descriptor covering the entire buffer
+    """
+    mins = [0 for _ in buffer.shape]
+    extents = [x for x in buffer.shape]
+    return region(T.BufferLoad(buffer, mins), access_type, *extents)
+
+
+def buffer_load_to_tile_region(load: BufferLoad, access_type: str, extents: list[PrimExpr]):
+    """Convert a buffer load operation to a tile region descriptor.
+
+    Args:
+        load (tir.BufferLoad): The buffer load operation
+        access_type (str): Type of access - 'r' for read, 'w' for write, 'rw' for read-write
+        extents (list[tir.PrimExpr]): list of expressions defining the region size
+
+    Returns:
+        tir.Call: A region descriptor for the loaded area
+    """
+    indices = load.indices
+
+    if len(indices) > len(extents):
+        # (f"mismatch between indices and extents for buffer load {load}: indices = {indices}, extents = {extents}, "
+        # f"region will be expanded in the last 2 dimensions")
+        new_extents = []
+        for _ in range(len(indices) - len(extents)):
+            new_extents.append(1)
+        for extent in extents:
+            new_extents.append(extent)
+        extents = new_extents
+    assert len(indices) == len(extents), f"indices = {indices}, extents = {extents}"
+    return region(load, access_type, *extents)
+
+
+def buffer_region_to_tile_region(buffer_region: tir.BufferRegion, access_type: str,
+                                 extents: list[tir.PrimExpr]):
+    """Convert a buffer region to a tile region descriptor.
+
+    Args:
+        buffer_region (tir.BufferRegion): The buffer region to convert
+        access_type (str): Type of access - 'r' for read, 'w' for write, 'rw' for read-write
+
+    Returns:
+        tir.Call: A region descriptor for the specified buffer region
+    """
+    mins = [x.min for x in buffer_region.region]
+    region_extents = [x.extent for x in buffer_region.region]
+    assert len(region_extents) >= len(
+        extents
+    ), f"region_extents must be >= extents, region_extents = {region_extents}, extents = {extents}"
+
+    return region(T.BufferLoad(buffer_region.buffer, mins), access_type, *region_extents)
+
+
+def index_to_coordinates(index, shape) -> list[PrimExpr]:
+    """
+    Convert a flat (linear) index into multi-dimensional coordinates for a given shape.
+
+    Given a linear index and a shape (sequence of dimension extents), returns a list of coordinates (one per dimension) such that converting those coordinates back to a linear index using the usual row-major / C-order formula yields the original index. The computation iterates from the last dimension to the first using modulo and integer division, then reverses the collected coordinates.
+
+    Parameters:
+        index (int or PrimExpr): The flat index to convert.
+        shape (Sequence[int]): The extents of each dimension (length >= 1).
+
+    Returns:
+        list[PrimExpr]: Coordinates for each dimension in the same order as `shape`.
+    """
+    coordinates = []
+    dims = len(shape)
+    for i in range(dims):
+        coordinates.append(index % shape[dims - i - 1])
+        index = index // shape[dims - i - 1]
+    coordinates.reverse()
+    return coordinates
+
+
+def linear_index(*args: PrimExpr) -> PrimExpr:
+    """
+    Compute a flat (linear) index from multi-dimensional coordinates and strides.
+
+    The function accepts a sequence of PrimExpr arguments where the first portion are coordinates
+    and the trailing portion are the corresponding strides. The number of strides must equal
+    (number of coordinates - 1). The linear index is computed as:
+
+        linear = coords[0]
+        for each (coord, stride) in zip(coords[1:], strides):
+            linear = linear * stride + coord
+
+    Examples:
+        - linear_index(i) -> i
+        - linear_index(i, j) -> i * j_stride + j  (requires j_stride provided as stride when needed)
+        - linear_index(i, j, stride_j) -> i * stride_j + j
+        - linear_index(i, j, k, stride_j, stride_k) -> i*stride_j*stride_k + j*stride_k + k
+        - linear_index(i, tx, v, threads, local_size) -> i*threads*local_size + tx*local_size + v
+
+    Raises:
+        ValueError: If called with no arguments, or if the number of strides is not one less than
+                    the number of coordinates.
+
+    Returns:
+        PrimExpr: The computed linear index expression.
+    """
+    n = len(args)
+    if n == 0:
+        raise ValueError("At least one index is required")
+
+    if n == 1:
+        return args[0]
+
+    # The first part is indices, the second part is strides (starting from the second dimension)
+    # A simpler way: the number of strides = total number of arguments - number of indices
+    # Actually, the args are designed as indices... + strides..., and the number of strides = number of indices - 1
+    num_coords = (n + 1) // 2
+    coords = args[:num_coords]
+    strides = args[num_coords:]
+
+    if len(strides) != len(coords) - 1:
+        raise ValueError("Stride count must be one less than coordinate count")
+
+    linear = coords[0]
+    for idx, stride in zip(coords[1:], strides):
+        linear = linear * stride + idx
+    return linear
diff --git a/tilelang/language/warpgroup.py b/tilelang/language/warpgroup.py
index 0d994be6..872d3001 100644
--- a/tilelang/language/warpgroup.py
+++ b/tilelang/language/warpgroup.py
@@ -1,10 +1,10 @@
 """The language interface for tl programs."""
+from __future__ import annotations
 
 from tvm.script.ir_builder.tir.frame import TIRFrame
-from tvm._ffi import register_object
+from tvm.ffi import register_object
 from tilelang import _ffi_api
 from .kernel import get_thread_bindings, get_thread_extents
-from typing import List
 
 
 @register_object("tl.WarpSpecializeFrame")
@@ -45,7 +45,7 @@ def WarpSpecialize(*warp_group_idx):
     # only available for nvidia gpus.
     warp_group_size = 128
 
-    warp_group_ids: List[int] = []
+    warp_group_ids: list[int] = []
     for warp_group_id in warp_group_idx:
         warp_group_ids.append(warp_group_id)
 
